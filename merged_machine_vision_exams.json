{
  "merged_source": "Machine Vision Exams Compilation (2014–2020)",
  "level": "advanced_undergraduate",
  "packages": [
    "unknown",
    "pkg1",
    "pkg3",
    "pkg4",
    "pkg2",
    "pkg8",
    "pkg6",
    "pkg7",
    "pkg10",
    "pkg7",
    "pkg8",
    "pkg9",
    "pkg15",
    "pkg12",
    "pkg24",
    "pkg23",
    "pkg22",
    "pkg25",
    "pkg13",
    "pkg14",
    "pkg4",
    "unknown",
    "pkg6",
    "pkg11",
    "pkg5",
    "pkg3",
    "pkg1",
    "pkg16",
    "pkg20",
    "pkg18",
    "pkg26",
    "pkg19",
    "pkg21",
    "pkg17",
    "pkg2",
    "pkg31",
    "pkg30",
    "pkg29",
    "pkg28",
    "package_3",
    "package_11"
  ],
  "mcqs": [
    {
      "id": "pkg1_mcq1",
      "question": "Which of the following best describes a limitation of 'informed consent' in the context of data mining, as discussed in the lecture slides?",
      "options": {
        "A": "Informed consent is always sufficient because once users agree they waive all privacy rights.",
        "B": "Informed consent may be insufficient because data mining can discover unforeseen sensitive information that could not be fully described to subjects in advance.",
        "C": "Informed consent is unnecessary when datasets are anonymised because anonymisation removes all privacy risk.",
        "D": "Informed consent is only required for data collected in clinical trials, not for routine digital traces."
      },
      "correct_option": "B",
      "difficulty": "medium",
      "learning_objective": "Recognise limits of informed consent for large-scale data mining and privacy implications",
      "slide_refs": [
        10,
        11,
        12,
        26
      ]
    },
    {
      "id": "pkg1_mcq2",
      "question": "Which anonymisation approach (mentioned in the slides) explicitly requires that each released record be indistinguishable from at least k-1 other records with respect to a set of quasi-identifiers?",
      "options": {
        "A": "Pseudonymisation",
        "B": "k-anonymity",
        "C": "Encryption",
        "D": "Differential privacy"
      },
      "correct_option": "B",
      "difficulty": "easy",
      "learning_objective": "Identify k-anonymity as a structural anonymisation technique for protecting privacy",
      "slide_refs": [
        29,
        31
      ]
    },
    {
      "id": "pkg1_mcq3",
      "question": "According to the lecture slides, which statement correctly contrasts relational databases and document-oriented NoSQL databases (e.g., MongoDB)?",
      "options": {
        "A": "Relational databases permit embedded documents and lack a fixed schema, while MongoDB enforces a strict schema across all documents.",
        "B": "Relational databases provide a standardised query language (SQL) and strong schema constraints; document stores offer flexible schemas but less standardisation.",
        "C": "Document stores always provide stronger consistency guarantees than relational databases.",
        "D": "Relational databases are unsuitable for any scalable or distributed use; NoSQL databases are the only scalable option."
      },
      "correct_option": "B",
      "difficulty": "medium",
      "learning_objective": "Compare trade-offs between relational and NoSQL systems (schema, standardisation, consistency, flexibility)",
      "slide_refs": [
        60,
        61,
        63
      ]
    },
    {
      "id": "pkg3_mcq1",
      "question": "Which of the following is **not** a common cause of missing data as described in the lecture?",
      "options": {
        "A": "Equipment malfunction or sensor failure",
        "B": "Data corruption during storage or transfer",
        "C": "Overfitting of predictive models",
        "D": "Non-response in surveys"
      },
      "correct_option": "C",
      "difficulty": "easy",
      "learning_objective": "Identify typical causes of missing values in datasets.",
      "slide_refs": [
        3,
        5
      ]
    },
    {
      "id": "pkg3_mcq2",
      "question": "In the context of missing data mechanisms, which statement best describes **Missing Completely at Random (MCAR)**?",
      "options": {
        "A": "The probability of a value being missing depends on other observed variables but not on the missing value itself.",
        "B": "The probability of a value being missing is related to the value that is missing.",
        "C": "The probability of a value being missing is independent of both observed and unobserved data.",
        "D": "The probability of a value being missing depends on unmeasured latent variables."
      },
      "correct_option": "C",
      "difficulty": "medium",
      "learning_objective": "Distinguish between MCAR, MAR, and MNAR mechanisms in data collection.",
      "slide_refs": [
        6,
        7,
        8
      ]
    },
    {
      "id": "pkg3_mcq3",
      "question": "Which deletion method uses all available data points for each variable pair when computing correlations?",
      "options": {
        "A": "Listwise deletion",
        "B": "Pairwise deletion",
        "C": "Hot deck imputation",
        "D": "Regression imputation"
      },
      "correct_option": "B",
      "difficulty": "medium",
      "learning_objective": "Differentiate between listwise and pairwise deletion strategies for handling missing data.",
      "slide_refs": [
        10,
        11
      ]
    },
    {
      "id": "pkg3_mcq4",
      "question": "Which imputation technique replaces missing values with the mean (numerical) or mode (categorical) of the variable?",
      "options": {
        "A": "Hot deck imputation",
        "B": "Regression imputation",
        "C": "Mean imputation",
        "D": "Multiple imputation"
      },
      "correct_option": "C",
      "difficulty": "easy",
      "learning_objective": "Understand basic imputation methods and their assumptions.",
      "slide_refs": [
        14,
        15
      ]
    },
    {
      "id": "pkg3_mcq5",
      "question": "Which imputation technique involves fitting a regression model on observed data to predict the missing values, assuming MAR holds?",
      "options": {
        "A": "Regression imputation",
        "B": "Hot deck imputation",
        "C": "k-Nearest Neighbour (kNN) imputation",
        "D": "Maximum likelihood estimation"
      },
      "correct_option": "A",
      "difficulty": "medium",
      "learning_objective": "Explain the process and assumptions of regression-based imputation.",
      "slide_refs": [
        16,
        17
      ]
    },
    {
      "id": "pkg3_mcq6",
      "question": "What is the main advantage of **multiple imputation** over single imputation methods?",
      "options": {
        "A": "It produces identical estimates across imputations, simplifying analysis.",
        "B": "It avoids uncertainty estimation and reduces computational complexity.",
        "C": "It accounts for the uncertainty in missing data by generating multiple plausible datasets and pooling results.",
        "D": "It always yields unbiased estimates, regardless of missingness type."
      },
      "correct_option": "C",
      "difficulty": "hard",
      "learning_objective": "Understand the rationale, steps, and advantages of multiple imputation.",
      "slide_refs": [
        21,
        22,
        23
      ]
    },
    {
      "id": "pkg3_mcq7",
      "question": "Which of the following statements about the **Expectation-Maximization (EM)** algorithm is true?",
      "options": {
        "A": "It is mainly used for categorical data sets.",
        "B": "It alternates between estimating missing data expectations and maximizing the likelihood function.",
        "C": "It requires no iteration and converges in a single step.",
        "D": "It always overestimates standard errors compared to multiple imputation."
      },
      "correct_option": "B",
      "difficulty": "hard",
      "learning_objective": "Explain the EM algorithm’s role in maximum likelihood estimation with missing data.",
      "slide_refs": [
        26,
        27,
        28
      ]
    },
    {
      "id": "pkg3_mcq8",
      "question": "In time series or longitudinal data, why can simple deletion lead to biased results?",
      "options": {
        "A": "Because deletion assumes MAR always holds.",
        "B": "Because deletion can disrupt temporal dependencies or autocorrelations.",
        "C": "Because deletion alters class distributions in classification tasks.",
        "D": "Because deletion introduces new missingness mechanisms."
      },
      "correct_option": "B",
      "difficulty": "medium",
      "learning_objective": "Recognize challenges of handling missing data in time-dependent datasets.",
      "slide_refs": [
        29
      ]
    },
    {
      "id": "pkg4_mcq1",
      "question": "When merging data from multiple sources, what is one key requirement for combining sensor data measuring the same phenomenon?",
      "options": {
        "A": "Sensors must all measure in different units for diversity.",
        "B": "Sensors must have identical sampling frequencies.",
        "C": "Sensors must come from the same manufacturer.",
        "D": "Sensors must be used under identical environmental conditions."
      },
      "correct_option": "B",
      "difficulty": "medium",
      "learning_objective": "Understand the technical requirements for merging multisource sensor data.",
      "slide_refs": [
        5,
        6
      ]
    },
    {
      "id": "pkg4_mcq2",
      "question": "Which of the following is a potential issue when merging datasets from different sources?",
      "options": {
        "A": "Identical timestamping and data alignment",
        "B": "Differences in data formats, units, and time representations",
        "C": "Overrepresentation of rare classes",
        "D": "High recall but low precision"
      },
      "correct_option": "B",
      "difficulty": "easy",
      "learning_objective": "Recognize format, unit, and timestamp issues in multisource data integration.",
      "slide_refs": [
        6,
        7
      ]
    },
    {
      "id": "pkg4_mcq3",
      "question": "What is the primary problem introduced by **downsampling** when aligning signals from different sensors?",
      "options": {
        "A": "Loss of temporal synchronization",
        "B": "Increased data noise",
        "C": "Loss of information due to reduced data density",
        "D": "Bias toward high-frequency data"
      },
      "correct_option": "C",
      "difficulty": "medium",
      "learning_objective": "Evaluate trade-offs in downsampling during data fusion.",
      "slide_refs": [
        12
      ]
    },
    {
      "id": "pkg4_mcq4",
      "question": "What is the **main consequence** of oversampling to match the least common multiple (LCM) of sampling rates?",
      "options": {
        "A": "Reduced computational time",
        "B": "Loss of information from low-frequency sensors",
        "C": "Increased data size and computational load",
        "D": "Destruction of temporal structure"
      },
      "correct_option": "C",
      "difficulty": "medium",
      "learning_objective": "Identify computational trade-offs of oversampling in multi-rate data integration.",
      "slide_refs": [
        13
      ]
    },
    {
      "id": "pkg4_mcq5",
      "question": "Which sampling method ensures every observation has an equal chance to be selected but is not replaced once drawn?",
      "options": {
        "A": "SRSWR (Simple Random Sampling With Replacement)",
        "B": "SRSWOR (Simple Random Sampling Without Replacement)",
        "C": "Balanced Sampling",
        "D": "Cluster Sampling"
      },
      "correct_option": "B",
      "difficulty": "easy",
      "learning_objective": "Differentiate between basic random sampling strategies.",
      "slide_refs": [
        17
      ]
    },
    {
      "id": "pkg4_mcq6",
      "question": "In the SMOTE algorithm, how are synthetic samples created?",
      "options": {
        "A": "By duplicating minority class samples exactly",
        "B": "By adding Gaussian noise to majority class data points",
        "C": "By interpolating between a sample and its k nearest neighbors",
        "D": "By randomly assigning new class labels to samples"
      },
      "correct_option": "C",
      "difficulty": "hard",
      "learning_objective": "Understand the SMOTE technique for addressing class imbalance.",
      "slide_refs": [
        23,
        24,
        25
      ]
    },
    {
      "id": "pkg4_mcq7",
      "question": "Which performance metric tends to overestimate model performance on imbalanced datasets?",
      "options": {
        "A": "Precision",
        "B": "Recall",
        "C": "Accuracy",
        "D": "F1 Score"
      },
      "correct_option": "C",
      "difficulty": "medium",
      "learning_objective": "Recognize limitations of accuracy as a performance measure in imbalanced data.",
      "slide_refs": [
        32,
        35
      ]
    },
    {
      "id": "pkg4_mcq8",
      "question": "Which formula correctly defines **balanced accuracy**?",
      "options": {
        "A": "Balanced accuracy = (Precision + Recall)/2",
        "B": "Balanced accuracy = (TP/(TP+FN) + TN/(TN+FP))/2",
        "C": "Balanced accuracy = (TP + TN)/(TP + FP + TN + FN)",
        "D": "Balanced accuracy = 2 * (Precision * Recall)/(Precision + Recall)"
      },
      "correct_option": "B",
      "difficulty": "hard",
      "learning_objective": "Apply the correct formula for balanced accuracy in performance evaluation.",
      "slide_refs": [
        36
      ]
    },
    {
      "id": "pkg4_mcq9",
      "question": "Which of the following metrics captures the balance between precision and recall?",
      "options": {
        "A": "Specificity",
        "B": "Accuracy",
        "C": "F1 Score",
        "D": "Cohen’s Kappa"
      },
      "correct_option": "C",
      "difficulty": "medium",
      "learning_objective": "Interpret and apply F1-score in evaluating classifier performance.",
      "slide_refs": [
        40
      ]
    },
    {
      "id": "pkg4_mcq10",
      "question": "Cohen’s Kappa is primarily used to:",
      "options": {
        "A": "Measure agreement between observed and predicted labels beyond chance",
        "B": "Estimate the percentage of correctly classified samples",
        "C": "Quantify the recall rate in multi-class problems",
        "D": "Compute mean differences between class probabilities"
      },
      "correct_option": "A",
      "difficulty": "hard",
      "learning_objective": "Explain Cohen’s Kappa as an inter-rater and classification agreement metric.",
      "slide_refs": [
        41
      ]
    },
    {
      "id": "pkg2_mcq1",
      "question": "According to the lecture, what was the main cause of the NASA Mars Climate Orbiter mission failure in 1999?",
      "options": {
        "A": "A software virus corrupted the navigation data.",
        "B": "Engineers failed to convert measurement units between English and metric systems.",
        "C": "The lander’s parachute deployed prematurely due to faulty sensors.",
        "D": "Incorrect sample size estimation led to navigation errors."
      },
      "correct_option": "B",
      "difficulty": "easy",
      "learning_objective": "Recognize the importance of consistent units and measurement standards in data collection and engineering.",
      "slide_refs": [
        6,
        7
      ]
    },
    {
      "id": "pkg2_mcq2",
      "question": "Which of the following best defines a non-stationary element in data collection as discussed in the lecture?",
      "options": {
        "A": "An element that maintains constant statistical properties over time.",
        "B": "An element whose characteristics or state change over time due to environmental or human factors.",
        "C": "An element that cannot be measured repeatedly with the same instrument.",
        "D": "An element that is unaffected by sensor drift or external conditions."
      },
      "correct_option": "B",
      "difficulty": "medium",
      "learning_objective": "Differentiate between stationary and non-stationary elements and understand their implications for data collection.",
      "slide_refs": [
        19,
        20
      ]
    },
    {
      "id": "pkg2_mcq3",
      "question": "Which principle from human research ethics emphasizes that individuals should be treated as autonomous agents and given additional protections if autonomy is diminished?",
      "options": {
        "A": "Beneficence",
        "B": "Justice",
        "C": "Respect for persons",
        "D": "Confidentiality"
      },
      "correct_option": "C",
      "difficulty": "medium",
      "learning_objective": "Identify the three core ethical principles governing human data collection and apply them appropriately.",
      "slide_refs": [
        26,
        27,
        28
      ]
    },
    {
      "id": "pkg8_mcq1",
      "question": "In data mining, what does 'generalizability' of a model refer to?",
      "options": {
        "A": "The ability of the model to fit the training data perfectly.",
        "B": "How well the model performs on new or unseen data.",
        "C": "The capacity of the model to memorize patterns from noise.",
        "D": "The degree to which the model parameters can be freely adjusted."
      },
      "correct_option": "B",
      "difficulty": "easy",
      "learning_objective": "Define model generalization and its importance in evaluating model performance.",
      "slide_refs": [
        6,
        7
      ]
    },
    {
      "id": "pkg8_mcq2",
      "question": "Which type of model is primarily used to test causal hypotheses and explain relationships between variables?",
      "options": {
        "A": "Descriptive model",
        "B": "Predictive model",
        "C": "Explanatory model",
        "D": "Diagnostic model"
      },
      "correct_option": "C",
      "difficulty": "medium",
      "learning_objective": "Distinguish between descriptive, predictive, and explanatory models in data mining.",
      "slide_refs": [
        11,
        13
      ]
    },
    {
      "id": "pkg8_mcq3",
      "question": "Predictive modeling focuses on:",
      "options": {
        "A": "Identifying causal mechanisms in data.",
        "B": "Summarizing existing patterns without forecasting.",
        "C": "Using training data to predict outputs for new observations.",
        "D": "Visualizing relationships between features."
      },
      "correct_option": "C",
      "difficulty": "easy",
      "learning_objective": "Understand the purpose and structure of predictive modeling.",
      "slide_refs": [
        12
      ]
    },
    {
      "id": "pkg8_mcq4",
      "question": "Which of the following statements correctly explains why explanatory and predictive models differ?",
      "options": {
        "A": "Explanatory models optimize accuracy; predictive models optimize interpretability.",
        "B": "Explanatory models are data-driven; predictive models are theory-driven.",
        "C": "Explanatory models aim for causal inference, while predictive models aim for accuracy on new data.",
        "D": "There are no practical differences between them."
      },
      "correct_option": "C",
      "difficulty": "medium",
      "learning_objective": "Explain the conceptual difference between explanatory and predictive modeling.",
      "slide_refs": [
        13,
        15
      ]
    },
    {
      "id": "pkg8_mcq5",
      "question": "What problem often occurs when the training dataset is too small?",
      "options": {
        "A": "Underfitting due to excessive model complexity.",
        "B": "Overfitting, leading to poor generalization on test data.",
        "C": "Reduced variance in model predictions.",
        "D": "Higher confidence intervals around estimates."
      },
      "correct_option": "B",
      "difficulty": "medium",
      "learning_objective": "Recognize the impact of sample size on model overfitting and generalizability.",
      "slide_refs": [
        18
      ]
    },
    {
      "id": "pkg8_mcq6",
      "question": "Which data partitioning strategy helps to ensure a model’s generalization ability?",
      "options": {
        "A": "Training only on the full dataset.",
        "B": "Using independent training and test sets with 80-20 or 2/3-1/3 split.",
        "C": "Evaluating on the same data used for training.",
        "D": "Merging training and test sets for better accuracy."
      },
      "correct_option": "B",
      "difficulty": "easy",
      "learning_objective": "Understand the role of data partitioning in training and testing models.",
      "slide_refs": [
        25
      ]
    },
    {
      "id": "pkg8_mcq7",
      "question": "What is the primary purpose of a validation set during model development?",
      "options": {
        "A": "To fine-tune model hyperparameters and prevent overfitting.",
        "B": "To evaluate final model performance on unseen data.",
        "C": "To reduce the size of the training set for faster computation.",
        "D": "To randomly mix samples from the test set."
      },
      "correct_option": "A",
      "difficulty": "medium",
      "learning_objective": "Differentiate between validation and test sets in model selection and tuning.",
      "slide_refs": [
        26
      ]
    },
    {
      "id": "pkg8_mcq8",
      "question": "Which statement about k-fold cross-validation is true?",
      "options": {
        "A": "It is used only when datasets are extremely large.",
        "B": "Each observation is used once for validation and multiple times for training.",
        "C": "It is identical to leave-one-out validation regardless of k.",
        "D": "It prevents any need for a separate test set."
      },
      "correct_option": "B",
      "difficulty": "medium",
      "learning_objective": "Understand how k-fold cross-validation balances training and testing efficiency.",
      "slide_refs": [
        28
      ]
    },
    {
      "id": "pkg8_mcq9",
      "question": "Leave-one-out validation is particularly characterized by:",
      "options": {
        "A": "High computational cost for large datasets.",
        "B": "Using 50% of data for training and 50% for testing.",
        "C": "Strong bias due to limited resampling.",
        "D": "Completely eliminating variance in results."
      },
      "correct_option": "A",
      "difficulty": "medium",
      "learning_objective": "Recognize the strengths and limitations of leave-one-out validation.",
      "slide_refs": [
        29
      ]
    },
    {
      "id": "pkg8_mcq10",
      "question": "Population models differ from individual models in that:",
      "options": {
        "A": "Population models generalize better across individuals.",
        "B": "Individual models require fewer observations.",
        "C": "Population models are prone to overfitting individual-level noise.",
        "D": "Individual models are always less interpretable."
      },
      "correct_option": "A",
      "difficulty": "medium",
      "learning_objective": "Compare individual-level and population-level models regarding generalization.",
      "slide_refs": [
        30
      ]
    },
    {
      "id": "pkg8_mcq11",
      "question": "When handling temporally dependent data, why is random sampling inappropriate for creating training and test sets?",
      "options": {
        "A": "It causes information leakage between temporally related samples.",
        "B": "It increases model bias but not variance.",
        "C": "Temporal data do not require validation.",
        "D": "It ensures test data contain only anomalies."
      },
      "correct_option": "A",
      "difficulty": "hard",
      "learning_objective": "Identify correct data partitioning strategies for time-dependent datasets.",
      "slide_refs": [
        36,
        37
      ]
    },
    {
      "id": "pkg8_mcq12",
      "question": "What is the main goal of feature extraction using sliding windows in time series data?",
      "options": {
        "A": "To transform raw data into fixed-length statistical features.",
        "B": "To increase data redundancy for training.",
        "C": "To segment continuous data into random samples.",
        "D": "To remove all temporal dependencies between samples."
      },
      "correct_option": "A",
      "difficulty": "medium",
      "learning_objective": "Explain feature extraction through segmentation and summarize its role in model efficiency.",
      "slide_refs": [
        37
      ]
    },
    {
      "id": "pkg8_mcq13",
      "question": "According to the lecture, simpler models tend to generalize better because:",
      "options": {
        "A": "They have more parameters to capture complex data patterns.",
        "B": "They reduce overfitting and improve interpretability.",
        "C": "They use less training data and lower variance estimates.",
        "D": "They assume perfect model fit."
      },
      "correct_option": "B",
      "difficulty": "medium",
      "learning_objective": "Understand the relationship between model complexity, overfitting, and generalization.",
      "slide_refs": [
        39
      ]
    },
    {
      "id": "pkg6_mcq1",
      "question": "Which of the following best defines 'attribute noise' in data mining?",
      "options": {
        "A": "Errors or missing values in data features rather than in class labels.",
        "B": "Incorrect class labels due to human bias.",
        "C": "Inconsistencies between data sources during merging.",
        "D": "Random variations in signal saturation levels."
      },
      "correct_option": "A",
      "difficulty": "easy",
      "learning_objective": "Identify different types of noise in data, particularly attribute and label noise.",
      "slide_refs": [
        3
      ]
    },
    {
      "id": "pkg6_mcq2",
      "question": "Which of the following approaches focuses on correcting noisy instances before model training?",
      "options": {
        "A": "Data polishing",
        "B": "Noise filtering",
        "C": "Robust learning",
        "D": "Feature normalization"
      },
      "correct_option": "A",
      "difficulty": "medium",
      "learning_objective": "Understand the role of data polishing as a preprocessing technique to handle noise.",
      "slide_refs": [
        4
      ]
    },
    {
      "id": "pkg6_mcq3",
      "question": "Noise filters that target specific frequencies, such as 50 Hz from power lines, primarily operate on which kind of data?",
      "options": {
        "A": "Categorical data",
        "B": "Continuous signals",
        "C": "Textual datasets",
        "D": "Sparse matrices"
      },
      "correct_option": "B",
      "difficulty": "medium",
      "learning_objective": "Recognize noise filtering applications in continuous signal data.",
      "slide_refs": [
        5
      ]
    },
    {
      "id": "pkg6_mcq4",
      "question": "What characterizes signal saturation during data collection?",
      "options": {
        "A": "A signal’s amplitude exceeds its maximum measurable value, causing flat peaks at threshold limits.",
        "B": "Signal values are randomly distributed across the expected range.",
        "C": "The sampling rate is too low to capture full waveform detail.",
        "D": "The sensor produces noisy outputs due to temperature drift."
      },
      "correct_option": "A",
      "difficulty": "easy",
      "learning_objective": "Explain the concept and implications of signal saturation in sensor data.",
      "slide_refs": [
        7
      ]
    },
    {
      "id": "pkg6_mcq5",
      "question": "What distinguishes an anomaly from an outlier according to the lecture?",
      "options": {
        "A": "An anomaly is always erroneous; an outlier is usually valid.",
        "B": "An anomaly represents a rare but valid event, while an outlier may be noise or error.",
        "C": "Anomalies and outliers are statistically identical concepts.",
        "D": "Anomalies occur only in categorical data."
      },
      "correct_option": "B",
      "difficulty": "medium",
      "learning_objective": "Differentiate between anomalies and outliers in the context of data mining.",
      "slide_refs": [
        9
      ]
    },
    {
      "id": "pkg6_mcq6",
      "question": "Which of the following describes the correct workflow for handling harmful outliers?",
      "options": {
        "A": "Detect → Normalize → Delete",
        "B": "Investigate → Correct → Recollect",
        "C": "Detect → Investigate → Correct or treat as missing",
        "D": "Ignore → Smooth → Recluster"
      },
      "correct_option": "C",
      "difficulty": "medium",
      "learning_objective": "Apply systematic procedures for identifying and managing harmful outliers.",
      "slide_refs": [
        12
      ]
    },
    {
      "id": "pkg6_mcq7",
      "question": "A group of points that collectively deviates from the overall dataset, but whose individual members may not be extreme, are called:",
      "options": {
        "A": "Global outliers",
        "B": "Contextual outliers",
        "C": "Collective outliers",
        "D": "Statistical residuals"
      },
      "correct_option": "C",
      "difficulty": "medium",
      "learning_objective": "Classify outlier types and identify examples of collective outliers.",
      "slide_refs": [
        14
      ]
    },
    {
      "id": "pkg6_mcq8",
      "question": "Why is building a universal model for outlier detection often impractical?",
      "options": {
        "A": "Data distributions and contexts vary widely across applications.",
        "B": "All datasets contain perfectly labeled outliers.",
        "C": "Outlier thresholds are globally defined constants.",
        "D": "Outlier detection always requires supervised learning."
      },
      "correct_option": "A",
      "difficulty": "medium",
      "learning_objective": "Understand why application-specific context complicates outlier detection.",
      "slide_refs": [
        15
      ]
    },
    {
      "id": "pkg6_mcq9",
      "question": "Which of the following statements about supervised outlier detection is correct?",
      "options": {
        "A": "It trains only on outliers to classify normal data.",
        "B": "It uses labeled data to distinguish normal from abnormal patterns.",
        "C": "It requires no labeled examples at all.",
        "D": "It assumes all outliers are randomly distributed."
      },
      "correct_option": "B",
      "difficulty": "medium",
      "learning_objective": "Explain how supervised learning can be used for outlier detection.",
      "slide_refs": [
        16
      ]
    },
    {
      "id": "pkg6_mcq10",
      "question": "Which of the following is an example of a nonparametric statistical outlier detection method?",
      "options": {
        "A": "Gaussian distribution modeling",
        "B": "Kernel density estimation",
        "C": "Linear regression residual analysis",
        "D": "Support Vector Machine"
      },
      "correct_option": "B",
      "difficulty": "hard",
      "learning_objective": "Differentiate between parametric and nonparametric statistical approaches to outlier detection.",
      "slide_refs": [
        17
      ]
    },
    {
      "id": "pkg6_mcq11",
      "question": "What is the key idea behind proximity-based outlier detection methods?",
      "options": {
        "A": "Outliers are data points with significantly fewer neighbors within a certain distance threshold.",
        "B": "Outliers form dense clusters that differ from normal data clusters.",
        "C": "Outliers always belong to a specific known distribution.",
        "D": "Outliers are identified using time-series autocorrelation."
      },
      "correct_option": "A",
      "difficulty": "medium",
      "learning_objective": "Describe the principle behind distance and density-based proximity methods.",
      "slide_refs": [
        18
      ]
    },
    {
      "id": "pkg6_mcq12",
      "question": "Which method can identify both global and local outliers by comparing densities in local neighborhoods?",
      "options": {
        "A": "k-Nearest Neighbor (kNN)",
        "B": "Local Outlier Factor (LOF)",
        "C": "Gaussian Mixture Model (GMM)",
        "D": "Principal Component Analysis (PCA)"
      },
      "correct_option": "B",
      "difficulty": "hard",
      "learning_objective": "Explain the Local Outlier Factor method for density-based detection.",
      "slide_refs": [
        18
      ]
    },
    {
      "id": "pkg6_mcq13",
      "question": "In clustering-based outlier detection, normal data typically belong to:",
      "options": {
        "A": "Sparse, scattered clusters",
        "B": "Large and dense clusters",
        "C": "No clusters at all",
        "D": "Temporal patterns in time series"
      },
      "correct_option": "B",
      "difficulty": "medium",
      "learning_objective": "Understand how clustering methods distinguish normal data from outliers.",
      "slide_refs": [
        19
      ]
    },
    {
      "id": "pkg6_mcq14",
      "question": "Which classification-based approach models only the normal class to detect outliers?",
      "options": {
        "A": "Random Forest Classifier",
        "B": "One-Class Support Vector Machine (SVM)",
        "C": "Naïve Bayes Classifier",
        "D": "Logistic Regression"
      },
      "correct_option": "B",
      "difficulty": "medium",
      "learning_objective": "Recognize the function of one-class models in outlier detection.",
      "slide_refs": [
        19
      ]
    },
    {
      "id": "pkg6_mcq15",
      "question": "Which of the following is most applicable for real-time outlier detection in time series data?",
      "options": {
        "A": "Autoregressive models and streaming analysis",
        "B": "Balanced sampling and cross-validation",
        "C": "Principal component projection",
        "D": "Batch-mode K-Means clustering"
      },
      "correct_option": "A",
      "difficulty": "medium",
      "learning_objective": "Identify time-series methods suitable for streaming outlier detection.",
      "slide_refs": [
        20
      ]
    },
    {
      "id": "pkg7_mcq1",
      "question": "Why is normalization often performed after outlier detection and noise removal?",
      "options": {
        "A": "It introduces artificial variation to enhance learning complexity.",
        "B": "It improves model performance, stabilizes learning, and simplifies result interpretation.",
        "C": "It reduces data dimensionality through feature extraction.",
        "D": "It ensures outliers remain visible in data visualization."
      },
      "correct_option": "B",
      "difficulty": "medium",
      "learning_objective": "Understand why normalization follows noise and outlier handling in preprocessing pipelines.",
      "slide_refs": [
        3
      ]
    },
    {
      "id": "pkg7_mcq2",
      "question": "What is the purpose of normalization or scaling in data preprocessing?",
      "options": {
        "A": "To convert categorical data to numeric codes.",
        "B": "To adjust values on different scales to a common range.",
        "C": "To remove multicollinearity among variables.",
        "D": "To enforce Gaussian distribution on non-normal data."
      },
      "correct_option": "B",
      "difficulty": "easy",
      "learning_objective": "Define normalization and identify its primary goal in data preprocessing.",
      "slide_refs": [
        5
      ]
    },
    {
      "id": "pkg7_mcq3",
      "question": "Which normalization technique rescales values to the range [0, 1]?",
      "options": {
        "A": "Z-score standardization",
        "B": "Min-Max normalization",
        "C": "Decimal scaling",
        "D": "Box-Cox transformation"
      },
      "correct_option": "B",
      "difficulty": "easy",
      "learning_objective": "Differentiate between common normalization and scaling methods.",
      "slide_refs": [
        9
      ]
    },
    {
      "id": "pkg7_mcq4",
      "question": "Z-score standardization is particularly suitable when:",
      "options": {
        "A": "Minimum and maximum population values are unknown.",
        "B": "All variables are categorical.",
        "C": "Data contains negative values that must be converted to positive.",
        "D": "The dataset is uniform without outliers."
      },
      "correct_option": "A",
      "difficulty": "medium",
      "learning_objective": "Identify scenarios where Z-score standardization is preferable.",
      "slide_refs": [
        10
      ]
    },
    {
      "id": "pkg7_mcq5",
      "question": "Decimal scaling normalization works by:",
      "options": {
        "A": "Subtracting the mean and dividing by standard deviation.",
        "B": "Shifting the decimal point based on the largest absolute value.",
        "C": "Dividing all data by the mean of the variable.",
        "D": "Applying logarithmic transformation for skewed data."
      },
      "correct_option": "B",
      "difficulty": "medium",
      "learning_objective": "Explain the process of decimal scaling normalization.",
      "slide_refs": [
        11
      ]
    },
    {
      "id": "pkg7_mcq6",
      "question": "When is it appropriate to discretize a continuous variable?",
      "options": {
        "A": "When researchers believe distinct groups exist or categorical methods are required.",
        "B": "When there are no meaningful patterns in the data.",
        "C": "When the data already consists of ordinal variables.",
        "D": "Only when missing values must be replaced."
      },
      "correct_option": "A",
      "difficulty": "medium",
      "learning_objective": "Recognize justifiable uses of discretization in data preprocessing.",
      "slide_refs": [
        15
      ]
    },
    {
      "id": "pkg7_mcq7",
      "question": "What is a major drawback of dichotomizing continuous variables?",
      "options": {
        "A": "It introduces nonlinearity into the dataset.",
        "B": "It causes loss of information and potential misclassification.",
        "C": "It prevents visualizations such as scatter plots.",
        "D": "It increases variance and statistical noise."
      },
      "correct_option": "B",
      "difficulty": "medium",
      "learning_objective": "Evaluate the trade-offs associated with discretization or dichotomization.",
      "slide_refs": [
        19,
        20
      ]
    },
    {
      "id": "pkg7_mcq8",
      "question": "Which of the following methods can handle categorical variables in regression models?",
      "options": {
        "A": "Principal Component Analysis",
        "B": "Box-Cox transformation",
        "C": "Dummy coding",
        "D": "Logarithmic scaling"
      },
      "correct_option": "C",
      "difficulty": "easy",
      "learning_objective": "Understand how dummy variables are used for categorical data in regression.",
      "slide_refs": [
        23
      ]
    },
    {
      "id": "pkg7_mcq9",
      "question": "What is the main goal of data reduction?",
      "options": {
        "A": "To simplify data while maintaining analytical integrity.",
        "B": "To remove all redundant and correlated features.",
        "C": "To discard irrelevant data to fit memory constraints only.",
        "D": "To normalize data to Gaussian distribution."
      },
      "correct_option": "A",
      "difficulty": "medium",
      "learning_objective": "Explain the purpose of data reduction in data mining workflows.",
      "slide_refs": [
        25
      ]
    },
    {
      "id": "pkg7_mcq10",
      "question": "Which data reduction method groups continuous values into intervals to simplify analysis?",
      "options": {
        "A": "Aggregation",
        "B": "Binning",
        "C": "Clustering",
        "D": "Sampling"
      },
      "correct_option": "B",
      "difficulty": "easy",
      "learning_objective": "Identify common value reduction methods such as binning and aggregation.",
      "slide_refs": [
        26
      ]
    },
    {
      "id": "pkg7_mcq11",
      "question": "Principal Component Analysis (PCA) primarily aims to:",
      "options": {
        "A": "Increase the dimensionality of feature space.",
        "B": "Select the most correlated variables.",
        "C": "Reduce dimensionality by identifying orthogonal components explaining maximum variance.",
        "D": "Convert categorical variables into dummy variables."
      },
      "correct_option": "C",
      "difficulty": "hard",
      "learning_objective": "Describe the goal and process of PCA for dimensionality reduction.",
      "slide_refs": [
        31
      ]
    },
    {
      "id": "pkg7_mcq12",
      "question": "Which of the following statements correctly differentiates PCA and Discrete Wavelet Transform (DWT)?",
      "options": {
        "A": "PCA uses variance-based selection; DWT uses coefficient magnitude-based selection.",
        "B": "PCA requires categorical data; DWT works on text data only.",
        "C": "DWT is non-linear; PCA is always non-parametric.",
        "D": "Both require combining samples across all datasets."
      },
      "correct_option": "A",
      "difficulty": "hard",
      "learning_objective": "Compare PCA and DWT as dimension reduction techniques.",
      "slide_refs": [
        32
      ]
    },
    {
      "id": "pkg7_mcq13",
      "question": "Box-Cox transformation is applicable only when:",
      "options": {
        "A": "The data contains both positive and negative values.",
        "B": "The dataset contains only positive values.",
        "C": "The data is already normalized to [0,1].",
        "D": "The data is categorical."
      },
      "correct_option": "B",
      "difficulty": "medium",
      "learning_objective": "Recognize constraints and applicability of Box-Cox transformations.",
      "slide_refs": [
        36
      ]
    },
    {
      "id": "pkg7_mcq14",
      "question": "Which of the following transformations can help achieve normality when data violates Gaussian assumptions?",
      "options": {
        "A": "Min-max scaling",
        "B": "Box-Cox transformation",
        "C": "Dummy coding",
        "D": "Equal-width discretization"
      },
      "correct_option": "B",
      "difficulty": "medium",
      "learning_objective": "Apply transformation techniques to achieve data normality for parametric analysis.",
      "slide_refs": [
        35,
        36
      ]
    },
    {
      "id": "pkg10_mcq1",
      "question": "The pinhole camera model assumes that a 3D world point \\((X, Y, Z)\\) projects to an image point \\((x, y)\\) through perspective projection. Under the standard pinhole model with the camera center at the origin and image plane at \\(Z = f\\), the correct projection equations are:",
      "options": {
        "A": "\\(x = X/Z, \\; y = Y/Z\\)",
        "B": "\\(x = f \\cdot X/Y, \\; y = f \\cdot Z/Y\\)",
        "C": "\\(x = f \\cdot X/Z, \\; y = f \\cdot Y/Z\\)",
        "D": "\\(x = f \\cdot Y/Z, \\; y = f \\cdot X/Z\\)"
      },
      "correct_option": "C",
      "difficulty": "easy",
      "learning_objective": "Recall the basic perspective projection equations of the pinhole camera model.",
      "slide_refs": [
        8
      ]
    },
    {
      "id": "pkg10_mcq2",
      "question": "In homogeneous coordinates, the full 3×4 camera projection matrix \\(P\\) for a pinhole camera with intrinsic matrix \\(K\\) and extrinsic parameters \\([R \\; t]\\) is given by:",
      "options": {
        "A": "\\(P = K [R \\; t]\\)",
        "B": "\\(P = [R \\; t] K\\)",
        "C": "\\(P = K [I \\; 0] [R \\; t]\\)",
        "D": "\\(P = [K \\; 0] [R \\; t]\\)"
      },
      "correct_option": "A",
      "difficulty": "medium",
      "learning_objective": "Derive the general form of the camera projection matrix in homogeneous coordinates.",
      "slide_refs": [
        12
      ]
    },
    {
      "id": "pkg10_mcq3",
      "question": "The intrinsic parameter \\(\\alpha_x = f \\cdot m_x\\) represents:",
      "options": {
        "A": "The skew between image axes",
        "B": "The focal length in horizontal pixel units",
        "C": "The principal point offset in y-direction",
        "D": "The aspect ratio correction factor"
      },
      "correct_option": "B",
      "difficulty": "medium",
      "learning_objective": "Interpret the components of the intrinsic calibration matrix \\(K\\).",
      "slide_refs": [
        10
      ]
    },
    {
      "id": "pkg10_mcq4",
      "question": "A finite camera has exactly how many degrees of freedom in its projection matrix \\(P\\)?",
      "options": {
        "A": "6",
        "B": "9",
        "C": "11",
        "D": "12"
      },
      "correct_option": "C",
      "difficulty": "hard",
      "learning_objective": "Understand the dimensionality of general projective cameras.",
      "slide_refs": [
        14
      ]
    },
    {
      "id": "pkg10_mcq5",
      "question": "Radial distortion in real lenses causes straight lines in the world to appear curved in the image. The distortion model commonly used corrects coordinates \\((x_d, y_d)\\) to undistorted \\((x_u, y_u)\\) via:",
      "options": {
        "A": "\\(x_u = x_d (1 + k_1 r^2 + k_2 r^4)\\)",
        "B": "\\(x_d = x_u (1 + k_1 r^2 + k_2 r^4)\\)",
        "C": "\\(x_u = x_d + t_x, \\; y_u = y_d + t_y\\)",
        "D": "\\(x_u = \\alpha x_d + \\beta y_d\\)"
      },
      "correct_option": "B",
      "difficulty": "medium",
      "learning_objective": "Recall the polynomial radial distortion model and its direction of correction.",
      "slide_refs": [
        18
      ]
    },
    {
      "id": "pkg10_mcq6",
      "question": "The principal point \\((c_x, c_y)\\) is defined as:",
      "options": {
        "A": "The center of the sensor array",
        "B": "The optical axis intersection with the image plane",
        "C": "The pixel with maximum intensity",
        "D": "The vanishing point of parallel lines"
      },
      "correct_option": "B",
      "difficulty": "easy",
      "learning_objective": "Define the role of the principal point in camera calibration.",
      "slide_refs": [
        10
      ]
    },
    {
      "id": "pkg10_mcq7",
      "question": "In the pinhole model, a point at infinity in 3D (direction vector) projects to:",
      "options": {
        "A": "The image origin",
        "B": "A vanishing point in the image",
        "C": "The principal point",
        "D": "No valid image point"
      },
      "correct_option": "B",
      "difficulty": "hard",
      "learning_objective": "Explain the imaging of points at infinity and vanishing points.",
      "slide_refs": [
        15
      ]
    },
    {
      "id": "pkg10_mcq8",
      "question": "Camera calibration using a known 3D calibration object (e.g., checkerboard) typically solves for:",
      "options": {
        "A": "Only intrinsic parameters",
        "B": "Only extrinsic parameters per view",
        "C": "Both intrinsic \\(K\\) and per-view extrinsics \\(R, t\\)",
        "D": "Only radial distortion coefficients"
      },
      "correct_option": "C",
      "difficulty": "medium",
      "learning_objective": "Describe the standard camera calibration pipeline.",
      "slide_refs": [
        20
      ]
    },
    {
      "id": "pkg10_mcq9",
      "question": "The skew parameter \\(s\\) in the intrinsic matrix is zero when:",
      "options": {
        "A": "Pixel aspect ratio is 1:1",
        "B": "Image axes are orthogonal (no shearing)",
        "C": "Focal length equals sensor size",
        "D": "Lens has no radial distortion"
      },
      "correct_option": "B",
      "difficulty": "hard",
      "learning_objective": "Interpret the skew parameter in modern digital cameras.",
      "slide_refs": [
        10
      ]
    },
    {
      "id": "pkg10_mcq10",
      "question": "The weak perspective (scaled orthographic) approximation is valid when:",
      "options": {
        "A": "Scene depth variation is small compared to average distance to camera",
        "B": "Camera is at infinity",
        "C": "Objects are perfectly planar",
        "D": "Lens distortion is negligible"
      },
      "correct_option": "A",
      "difficulty": "medium",
      "learning_objective": "Recognize conditions under which perspective effects can be approximated linearly.",
      "slide_refs": [
        22
      ]
    },
    {
      "id": "pkg7_mcq1",
      "question": "What is the primary goal of texture analysis in computer vision?",
      "options": {
        "A": "To reconstruct 3D geometry from a single image",
        "B": "To characterize local image patterns for segmentation, classification, or shape inference",
        "C": "To detect and track moving objects in video",
        "D": "To calibrate camera intrinsic parameters"
      },
      "correct_option": "B",
      "difficulty": "easy",
      "learning_objective": "Understand the role of texture in image interpretation.",
      "slide_refs": [
        5
      ]
    },
    {
      "id": "pkg7_mcq2",
      "question": "Texture can be used as a monocular cue for 3D shape perception because:",
      "options": {
        "A": "It remains constant under perspective projection",
        "B": "Regular texture elements distort predictably with surface orientation (slant and tilt)",
        "C": "It is invariant to lighting conditions",
        "D": "It encodes depth directly in intensity"
      },
      "correct_option": "B",
      "difficulty": "medium",
      "learning_objective": "Explain how texture gradient provides surface orientation cues.",
      "slide_refs": [
        6
      ]
    },
    {
      "id": "pkg7_mcq3",
      "question": "In the context of shape from texture, the two key parameters describing surface orientation are:",
      "options": {
        "A": "Scale and rotation",
        "B": "Slant (\\(\\phi\\)) and tilt (\\(\\tau\\))",
        "C": "Translation and shear",
        "D": "Foreshortening and compression"
      },
      "correct_option": "B",
      "difficulty": "easy",
      "learning_objective": "Recall the geometric parameters in texture-based surface estimation.",
      "slide_refs": [
        7
      ]
    },
    {
      "id": "pkg7_mcq4",
      "question": "Under the isotropy assumption in shape from texture, the texture elements are:",
      "options": {
        "A": "Perfectly circular in 3D",
        "B": "Statistically identical in all directions on the surface",
        "C": "Aligned with the image axes",
        "D": "Projected with uniform scale"
      },
      "correct_option": "B",
      "difficulty": "medium",
      "learning_objective": "State the isotropy assumption and its implication for foreshortening.",
      "slide_refs": [
        8
      ]
    },
    {
      "id": "pkg7_mcq5",
      "question": "The foreshortening effect in projected texture causes:",
      "options": {
        "A": "Texture elements to appear larger when surface is tilted away",
        "B": "Compression of texture in the direction of maximum slant",
        "C": "Uniform scaling in all directions",
        "D": "Rotation without distortion"
      },
      "correct_option": "B",
      "difficulty": "medium",
      "learning_objective": "Describe how slant affects texture element appearance.",
      "slide_refs": [
        9
      ]
    },
    {
      "id": "pkg7_mcq6",
      "question": "Which of the following is a common method to estimate local texture distortion for shape from texture?",
      "options": {
        "A": "Applying global Fourier transform",
        "B": "Fitting ellipses to the autocorrelation or power spectrum of local patches",
        "C": "Using Harris corner detection",
        "D": "Computing optical flow"
      },
      "correct_option": "B",
      "difficulty": "hard",
      "learning_objective": "Identify spectral or statistical methods for measuring texture anisotropy.",
      "slide_refs": [
        12
      ]
    },
    {
      "id": "pkg7_mcq7",
      "question": "The tilt angle \\(\\tau\\) in shape from texture corresponds to:",
      "options": {
        "A": "The magnitude of surface slant",
        "B": "The direction in the image plane toward which the surface is most slanted",
        "C": "The scale factor of texture elements",
        "D": "The camera focal length"
      },
      "correct_option": "B",
      "difficulty": "medium",
      "learning_objective": "Differentiate between slant and tilt in surface orientation.",
      "slide_refs": [
        7
      ]
    },
    {
      "id": "pkg7_mcq8",
      "question": "A major limitation of shape from texture methods is:",
      "options": {
        "A": "They require stereo image pairs",
        "B": "They assume texture is regular and isotropic in 3D, which is often violated",
        "C": "They cannot handle colored textures",
        "D": "They are computationally more expensive than shape from shading"
      },
      "correct_option": "B",
      "difficulty": "hard",
      "learning_objective": "Recognize real-world violations of texture assumptions.",
      "slide_refs": [
        15
      ]
    },
    {
      "id": "pkg7_mcq9",
      "question": "In texture segmentation, co-occurrence matrices are used to capture:",
      "options": {
        "A": "Global frequency content",
        "B": "Second-order statistics of intensity pairs at specific offsets",
        "C": "Edge orientations only",
        "D": "Motion vectors"
      },
      "correct_option": "B",
      "difficulty": "medium",
      "learning_objective": "Recall classical texture descriptors based on spatial relationships.",
      "slide_refs": [
        18
      ]
    },
    {
      "id": "pkg7_mcq10",
      "question": "Which of the following is NOT a typical application of texture analysis mentioned in the lecture?",
      "options": {
        "A": "Material classification",
        "B": "Image retrieval by content",
        "C": "Absolute camera pose estimation",
        "D": "Surface defect detection"
      },
      "correct_option": "C",
      "difficulty": "easy",
      "learning_objective": "Identify standard use cases of texture in vision systems.",
      "slide_refs": [
        5
      ]
    },
    {
      "id": "pkg8_mcq1",
      "question": "What is the primary purpose of binarization in image processing?",
      "options": {
        "A": "To enhance color saturation",
        "B": "To separate foreground objects from the background using a single threshold",
        "C": "To compute optical flow",
        "D": "To perform 3D reconstruction"
      },
      "correct_option": "B",
      "difficulty": "easy",
      "learning_objective": "Understand the goal of converting a grayscale image into a binary representation.",
      "slide_refs": [
        5
      ]
    },
    {
      "id": "pkg8_mcq2",
      "question": "In global thresholding, the binary image \\(B(x,y)\\) is defined as:",
      "options": {
        "A": "\\(B(x,y) = I(x,y) > T\\)",
        "B": "\\(B(x,y) = I(x,y) \\times T\\)",
        "C": "\\(B(x,y) = \\max(I(x,y), T)\\)",
        "D": "\\(B(x,y) = I(x,y)^T\\)"
      },
      "correct_option": "A",
      "difficulty": "easy",
      "learning_objective": "Recall the mathematical formulation of thresholding.",
      "slide_refs": [
        6
      ]
    },
    {
      "id": "pkg8_mcq3",
      "question": "Which of the following is a common challenge when applying a fixed global threshold?",
      "options": {
        "A": "Perfect separation in all lighting conditions",
        "B": "Non-uniform illumination causing intensity variation across the image",
        "C": "Too many connected components",
        "D": "Loss of texture details only in color images"
      },
      "correct_option": "B",
      "difficulty": "medium",
      "learning_objective": "Identify limitations of global thresholding in real-world images.",
      "slide_refs": [
        7
      ]
    },
    {
      "id": "pkg8_mcq4",
      "question": "The 4-connected neighborhood of a pixel \\((x,y)\\) includes:",
      "options": {
        "A": "Only the pixel itself",
        "B": "Pixels at \\((x\\pm1, y)\\) and \\((x, y\\pm1)\\)",
        "C": "All eight surrounding pixels",
        "D": "Diagonal pixels only"
      },
      "correct_option": "B",
      "difficulty": "easy",
      "learning_objective": "Define pixel connectivity in binary images.",
      "slide_refs": [
        10
      ]
    },
    {
      "id": "pkg8_mcq5",
      "question": "In connected component labeling, a foreground object is defined as:",
      "options": {
        "A": "A maximal set of background pixels",
        "B": "A maximal 4- or 8-connected set of foreground pixels",
        "C": "Any pixel with intensity above 128",
        "D": "The boundary between foreground and background"
      },
      "correct_option": "B",
      "difficulty": "medium",
      "learning_objective": "Understand the concept of connected components in binary segmentation.",
      "slide_refs": [
        12
      ]
    },
    {
      "id": "pkg8_mcq6",
      "question": "Which morphological operation uses a structuring element to remove small noise blobs in a binary image?",
      "options": {
        "A": "Dilation",
        "B": "Erosion",
        "C": "Opening",
        "D": "Closing"
      },
      "correct_option": "C",
      "difficulty": "medium",
      "learning_objective": "Apply morphological operations to clean binary images.",
      "slide_refs": [
        15
      ]
    },
    {
      "id": "pkg8_mcq7",
      "question": "The operation of closing in binary morphology is equivalent to:",
      "options": {
        "A": "Erosion followed by dilation",
        "B": "Dilation followed by erosion",
        "C": "Opening followed by thresholding",
        "D": "Boundary extraction"
      },
      "correct_option": "B",
      "difficulty": "medium",
      "learning_objective": "Recall the sequence of operations in morphological closing.",
      "slide_refs": [
        16
      ]
    },
    {
      "id": "pkg8_mcq8",
      "question": "In the context of document image analysis, binarization is crucial for:",
      "options": {
        "A": "Color restoration",
        "B": "Enabling OCR by isolating text from paper background",
        "C": "Estimating camera motion",
        "D": "3D surface reconstruction"
      },
      "correct_option": "B",
      "difficulty": "easy",
      "learning_objective": "Recognize application-specific importance of binary processing.",
      "slide_refs": [
        18
      ]
    },
    {
      "id": "pkg8_mcq9",
      "question": "The two-pass algorithm for connected component labeling works by:",
      "options": {
        "A": "Scanning the image once and assigning final labels immediately",
        "B": "First assigning provisional labels and resolving equivalences in a second pass using Union-Find",
        "C": "Using depth-first search on the fly",
        "D": "Applying morphological dilation repeatedly"
      },
      "correct_option": "B",
      "difficulty": "hard",
      "learning_objective": "Describe efficient algorithms for labeling connected regions.",
      "slide_refs": [
        13
      ]
    },
    {
      "id": "pkg8_mcq10",
      "question": "Which of the following is NOT a typical post-processing step after binarization?",
      "options": {
        "A": "Morphological opening to remove salt noise",
        "B": "Connected component analysis to extract text lines",
        "C": "Applying Gaussian blur to restore gradients",
        "D": "Hole filling via morphological closing"
      },
      "correct_option": "C",
      "difficulty": "hard",
      "learning_objective": "Distinguish appropriate binary image cleanup operations.",
      "slide_refs": [
        17
      ]
    },
    {
      "id": "pkg9_mcq1",
      "question": "What are the three primary additive colors used in digital color displays?",
      "options": {
        "A": "Cyan, Magenta, Yellow",
        "B": "Red, Green, Blue",
        "C": "Hue, Saturation, Value",
        "D": "Lightness, a, b"
      },
      "correct_option": "B",
      "difficulty": "easy",
      "learning_objective": "Recall the basis of the RGB color model for image representation.",
      "slide_refs": [
        5
      ]
    },
    {
      "id": "pkg9_mcq2",
      "question": "In the CIE 1931 chromaticity diagram, the horseshoe-shaped boundary represents:",
      "options": {
        "A": "All possible RGB values",
        "B": "The locus of pure spectral colors (monochromatic light)",
        "C": "The gamut of typical CMYK printers",
        "D": "Perceptual uniformity in color differences"
      },
      "correct_option": "B",
      "difficulty": "medium",
      "learning_objective": "Understand the spectral locus in the standard chromaticity space.",
      "slide_refs": [
        10
      ]
    },
    {
      "id": "pkg9_mcq3",
      "question": "The dichromatic reflection model separates surface reflectance into:",
      "options": {
        "A": "Diffuse and specular components only",
        "B": "Body (diffuse) reflection and interface (specular) reflection",
        "C": "Ambient and directed lighting",
        "D": "Matte and glossy albedo"
      },
      "correct_option": "B",
      "difficulty": "medium",
      "learning_objective": "Recall the two-term reflection model for color highlight analysis.",
      "slide_refs": [
        15
      ]
    },
    {
      "id": "pkg9_mcq4",
      "question": "In the Shafer dichromatic model, specular highlights from a white light source on a colored surface lie along a line in RGB space that:",
      "options": {
        "A": "Passes through the origin",
        "B": "Is parallel to the lighting color vector",
        "C": "Connects the surface body color to the illuminant color",
        "D": "Is perpendicular to the surface normal"
      },
      "correct_option": "C",
      "difficulty": "hard",
      "learning_objective": "Explain the geometry of specular highlights in RGB color space.",
      "slide_refs": [
        16
      ]
    },
    {
      "id": "pkg9_mcq5",
      "question": "Which color space is designed to be perceptually uniform, meaning equal Euclidean distances correspond roughly to equal perceived color differences?",
      "options": {
        "A": "RGB",
        "B": "HSV",
        "C": "CIE L*a*b*",
        "D": "YCbCr"
      },
      "correct_option": "C",
      "difficulty": "medium",
      "learning_objective": "Identify color spaces intended for perceptual uniformity.",
      "slide_refs": [
        12
      ]
    },
    {
      "id": "pkg9_mcq6",
      "question": "The phenomenon where the perceived color of a surface remains relatively constant under changing illumination is known as:",
      "options": {
        "A": "Color quantization",
        "B": "Color constancy",
        "C": "Metamerism",
        "D": "Gamut mapping"
      },
      "correct_option": "B",
      "difficulty": "easy",
      "learning_objective": "Define the human visual system's ability to discount illuminant changes.",
      "slide_refs": [
        20
      ]
    },
    {
      "id": "pkg9_mcq7",
      "question": "A simple algorithm for computational color constancy estimates the illuminant by assuming:",
      "options": {
        "A": "The brightest pixel is a specular highlight from white light",
        "B": "The average color of the image is gray (Gray World assumption)",
        "C": "The darkest pixel represents black shadow",
        "D": "All colors are equally likely"
      },
      "correct_option": "B",
      "difficulty": "medium",
      "learning_objective": "Recall the Gray World hypothesis for illuminant estimation.",
      "slide_refs": [
        22
      ]
    },
    {
      "id": "pkg9_mcq8",
      "question": "In the context of specular highlight removal, clustering in RGB space works because:",
      "options": {
        "A": "Diffuse pixels form a compact cluster; specular pixels form a separate skewed cluster",
        "B": "All pixels lie on the Planckian locus",
        "C": "Specularities reduce saturation uniformly",
        "D": "Highlights are always white regardless of surface color"
      },
      "correct_option": "A",
      "difficulty": "hard",
      "learning_objective": "Apply the dichromatic model to separate reflection components via clustering.",
      "slide_refs": [
        18
      ]
    },
    {
      "id": "pkg9_mcq9",
      "question": "The conversion from RGB to HSV separates color into:",
      "options": {
        "A": "Luminance, chrominance, and saturation",
        "B": "Hue (dominant wavelength), Saturation (purity), Value (brightness)",
        "C": "Opponent red-green, yellow-blue, and light-dark channels",
        "D": "CIELAB lightness and chromatic opponents"
      },
      "correct_option": "B",
      "difficulty": "easy",
      "learning_objective": "Describe the intuitive components of the HSV color representation.",
      "slide_refs": [
        8
      ]
    },
    {
      "id": "pkg9_mcq10",
      "question": "Metamerism occurs when:",
      "options": {
        "A": "Two different spectral distributions produce the same tristimulus values (RGB)",
        "B": "A single surface appears different under two illuminants",
        "C": "The camera sensor has non-ideal spectral sensitivities",
        "D": "White balancing fails in post-processing"
      },
      "correct_option": "A",
      "difficulty": "hard",
      "learning_objective": "Understand the fundamental reason for color matching ambiguities.",
      "slide_refs": [
        9
      ]
    },
    {
      "id": "pkg15_mcq1",
      "question": "A time-of-flight camera measures:",
      "options": {
        "A": "Depth via light travel time",
        "B": "Color spectra",
        "C": "Motion vectors",
        "D": "Texture patterns"
      },
      "correct_option": "A",
      "difficulty": "easy",
      "learning_objective": "Define ToF depth sensing.",
      "slide_refs": [
        1
      ]
    },
    {
      "id": "pkg15_mcq2",
      "question": "Metamers are spectra that:",
      "options": {
        "A": "Match in color appearance but differ physically",
        "B": "Cause distortion",
        "C": "Define camera calibration",
        "D": "Represent textures"
      },
      "correct_option": "A",
      "difficulty": "medium",
      "learning_objective": "Explain metamerism.",
      "slide_refs": [
        1
      ]
    },
    {
      "id": "pkg15_mcq3",
      "question": "Connected component labeling assigns:",
      "options": {
        "A": "Unique IDs to contiguous foreground regions",
        "B": "Gradients to edges",
        "C": "Colors to pixels",
        "D": "Depths to points"
      },
      "correct_option": "A",
      "difficulty": "medium",
      "learning_objective": "Describe CCL in binary images.",
      "slide_refs": [
        1
      ]
    },
    {
      "id": "pkg15_mcq4",
      "question": "A local descriptor describes:",
      "options": {
        "A": "Neighborhood around a feature point",
        "B": "Global image",
        "C": "Camera pose",
        "D": "Light source"
      },
      "correct_option": "A",
      "difficulty": "easy",
      "learning_objective": "Define local descriptors.",
      "slide_refs": [
        1
      ]
    },
    {
      "id": "pkg15_mcq5",
      "question": "Motion parallax provides:",
      "options": {
        "A": "Depth cues from relative motion",
        "B": "Color constancy",
        "C": "Texture classification",
        "D": "Edge detection"
      },
      "correct_option": "A",
      "difficulty": "medium",
      "learning_objective": "Explain parallax as monocular cue.",
      "slide_refs": [
        1
      ]
    },
    {
      "id": "pkg15_mcq6",
      "question": "An epipolar line is:",
      "options": {
        "A": "The image of the ray from one camera in the other",
        "B": "A motion trajectory",
        "C": "A texture gradient",
        "D": "A color channel"
      },
      "correct_option": "A",
      "difficulty": "hard",
      "learning_objective": "Define epipolar lines.",
      "slide_refs": [
        1
      ]
    },
    {
      "id": "pkg15_mcq7",
      "question": "K-means minimizes:",
      "options": {
        "A": "Within-cluster sum of squares",
        "B": "Between-class variance",
        "C": "Edge strengths",
        "D": "Color differences"
      },
      "correct_option": "A",
      "difficulty": "medium",
      "learning_objective": "Principle of k-means.",
      "slide_refs": [
        1
      ]
    },
    {
      "id": "pkg15_mcq8",
      "question": "Photometric stereo recovers:",
      "options": {
        "A": "Normals from multiple illuminations",
        "B": "Motion from video",
        "C": "Textures from filters",
        "D": "Colors from metamer matching"
      },
      "correct_option": "A",
      "difficulty": "hard",
      "learning_objective": "Describe photometric stereo.",
      "slide_refs": [
        1
      ]
    },
    {
      "id": "pkg15_mcq9",
      "question": "Roberts gradient masks compute:",
      "options": {
        "A": "Diagonal derivatives",
        "B": "Horizontal only",
        "C": "Vertical only",
        "D": "Laplacian"
      },
      "correct_option": "A",
      "difficulty": "medium",
      "learning_objective": "Recall Roberts operator.",
      "slide_refs": [
        1
      ]
    },
    {
      "id": "pkg15_mcq10",
      "question": "LBP for texture classification uses:",
      "options": {
        "A": "Histogram of binary patterns",
        "B": "Mean gradients",
        "C": "Color histograms",
        "D": "3D models"
      },
      "correct_option": "A",
      "difficulty": "medium",
      "learning_objective": "Explain LBP-based recognition.",
      "slide_refs": [
        1
      ]
    },
    {
      "id": "pkg12_mcq1",
      "question": "What is an epipolar line in the context of stereo vision?",
      "options": {
        "A": "The line connecting the camera centers",
        "B": "The projection of the epipolar plane onto one image, constraining the search for corresponding points",
        "C": "A line of constant depth in the scene",
        "D": "The baseline between two cameras"
      },
      "correct_option": "B",
      "difficulty": "easy",
      "learning_objective": "Define the epipolar line and its role in correspondence search.",
      "slide_refs": [
        1
      ]
    },
    {
      "id": "pkg12_mcq2",
      "question": "A grayscale histogram represents:",
      "options": {
        "A": "The spatial distribution of pixels",
        "B": "The frequency of occurrence of each intensity level in the image",
        "C": "The gradient magnitudes at each pixel",
        "D": "The color channels separately"
      },
      "correct_option": "B",
      "difficulty": "easy",
      "learning_objective": "Understand the concept of image histograms.",
      "slide_refs": [
        1
      ]
    },
    {
      "id": "pkg12_mcq3",
      "question": "In HSV color space, 'H' stands for:",
      "options": {
        "A": "Hue, representing the dominant wavelength",
        "B": "Horizontal component",
        "C": "Highlight intensity",
        "D": "Histogram value"
      },
      "correct_option": "A",
      "difficulty": "easy",
      "learning_objective": "Recall the components of HSV color model.",
      "slide_refs": [
        1
      ]
    },
    {
      "id": "pkg12_mcq4",
      "question": "Optical flow estimates:",
      "options": {
        "A": "The 3D motion of the camera",
        "B": "The apparent 2D motion field in the image plane",
        "C": "Absolute depth values",
        "D": "Surface normals"
      },
      "correct_option": "B",
      "difficulty": "medium",
      "learning_objective": "Define optical flow and its computation.",
      "slide_refs": [
        1
      ]
    },
    {
      "id": "pkg12_mcq5",
      "question": "An interest point is typically:",
      "options": {
        "A": "A uniform region with no texture",
        "B": "A distinctive location in the image, such as a corner or blob, repeatable across views",
        "C": "The center of the image",
        "D": "A pixel with maximum intensity"
      },
      "correct_option": "B",
      "difficulty": "easy",
      "learning_objective": "Explain the concept of interest points or keypoints.",
      "slide_refs": [
        1
      ]
    },
    {
      "id": "pkg12_mcq6",
      "question": "Chromatic aberration is:",
      "options": {
        "A": "Distortion due to lens curvature",
        "B": "Color fringing caused by wavelength-dependent refraction in lenses",
        "C": "Noise in the sensor",
        "D": "Vignetting at image corners"
      },
      "correct_option": "B",
      "difficulty": "medium",
      "learning_objective": "Define chromatic aberration in imaging.",
      "slide_refs": [
        1
      ]
    },
    {
      "id": "pkg12_mcq7",
      "question": "RANSAC is primarily used for:",
      "options": {
        "A": "Image segmentation",
        "B": "Robust model fitting in the presence of outliers",
        "C": "Color correction",
        "D": "Edge detection"
      },
      "correct_option": "B",
      "difficulty": "medium",
      "learning_objective": "Describe the principle of RANSAC for robust estimation.",
      "slide_refs": [
        1
      ]
    },
    {
      "id": "pkg12_mcq8",
      "question": "A precision-recall curve evaluates:",
      "options": {
        "A": "Regression models",
        "B": "Binary classifiers by trading off precision and recall at different thresholds",
        "C": "Clustering quality",
        "D": "Image compression ratios"
      },
      "correct_option": "B",
      "difficulty": "medium",
      "learning_objective": "Understand precision-recall for performance evaluation.",
      "slide_refs": [
        1
      ]
    },
    {
      "id": "pkg12_mcq9",
      "question": "The SIFT descriptor is:",
      "options": {
        "A": "A global image feature",
        "B": "A local histogram-based descriptor invariant to scale and rotation",
        "C": "A color space transformation",
        "D": "A motion estimation algorithm"
      },
      "correct_option": "B",
      "difficulty": "medium",
      "learning_objective": "Recall the properties of SIFT descriptors.",
      "slide_refs": [
        1
      ]
    },
    {
      "id": "pkg12_mcq10",
      "question": "In homogeneous coordinates, the similarity transformation matrix has how many degrees of freedom?",
      "options": {
        "A": "3",
        "B": "4",
        "C": "6",
        "D": "8"
      },
      "correct_option": "B",
      "difficulty": "hard",
      "learning_objective": "Determine DOF for 2D transformations.",
      "slide_refs": [
        1
      ]
    },
    {
      "id": "pkg24_mcq1",
      "question": "Depth of field is the:",
      "options": {
        "A": "Range of distances in focus",
        "B": "Sensor size",
        "C": "Focal length",
        "D": "Aperture diameter"
      },
      "correct_option": "A",
      "difficulty": "easy",
      "learning_objective": "DOF definition.",
      "slide_refs": [
        1
      ]
    },
    {
      "id": "pkg24_mcq2",
      "question": "HSV color space:",
      "options": {
        "A": "Hue for color, value for intensity",
        "B": "RGB additive",
        "C": "CMYK subtractive",
        "D": "LAB perceptual"
      },
      "correct_option": "A",
      "difficulty": "easy",
      "learning_objective": "HSV.",
      "slide_refs": [
        1
      ]
    },
    {
      "id": "pkg24_mcq3",
      "question": "Diffuse reflection is view-independent because:",
      "options": {
        "A": "Scatters light equally",
        "B": "Reflects specularly",
        "C": "Absorbs all light",
        "D": "Transmits light"
      },
      "correct_option": "A",
      "difficulty": "medium",
      "learning_objective": "Diffuse vs specular.",
      "slide_refs": [
        1
      ]
    },
    {
      "id": "pkg24_mcq4",
      "question": "Confusion matrix tabulates:",
      "options": {
        "A": "Predictions vs actual classes",
        "B": "Feature correlations",
        "C": "Gradient directions",
        "D": "Motion fields"
      },
      "correct_option": "A",
      "difficulty": "medium",
      "learning_objective": "Matrix use.",
      "slide_refs": [
        1
      ]
    },
    {
      "id": "pkg24_mcq5",
      "question": "Local descriptor is for:",
      "options": {
        "A": "Matching local regions",
        "B": "Global alignment",
        "C": "Color balancing",
        "D": "Depth fusion"
      },
      "correct_option": "A",
      "difficulty": "easy",
      "learning_objective": "Descriptors.",
      "slide_refs": [
        1
      ]
    },
    {
      "id": "pkg24_mcq6",
      "question": "P3P problem solves:",
      "options": {
        "A": "Camera pose from 3 points",
        "B": "Stereo disparity",
        "C": "Texture classification",
        "D": "Color constancy"
      },
      "correct_option": "A",
      "difficulty": "hard",
      "learning_objective": "P3P.",
      "slide_refs": [
        1
      ]
    },
    {
      "id": "pkg24_mcq7",
      "question": "Image segmentation partitions into:",
      "options": {
        "A": "Meaningful regions",
        "B": "Random clusters",
        "C": "Individual pixels",
        "D": "Frequency bands"
      },
      "correct_option": "A",
      "difficulty": "easy",
      "learning_objective": "Segmentation.",
      "slide_refs": [
        1
      ]
    },
    {
      "id": "pkg24_mcq8",
      "question": "Optical flow for:",
      "options": {
        "A": "Motion estimation in video",
        "B": "Static images only",
        "C": "Color analysis",
        "D": "Lens calibration"
      },
      "correct_option": "A",
      "difficulty": "medium",
      "learning_objective": "Flow usage.",
      "slide_refs": [
        1
      ]
    },
    {
      "id": "pkg24_mcq9",
      "question": "Structure from motion uses:",
      "options": {
        "A": "Uncalibrated images for 3D",
        "B": "Known poses only",
        "C": "Single view",
        "D": "Shading cues"
      },
      "correct_option": "A",
      "difficulty": "hard",
      "learning_objective": "SfM.",
      "slide_refs": [
        1
      ]
    },
    {
      "id": "pkg24_mcq10",
      "question": "Essential matrix encodes:",
      "options": {
        "A": "Relative pose between views",
        "B": "Intrinsic parameters",
        "C": "Color transformation",
        "D": "Texture mapping"
      },
      "correct_option": "A",
      "difficulty": "hard",
      "learning_objective": "Essential matrix.",
      "slide_refs": [
        1
      ]
    },
    {
      "id": "pkg23_mcq1",
      "question": "The epipolar constraint states that:",
      "options": {
        "A": "Corresponding points lie on epipolar lines",
        "B": "Motion is rigid",
        "C": "Textures are isotropic",
        "D": "Colors are constant"
      },
      "correct_option": "A",
      "difficulty": "medium",
      "learning_objective": "Epipolar constraint.",
      "slide_refs": [
        1
      ]
    },
    {
      "id": "pkg23_mcq2",
      "question": "A decision tree splits data based on:",
      "options": {
        "A": "Features to minimize impurity",
        "B": "Random assignment",
        "C": "Distance metrics",
        "D": "Gradient descent"
      },
      "correct_option": "A",
      "difficulty": "medium",
      "learning_objective": "Decision trees.",
      "slide_refs": [
        1
      ]
    },
    {
      "id": "pkg23_mcq3",
      "question": "HSV is used for:",
      "options": {
        "A": "Robust color analysis under varying light",
        "B": "Depth estimation",
        "C": "Motion tracking",
        "D": "Texture synthesis"
      },
      "correct_option": "A",
      "difficulty": "easy",
      "learning_objective": "HSV usage.",
      "slide_refs": [
        1
      ]
    },
    {
      "id": "pkg23_mcq4",
      "question": "Radial distortion affects:",
      "options": {
        "A": "Peripheral image points more",
        "B": "Center only",
        "C": "Color channels",
        "D": "Brightness"
      },
      "correct_option": "A",
      "difficulty": "medium",
      "learning_objective": "Distortion effect.",
      "slide_refs": [
        1
      ]
    },
    {
      "id": "pkg23_mcq5",
      "question": "SIFT descriptor uses:",
      "options": {
        "A": "Orientation histograms in subregions",
        "B": "Global means",
        "C": "Color gradients",
        "D": "Binary codes"
      },
      "correct_option": "A",
      "difficulty": "hard",
      "learning_objective": "SIFT details.",
      "slide_refs": [
        1
      ]
    },
    {
      "id": "pkg23_mcq6",
      "question": "Diffuse reflection depends on:",
      "options": {
        "A": "Surface normal and light direction",
        "B": "View direction",
        "C": "Specular angle",
        "D": "Transparency"
      },
      "correct_option": "A",
      "difficulty": "medium",
      "learning_objective": "Lambertian model.",
      "slide_refs": [
        1
      ]
    },
    {
      "id": "pkg23_mcq7",
      "question": "Background subtraction detects:",
      "options": {
        "A": "Moving objects by frame differencing",
        "B": "Static backgrounds only",
        "C": "Color changes",
        "D": "Textures"
      },
      "correct_option": "A",
      "difficulty": "medium",
      "learning_objective": "Bg subtraction.",
      "slide_refs": [
        1
      ]
    },
    {
      "id": "pkg23_mcq8",
      "question": "Structure from motion requires:",
      "options": {
        "A": "Feature correspondences across views",
        "B": "Known depths",
        "C": "Fixed camera",
        "D": "Single image"
      },
      "correct_option": "A",
      "difficulty": "hard",
      "learning_objective": "SfM requirements.",
      "slide_refs": [
        1
      ]
    },
    {
      "id": "pkg23_mcq9",
      "question": "LBP is invariant to:",
      "options": {
        "A": "Monotonic illumination changes",
        "B": "Rotation without extension",
        "C": "Scale",
        "D": "Color"
      },
      "correct_option": "A",
      "difficulty": "medium",
      "learning_objective": "LBP invariance.",
      "slide_refs": [
        1
      ]
    },
    {
      "id": "pkg23_mcq10",
      "question": "In stereo, z = f b / d, for d=2, b=6, f=1, z=:",
      "options": {
        "A": "3",
        "B": "12",
        "C": "0.5",
        "D": "6"
      },
      "correct_option": "A",
      "difficulty": "easy",
      "learning_objective": "Depth calc."
    },
    {
      "id": "pkg22_mcq1",
      "question": "A confusion matrix is used to:",
      "options": {
        "A": "Visualize classification performance showing true and predicted labels",
        "B": "Compute image gradients",
        "C": "Estimate camera pose",
        "D": "Generate textures"
      },
      "correct_option": "A",
      "difficulty": "easy",
      "learning_objective": "Explain confusion matrix for evaluation.",
      "slide_refs": [
        1
      ]
    },
    {
      "id": "pkg22_mcq2",
      "question": "HSV color space decouples:",
      "options": {
        "A": "Color (hue, saturation) from brightness (value)",
        "B": "Red, green, blue channels",
        "C": "Opponent colors",
        "D": "Spectral wavelengths"
      },
      "correct_option": "A",
      "difficulty": "easy",
      "learning_objective": "Describe HSV components.",
      "slide_refs": [
        1
      ]
    },
    {
      "id": "pkg22_mcq3",
      "question": "The pinhole model idealizes:",
      "options": {
        "A": "Perspective projection without lens effects",
        "B": "Orthographic projection",
        "C": "Fisheye distortion",
        "D": "Chromatic effects"
      },
      "correct_option": "A",
      "difficulty": "medium",
      "learning_objective": "Define pinhole camera model.",
      "slide_refs": [
        1
      ]
    },
    {
      "id": "pkg22_mcq4",
      "question": "A feature descriptor provides:",
      "options": {
        "A": "Robust representation of local image patch",
        "B": "Global histogram",
        "C": "3D coordinates",
        "D": "Motion vectors"
      },
      "correct_option": "A",
      "difficulty": "easy",
      "learning_objective": "Role of descriptors.",
      "slide_refs": [
        1
      ]
    },
    {
      "id": "pkg22_mcq5",
      "question": "Chromatic aberration results from:",
      "options": {
        "A": "Different refraction for wavelengths",
        "B": "Sensor Bayer pattern",
        "C": "Motion blur",
        "D": "Radial warping"
      },
      "correct_option": "A",
      "difficulty": "medium",
      "learning_objective": "Cause of chromatic aberration.",
      "slide_refs": [
        1
      ]
    },
    {
      "id": "pkg22_mcq6",
      "question": "Structure-from-motion combines:",
      "options": {
        "A": "Feature tracking and bundle adjustment for 3D and poses",
        "B": "Stereo pairs only",
        "C": "Shading cues",
        "D": "Texture gradients"
      },
      "correct_option": "A",
      "difficulty": "hard",
      "learning_objective": "SfM overview.",
      "slide_refs": [
        1
      ]
    },
    {
      "id": "pkg22_mcq7",
      "question": "Optical flow assumes brightness constancy to estimate:",
      "options": {
        "A": "2D motion field",
        "B": "3D structure",
        "C": "Camera intrinsics",
        "D": "Light sources"
      },
      "correct_option": "A",
      "difficulty": "medium",
      "learning_objective": "Flow principle.",
      "slide_refs": [
        1
      ]
    },
    {
      "id": "pkg22_mcq8",
      "question": "Otsu's method finds threshold by:",
      "options": {
        "A": "Maximizing inter-class variance",
        "B": "Minimizing entropy",
        "C": "Edge detection",
        "D": "Clustering colors"
      },
      "correct_option": "A",
      "difficulty": "medium",
      "learning_objective": "Otsu algorithm.",
      "slide_refs": [
        1
      ]
    },
    {
      "id": "pkg22_mcq9",
      "question": "K-means iteratively:",
      "options": {
        "A": "Assigns points to centroids and updates",
        "B": "Builds decision trees",
        "C": "Fits lines",
        "D": "Computes gradients"
      },
      "correct_option": "A",
      "difficulty": "medium",
      "learning_objective": "k-means steps.",
      "slide_refs": [
        1
      ]
    },
    {
      "id": "pkg22_mcq10",
      "question": "Roberts masks detect edges in:",
      "options": {
        "A": "Diagonal directions",
        "B": "Horizontal only",
        "C": "Vertical only",
        "D": "Circular patterns"
      },
      "correct_option": "A",
      "difficulty": "medium",
      "learning_objective": "Roberts for gradients.",
      "slide_refs": [
        1
      ]
    },
    {
      "id": "pkg25_mcq1",
      "question": "Radial distortion is corrected by:",
      "options": {
        "A": "Polynomial mapping",
        "B": "Linear transformation",
        "C": "Color adjustment",
        "D": "Motion compensation"
      },
      "correct_option": "A",
      "difficulty": "medium",
      "learning_objective": "Distortion correction.",
      "slide_refs": [
        1
      ]
    },
    {
      "id": "pkg25_mcq2",
      "question": "k-nearest neighbor uses:",
      "options": {
        "A": "Distance to k examples for vote",
        "B": "Centroids",
        "C": "Trees",
        "D": "Hyperplanes"
      },
      "correct_option": "A",
      "difficulty": "medium",
      "learning_objective": "kNN.",
      "slide_refs": [
        1
      ]
    },
    {
      "id": "pkg25_mcq3",
      "question": "HSV space is:",
      "options": {
        "A": "Intuitive for color selection",
        "B": "Perceptual uniform",
        "C": "Additive",
        "D": "Subtractive"
      },
      "correct_option": "A",
      "difficulty": "easy",
      "learning_objective": "HSV.",
      "slide_refs": [
        1
      ]
    },
    {
      "id": "pkg25_mcq4",
      "question": "Specular reflection is:",
      "options": {
        "A": "Mirror-like, view-dependent",
        "B": "Diffuse scattering",
        "C": "Absorption",
        "D": "Emission"
      },
      "correct_option": "A",
      "difficulty": "medium",
      "learning_objective": "Reflection types.",
      "slide_refs": [
        1
      ]
    },
    {
      "id": "pkg25_mcq5",
      "question": "Structured light projects:",
      "options": {
        "A": "Patterns for correspondence",
        "B": "Uniform light",
        "C": "Colors only",
        "D": "Shadows"
      },
      "correct_option": "A",
      "difficulty": "hard",
      "learning_objective": "Structured light 3D.",
      "slide_refs": [
        1
      ]
    },
    {
      "id": "pkg25_mcq6",
      "question": "Camera extrinsics are:",
      "options": {
        "A": "Rotation and translation",
        "B": "Focal length, principal point",
        "C": "Distortion coeffs",
        "D": "Sensor size"
      },
      "correct_option": "A",
      "difficulty": "medium",
      "learning_objective": "Extrinsics vs intrinsics.",
      "slide_refs": [
        1
      ]
    },
    {
      "id": "pkg25_mcq7",
      "question": "Optical flow usage example:",
      "options": {
        "A": "Video compression, tracking",
        "B": "Static classification",
        "C": "Lens design",
        "D": "Color printing"
      },
      "correct_option": "A",
      "difficulty": "medium",
      "learning_objective": "Flow applications.",
      "slide_refs": [
        1
      ]
    },
    {
      "id": "pkg25_mcq8",
      "question": "SIFT descriptor is 128D vector of:",
      "options": {
        "A": "Gradient orientation histograms",
        "B": "Pixel intensities",
        "C": "Color values",
        "D": "Edge maps"
      },
      "correct_option": "A",
      "difficulty": "hard",
      "learning_objective": "SIFT details.",
      "slide_refs": [
        1
      ]
    },
    {
      "id": "pkg25_mcq9",
      "question": "Quadtree is a:",
      "options": {
        "A": "Hierarchical image decomposition",
        "B": "Decision tree variant",
        "C": "Graph algorithm",
        "D": "Distance measure"
      },
      "correct_option": "A",
      "difficulty": "hard",
      "learning_objective": "Quadtree for segmentation.",
      "slide_refs": [
        1
      ]
    },
    {
      "id": "pkg25_mcq10",
      "question": "Hough transform example:",
      "options": {
        "A": "Line detection in edges",
        "B": "Region filling",
        "C": "Color quantization",
        "D": "Motion blur removal"
      },
      "correct_option": "A",
      "difficulty": "medium",
      "learning_objective": "Hough usage.",
      "slide_refs": [
        1
      ]
    },
    {
      "id": "pkg13_mcq1",
      "question": "Metamers are:",
      "options": {
        "A": "Different spectral distributions that appear the same color to an observer",
        "B": "Camera sensor artifacts",
        "C": "3D reconstruction errors",
        "D": "Texture patterns"
      },
      "correct_option": "A",
      "difficulty": "easy",
      "learning_objective": "Define metamerism in color perception.",
      "slide_refs": [
        1
      ]
    },
    {
      "id": "pkg13_mcq2",
      "question": "Unsupervised learning involves:",
      "options": {
        "A": "Labeled training data",
        "B": "Discovering patterns without labels, e.g., clustering",
        "C": "Regression prediction",
        "D": "Reinforcement rewards"
      },
      "correct_option": "B",
      "difficulty": "easy",
      "learning_objective": "Distinguish unsupervised from supervised learning.",
      "slide_refs": [
        1
      ]
    },
    {
      "id": "pkg13_mcq3",
      "question": "A confusion matrix shows:",
      "options": {
        "A": "Classifier performance via true/false positives/negatives",
        "B": "Image convolution results",
        "C": "Color space transformations",
        "D": "Feature descriptors"
      },
      "correct_option": "A",
      "difficulty": "medium",
      "learning_objective": "Explain confusion matrix for evaluation.",
      "slide_refs": [
        1
      ]
    },
    {
      "id": "pkg13_mcq4",
      "question": "A feature descriptor encodes:",
      "options": {
        "A": "Global image statistics",
        "B": "Local neighborhood properties around a keypoint",
        "C": "Camera parameters",
        "D": "3D coordinates"
      },
      "correct_option": "B",
      "difficulty": "easy",
      "learning_objective": "Define feature descriptors.",
      "slide_refs": [
        1
      ]
    },
    {
      "id": "pkg13_mcq5",
      "question": "An epipolar line constrains:",
      "options": {
        "A": "The location of corresponding points in stereo images",
        "B": "Motion vectors",
        "C": "Texture gradients",
        "D": "Color histograms"
      },
      "correct_option": "A",
      "difficulty": "medium",
      "learning_objective": "Recall epipolar geometry.",
      "slide_refs": [
        1
      ]
    },
    {
      "id": "pkg13_mcq6",
      "question": "Multi-view stereo reconstructs:",
      "options": {
        "A": "2D panoramas",
        "B": "3D models from multiple images via triangulation",
        "C": "Optical flow fields",
        "D": "Texture maps"
      },
      "correct_option": "B",
      "difficulty": "medium",
      "learning_objective": "Describe multi-view stereo.",
      "slide_refs": [
        1
      ]
    },
    {
      "id": "pkg13_mcq7",
      "question": "Otsu's method optimizes:",
      "options": {
        "A": "Threshold for binary segmentation by maximizing between-class variance",
        "B": "Edge detection filters",
        "C": "Feature matching",
        "D": "Color balance"
      },
      "correct_option": "A",
      "difficulty": "medium",
      "learning_objective": "Explain Otsu's thresholding.",
      "slide_refs": [
        1
      ]
    },
    {
      "id": "pkg13_mcq8",
      "question": "Convolutional neural networks use:",
      "options": {
        "A": "Shared weights and pooling for hierarchical feature learning",
        "B": "Only fully connected layers",
        "C": "No activation functions",
        "D": "Unsupervised pretraining always"
      },
      "correct_option": "A",
      "difficulty": "hard",
      "learning_objective": "Outline CNN principles.",
      "slide_refs": [
        1
      ]
    },
    {
      "id": "pkg13_mcq9",
      "question": "Local binary patterns (LBP) compute:",
      "options": {
        "A": "Binary codes from neighborhood intensity comparisons",
        "B": "Global histograms",
        "C": "3D normals",
        "D": "Motion vectors"
      },
      "correct_option": "A",
      "difficulty": "medium",
      "learning_objective": "Describe LBP operator.",
      "slide_refs": [
        1
      ]
    },
    {
      "id": "pkg13_mcq10",
      "question": "The temporal derivative ∂f/∂t in optical flow is estimated by:",
      "options": {
        "A": "Difference between consecutive frames at the same position",
        "B": "Spatial gradients only",
        "C": "Color changes",
        "D": "Camera motion"
      },
      "correct_option": "A",
      "difficulty": "hard",
      "learning_objective": "Compute derivatives for flow constraint.",
      "slide_refs": [
        1
      ]
    },
    {
      "id": "pkg14_mcq1",
      "question": "Chromatic aberration causes:",
      "options": {
        "A": "Geometric distortion",
        "B": "Color-dependent focusing errors leading to fringing",
        "C": "Sensor noise",
        "D": "Vignetting"
      },
      "correct_option": "B",
      "difficulty": "easy",
      "learning_objective": "Define chromatic aberration.",
      "slide_refs": [
        1
      ]
    },
    {
      "id": "pkg14_mcq2",
      "question": "A Bayer filter is used for:",
      "options": {
        "A": "Demosaicing color images from single-sensor arrays",
        "B": "Edge detection",
        "C": "Motion blur reduction",
        "D": "3D reconstruction"
      },
      "correct_option": "A",
      "difficulty": "medium",
      "learning_objective": "Explain Bayer pattern in color imaging.",
      "slide_refs": [
        1
      ]
    },
    {
      "id": "pkg14_mcq3",
      "question": "The aperture problem refers to:",
      "options": {
        "A": "Ambiguity in motion direction along edges",
        "B": "Lens aperture size affecting depth of field",
        "C": "Camera calibration issues",
        "D": "Color space conversion"
      },
      "correct_option": "A",
      "difficulty": "medium",
      "learning_objective": "Describe aperture problem in flow.",
      "slide_refs": [
        1
      ]
    },
    {
      "id": "pkg14_mcq4",
      "question": "Hough transform detects:",
      "options": {
        "A": "Arbitrary shapes via voting in parameter space",
        "B": "Only circles",
        "C": "Textures",
        "D": "Motion"
      },
      "correct_option": "A",
      "difficulty": "medium",
      "learning_objective": "Principle of Hough for line detection.",
      "slide_refs": [
        1
      ]
    },
    {
      "id": "pkg14_mcq5",
      "question": "Mahalanobis distance accounts for:",
      "options": {
        "A": "Covariance in multivariate data",
        "B": "Only mean differences",
        "C": "Binary features",
        "D": "Image gradients"
      },
      "correct_option": "A",
      "difficulty": "hard",
      "learning_objective": "Compare to Euclidean distance.",
      "slide_refs": [
        1
      ]
    },
    {
      "id": "pkg14_mcq6",
      "question": "Structure-from-motion recovers:",
      "options": {
        "A": "3D structure and camera poses from image sequence",
        "B": "Only 2D homographies",
        "C": "Color constancy",
        "D": "Texture synthesis"
      },
      "correct_option": "A",
      "difficulty": "medium",
      "learning_objective": "Define SfM pipeline.",
      "slide_refs": [
        1
      ]
    },
    {
      "id": "pkg14_mcq7",
      "question": "K-means clustering partitions data into:",
      "options": {
        "A": "K groups by minimizing within-cluster variance",
        "B": "Hierarchical trees",
        "C": "Overlapping sets",
        "D": "Labeled classes"
      },
      "correct_option": "A",
      "difficulty": "medium",
      "learning_objective": "Describe k-means algorithm.",
      "slide_refs": [
        1
      ]
    },
    {
      "id": "pkg14_mcq8",
      "question": "Photometric stereo uses:",
      "options": {
        "A": "Multiple lights to recover surface normals from shading",
        "B": "Stereo pairs for depth",
        "C": "Motion for structure",
        "D": "Texture for classification"
      },
      "correct_option": "A",
      "difficulty": "hard",
      "learning_objective": "Explain photometric stereo.",
      "slide_refs": [
        1
      ]
    },
    {
      "id": "pkg14_mcq9",
      "question": "In stereo, disparity d relates to depth z by:",
      "options": {
        "A": "z = f * b / d",
        "B": "z = d / f",
        "C": "z = b / d",
        "D": "z = f / d"
      },
      "correct_option": "A",
      "difficulty": "medium",
      "learning_objective": "Derive depth from disparity.",
      "slide_refs": [
        1
      ]
    },
    {
      "id": "pkg14_mcq10",
      "question": "Stereo correspondence is similar to optical flow matching but differs in:",
      "options": {
        "A": "Fixed baseline vs. temporal change",
        "B": "Both use block matching",
        "C": "Stereo has epipolar constraint",
        "D": "All of the above"
      },
      "correct_option": "D",
      "difficulty": "hard",
      "learning_objective": "Compare stereo and flow.",
      "slide_refs": [
        1
      ]
    },
    {
      "id": "pkg4_mcq1",
      "question": "Which of the following is NOT a reason why analyzing visual motion is important in computer vision?",
      "options": {
        "A": "To stabilize shaky video caused by camera jitter",
        "B": "To track and analyze object trajectories",
        "C": "To directly compute absolute depth from a single image",
        "D": "To reveal spatial layout via motion parallax"
      },
      "correct_option": "C",
      "difficulty": "easy",
      "learning_objective": "Identify valid applications of motion analysis in vision systems.",
      "slide_refs": [
        8
      ]
    },
    {
      "id": "pkg4_mcq2",
      "question": "Motion parallax refers to the phenomenon where:",
      "options": {
        "A": "Closer objects appear to move faster than distant ones when the camera translates",
        "B": "Objects rotate around their center when the camera moves",
        "C": "Parallel lines converge at the vanishing point due to camera motion",
        "D": "Image brightness changes uniformly across the frame"
      },
      "correct_option": "A",
      "difficulty": "easy",
      "learning_objective": "Recall the definition and visual effect of motion parallax.",
      "slide_refs": [
        8
      ]
    },
    {
      "id": "pkg4_mcq3",
      "question": "In the context of perceptual organization, motion can serve as a powerful cue for grouping when:",
      "options": {
        "A": "All points in the image move at the same velocity",
        "B": "Regions move coherently with a common motion pattern",
        "C": "Motion is completely random across the image",
        "D": "Only static edges are present"
      },
      "correct_option": "B",
      "difficulty": "medium",
      "learning_objective": "Understand motion as a segmentation and grouping cue.",
      "slide_refs": [
        9
      ]
    },
    {
      "id": "pkg4_mcq4",
      "question": "The aperture problem states that:",
      "options": {
        "A": "Local motion measurements through a small aperture are ambiguous",
        "B": "Global optical flow is always unique",
        "C": "Only translational motion can be recovered",
        "D": "Motion perpendicular to an edge is always detectable"
      },
      "correct_option": "A",
      "difficulty": "medium",
      "learning_objective": "Explain the fundamental ambiguity in local motion estimation.",
      "slide_refs": [
        12
      ]
    },
    {
      "id": "pkg4_mcq5",
      "question": "For a rigidly moving object under orthographic projection, the instantaneous 2D motion field is best approximated by:",
      "options": {
        "A": "A pure translation",
        "B": "An affine transformation with 6 parameters",
        "C": "A full projective transformation",
        "D": "A single scale factor"
      },
      "correct_option": "B",
      "difficulty": "hard",
      "learning_objective": "Derive the parametric form of 2D motion under rigid 3D motion and orthographic camera.",
      "slide_refs": [
        15
      ]
    },
    {
      "id": "pkg4_mcq6",
      "question": "The brightness constancy assumption in optical flow estimation states that:",
      "options": {
        "A": "Image intensity at a point remains constant over time if the point moves",
        "B": "All pixels have the same brightness",
        "C": "Motion causes uniform change in intensity",
        "D": "Lighting direction is known and fixed"
      },
      "correct_option": "A",
      "difficulty": "easy",
      "learning_objective": "State the core assumption behind differential optical flow methods.",
      "slide_refs": [
        18
      ]
    },
    {
      "id": "pkg4_mcq7",
      "question": "The Lucas-Kanade method assumes that motion is approximately constant:",
      "options": {
        "A": "Over the entire image",
        "B": "In a small spatial neighborhood around each pixel",
        "C": "Only at feature points",
        "D": "Along epipolar lines"
      },
      "correct_option": "B",
      "difficulty": "medium",
      "learning_objective": "Recall the local smoothness assumption in Lucas-Kanade optical flow.",
      "slide_refs": [
        20
      ]
    },
    {
      "id": "pkg4_mcq8",
      "question": "In the Horn-Schunck optical flow formulation, the smoothness term regularizes the flow field by minimizing:",
      "options": {
        "A": "The magnitude of the flow vectors",
        "B": "The spatial gradients of the flow components \\(\\frac{\\partial u}{\\partial x}, \\frac{\\partial u}{\\partial y}, \\frac{\\partial v}{\\partial x}, \\frac{\\partial v}{\\partial y}\\)",
        "C": "The difference between consecutive frames",
        "D": "The total number of non-zero flow vectors"
      },
      "correct_option": "B",
      "difficulty": "hard",
      "learning_objective": "Understand the global regularization in dense optical flow estimation.",
      "slide_refs": [
        22
      ]
    },
    {
      "id": "pkg4_mcq9",
      "question": "Which of the following is a common failure case of the brightness constancy assumption?",
      "options": {
        "A": "Slow camera motion",
        "B": "Specular reflections or moving shadows",
        "C": "Pure translational motion",
        "D": "High-contrast textures"
      },
      "correct_option": "B",
      "difficulty": "medium",
      "learning_objective": "Identify real-world violations of optical flow assumptions.",
      "slide_refs": [
        19
      ]
    },
    {
      "id": "pkg4_mcq10",
      "question": "To estimate an affine 2D motion model using least squares, the minimum number of point correspondences required is:",
      "options": {
        "A": "2",
        "B": "3",
        "C": "4",
        "D": "6"
      },
      "correct_option": "B",
      "difficulty": "medium",
      "learning_objective": "Determine minimal sample size for parametric motion estimation.",
      "slide_refs": [
        16
      ]
    },
    {
      "id": "pkg6_mcq1",
      "question": "What is the primary purpose of detecting local features in images?",
      "options": {
        "A": "To compress the image for storage",
        "B": "To enable robust image matching by finding corresponding points across views",
        "C": "To directly reconstruct 3D geometry without correspondences",
        "D": "To perform global color correction"
      },
      "correct_option": "B",
      "difficulty": "easy",
      "learning_objective": "Understand the motivation for local feature detection in image matching.",
      "slide_refs": [
        8
      ]
    },
    {
      "id": "pkg6_mcq2",
      "question": "The Harris corner detector identifies points where:",
      "options": {
        "A": "Intensity is locally maximum",
        "B": "The image gradient changes significantly in two orthogonal directions",
        "C": "Edges are perfectly straight",
        "D": "Texture is uniform"
      },
      "correct_option": "B",
      "difficulty": "medium",
      "learning_objective": "Recall the core principle behind Harris corner detection.",
      "slide_refs": [
        7
      ]
    },
    {
      "id": "pkg6_mcq3",
      "question": "In the Harris corner response function, the matrix \\(M\\) is constructed from:",
      "options": {
        "A": "Raw pixel intensities in a local window",
        "B": "Spatial gradients \\(I_x\\) and \\(I_y\\) averaged over a neighborhood",
        "C": "Second derivatives of the image",
        "D": "Laplacian of Gaussian responses"
      },
      "correct_option": "B",
      "difficulty": "medium",
      "learning_objective": "Describe the structure tensor used in Harris corner detection.",
      "slide_refs": [
        10
      ]
    },
    {
      "id": "pkg6_mcq4",
      "question": "The corner response \\(R = \\det(M) - k \\cdot \\trace(M)^2\\) is high when:",
      "options": {
        "A": "Both eigenvalues of \\(M\\) are small",
        "B": "One eigenvalue is large and the other is small",
        "C": "Both eigenvalues are large",
        "D": "The trace is zero"
      },
      "correct_option": "C",
      "difficulty": "hard",
      "learning_objective": "Interpret the eigenvalue analysis of the structure tensor for corner classification.",
      "slide_refs": [
        11
      ]
    },
    {
      "id": "pkg6_mcq5",
      "question": "Which of the following is a key advantage of Harris corners over simple edge detectors?",
      "options": {
        "A": "They are invariant to full projective transformations",
        "B": "They provide precise localization at junctions where two edges meet",
        "C": "They require no gradient computation",
        "D": "They are scale-invariant by default"
      },
      "correct_option": "B",
      "difficulty": "medium",
      "learning_objective": "Evaluate the localization precision of corner features.",
      "slide_refs": [
        9
      ]
    },
    {
      "id": "pkg6_mcq6",
      "question": "In practice, non-maximum suppression is applied to Harris corner responses to:",
      "options": {
        "A": "Increase the number of detected corners",
        "B": "Select the strongest local maxima and avoid clustered responses",
        "C": "Smooth the image before detection",
        "D": "Threshold based on global intensity"
      },
      "correct_option": "B",
      "difficulty": "easy",
      "learning_objective": "Explain post-processing steps in feature detection.",
      "slide_refs": [
        12
      ]
    },
    {
      "id": "pkg6_mcq7",
      "question": "The parameter \\(k\\) in the Harris response function typically ranges between:",
      "options": {
        "A": "0.001 to 0.01",
        "B": "0.04 to 0.06",
        "C": "0.5 to 1.0",
        "D": "10 to 100"
      },
      "correct_option": "B",
      "difficulty": "hard",
      "learning_objective": "Recall empirical values for the Harris detector sensitivity parameter.",
      "slide_refs": [
        11
      ]
    },
    {
      "id": "pkg6_mcq8",
      "question": "Which visual example in the lecture demonstrates successful matching using local features?",
      "options": {
        "A": "Two identical images of a flat wall",
        "B": "Two mountain panorama images with corresponding red lines connecting feature points",
        "C": "A single image with motion blur",
        "D": "A 3D reconstructed point cloud"
      },
      "correct_option": "B",
      "difficulty": "easy",
      "learning_objective": "Identify real-world application of feature matching.",
      "slide_refs": [
        8
      ]
    },
    {
      "id": "pkg6_mcq9",
      "question": "The Harris detector is based on the autocorrelation function of the image. A corner corresponds to a point where this function:",
      "options": {
        "A": "Is flat in all directions",
        "B": "Has a peak in one direction only",
        "C": "Drops rapidly in multiple directions",
        "D": "Is zero"
      },
      "correct_option": "C",
      "difficulty": "hard",
      "learning_objective": "Connect Harris method to the concept of image autocorrelation.",
      "slide_refs": [
        9
      ]
    },
    {
      "id": "pkg6_mcq10",
      "question": "Why are local features preferred over global image descriptors for matching under viewpoint changes?",
      "options": {
        "A": "They are computationally cheaper to extract globally",
        "B": "They allow partial matching and are robust to occlusion and clutter",
        "C": "They encode color information more accurately",
        "D": "They require calibrated cameras"
      },
      "correct_option": "B",
      "difficulty": "medium",
      "learning_objective": "Justify the use of sparse local features in robust correspondence.",
      "slide_refs": [
        8
      ]
    },
    {
      "id": "pkg11_mcq1",
      "question": "The lecture series is structured around which central theme?",
      "options": {
        "A": "Machine learning for image classification",
        "B": "From photons to semantics – building a complete computer vision pipeline",
        "C": "Real-time video processing on embedded devices",
        "D": "Deep neural network architectures for vision"
      },
      "correct_option": "B",
      "difficulty": "easy",
      "learning_objective": "Identify the overarching goal of the course as a full vision pipeline.",
      "slide_refs": [
        3
      ]
    },
    {
      "id": "pkg11_mcq2",
      "question": "Which of the following is NOT listed as a core module in the course outline?",
      "options": {
        "A": "Imaging and the pinhole model",
        "B": "Light and color – specular reflection",
        "C": "Graph cuts and energy minimization",
        "D": "Recognition – basics"
      },
      "correct_option": "C",
      "difficulty": "medium",
      "learning_objective": "Recall the sequence and titles of the 11 main lecture modules.",
      "slide_refs": [
        5
      ]
    },
    {
      "id": "pkg11_mcq3",
      "question": "The course emphasizes that modern computer vision relies heavily on:",
      "options": {
        "A": "Hand-crafted geometric models only",
        "B": "Learning from large labeled datasets",
        "C": "Real-time GPU acceleration",
        "D": "Quantum imaging sensors"
      },
      "correct_option": "B",
      "difficulty": "easy",
      "learning_objective": "Understand the paradigm shift toward data-driven methods.",
      "slide_refs": [
        7
      ]
    },
    {
      "id": "pkg11_mcq4",
      "question": "A key challenge highlighted in the introduction is:",
      "options": {
        "A": "The inverse problem of recovering 3D scene properties from 2D projections",
        "B": "Storing high-resolution images efficiently",
        "C": "Training neural networks in under one second",
        "D": "Designing lenses with zero distortion"
      },
      "correct_option": "A",
      "difficulty": "medium",
      "learning_objective": "Recognize the ill-posed nature of vision as inverse optics.",
      "slide_refs": [
        9
      ]
    },
    {
      "id": "pkg11_mcq5",
      "question": "The lecture on 'Depth from images – 3D cues' (10.1) primarily covers:",
      "options": {
        "A": "Active structured light and time-of-flight",
        "B": "Monocular cues such as texture gradient, shading, and focus",
        "C": "Multi-view stereo reconstruction",
        "D": "Photometric stereo with known light directions"
      },
      "correct_option": "B",
      "difficulty": "medium",
      "learning_objective": "Differentiate passive monocular depth cues from active or stereo methods.",
      "slide_refs": [
        5
      ]
    },
    {
      "id": "pkg11_mcq6",
      "question": "Which module directly follows 'Local features – Harris corners' (6.1)?",
      "options": {
        "A": "Recognition – Basics",
        "B": "2D models – Fitting",
        "C": "Motion – Basics",
        "D": "Texture analysis – Basics"
      },
      "correct_option": "A",
      "difficulty": "easy",
      "learning_objective": "Memorize the chronological order of lecture topics.",
      "slide_refs": [
        5
      ]
    },
    {
      "id": "pkg11_mcq7",
      "question": "The introductory slides claim that vision is 'easy for humans, hard for machines' because:",
      "options": {
        "A": "Humans have higher resolution retinas",
        "B": "The mapping from 3D world to 2D image loses information irreversibly",
        "C": "Machines cannot process color information",
        "D": "Human eyes have larger field of view"
      },
      "correct_option": "B",
      "difficulty": "medium",
      "learning_objective": "Explain the information loss in image formation.",
      "slide_refs": [
        8
      ]
    },
    {
      "id": "pkg11_mcq8",
      "question": "In the course roadmap, '3D – Affine transformation' (11.1) is positioned as:",
      "options": {
        "A": "The final topic before advanced deep learning",
        "B": "A bridge between 2D image processing and full 3D reconstruction",
        "C": "An optional module on linear algebra",
        "D": "A review of camera calibration"
      },
      "correct_option": "B",
      "difficulty": "hard",
      "learning_objective": "Interpret the pedagogical progression from 2D to 3D reasoning.",
      "slide_refs": [
        5
      ]
    },
    {
      "id": "pkg11_mcq9",
      "question": "Which real-world application is explicitly mentioned as a motivation for studying motion analysis?",
      "options": {
        "A": "Autonomous driving and action recognition",
        "B": "Medical image registration",
        "C": "Satellite image mosaicking",
        "D": "All of the above"
      },
      "correct_option": "D",
      "difficulty": "easy",
      "learning_objective": "Connect theoretical modules to practical vision systems.",
      "slide_refs": [
        6
      ]
    },
    {
      "id": "pkg11_mcq10",
      "question": "The slide titled 'What is computer vision?' defines it as:",
      "options": {
        "A": "Making useful decisions about real physical objects and scenes based on sensed images",
        "B": "Digitizing analog photographs",
        "C": "Compressing image files",
        "D": "Designing better camera hardware"
      },
      "correct_option": "A",
      "difficulty": "easy",
      "learning_objective": "State the standard definition of the field.",
      "slide_refs": [
        4
      ]
    },
    {
      "id": "pkg5_mcq1",
      "question": "According to the lecture, pattern recognition is fundamentally the assignment of a label to a given input value. Which of the following is NOT explicitly listed as an example of pattern recognition?",
      "options": {
        "A": "Classification",
        "B": "Regression",
        "C": "Clustering",
        "D": "Handprinted character recognition"
      },
      "correct_option": "C",
      "difficulty": "easy",
      "learning_objective": "Recall the definition and common examples of pattern recognition.",
      "slide_refs": [
        12
      ]
    },
    {
      "id": "pkg5_mcq2",
      "question": "In the context of pattern recognition, the lecture emphasizes that the process typically involves:",
      "options": {
        "A": "Manual rule-based decision making",
        "B": "Learning from data in most practical cases",
        "C": "Hard-coded geometric models only",
        "D": "Direct pixel intensity comparison"
      },
      "correct_option": "B",
      "difficulty": "easy",
      "learning_objective": "Understand the role of learning in modern pattern recognition systems.",
      "slide_refs": [
        12
      ]
    },
    {
      "id": "pkg5_mcq3",
      "question": "The lecture shows binary images of handwritten digits. What is the primary challenge illustrated by these examples?",
      "options": {
        "A": "High-resolution color variations",
        "B": "Large intra-class variability in shape and style",
        "C": "Complex background clutter",
        "D": "3D pose estimation"
      },
      "correct_option": "B",
      "difficulty": "medium",
      "learning_objective": "Identify the core difficulty in recognizing deformable or variable patterns like handwriting.",
      "slide_refs": [
        12
      ]
    },
    {
      "id": "pkg5_mcq4",
      "question": "Which of the following best describes the goal of a pattern recognition system for handwritten character recognition?",
      "options": {
        "A": "To reconstruct the 3D pen trajectory",
        "B": "To assign a discrete class label (e.g., '0'–'9') to the input image",
        "C": "To estimate continuous stroke thickness",
        "D": "To detect ink color"
      },
      "correct_option": "B",
      "difficulty": "easy",
      "learning_objective": "Define the output of a classification-based recognition task.",
      "slide_refs": [
        12
      ]
    },
    {
      "id": "pkg5_mcq5",
      "question": "The lecture contrasts pattern recognition with other vision tasks. Which statement is correct?",
      "options": {
        "A": "Recognition is only about detecting objects, not classifying them.",
        "B": "Recognition often requires learning statistical models from labeled training data.",
        "C": "Recognition can always be solved using geometric fitting alone.",
        "D": "Recognition does not benefit from machine learning techniques."
      },
      "correct_option": "B",
      "difficulty": "medium",
      "learning_objective": "Differentiate recognition from low-level vision tasks and emphasize data-driven approaches.",
      "slide_refs": [
        12
      ]
    },
    {
      "id": "pkg5_mcq6",
      "question": "In the broader context of the course (as referenced in prior lectures), object recognition may build upon earlier modules. Which of the following is a prerequisite for robust recognition in real-world images?",
      "options": {
        "A": "Accurate 2D model fitting under occlusion and clutter",
        "B": "Direct pixel-to-label mapping without preprocessing",
        "C": "Assuming all objects are rigid and fully visible",
        "D": "Using only global image statistics"
      },
      "correct_option": "A",
      "difficulty": "hard",
      "learning_objective": "Connect recognition challenges to model fitting and robustness (cross-reference with Lecture 9).",
      "slide_refs": [
        12,
        14
      ]
    },
    {
      "id": "pkg5_mcq7",
      "question": "The lecture introduces recognition as a decision-making process. What is the input to this decision in the handwritten digit example?",
      "options": {
        "A": "A 3D point cloud",
        "B": "A binary or grayscale image patch containing a single digit",
        "C": "A sequence of video frames",
        "D": "A set of edge maps"
      },
      "correct_option": "B",
      "difficulty": "easy",
      "learning_objective": "Identify the typical input representation in a recognition pipeline.",
      "slide_refs": [
        12
      ]
    },
    {
      "id": "pkg5_mcq8",
      "question": "Which of the following is a key difference between classification and regression in the context of pattern recognition?",
      "options": {
        "A": "Classification outputs discrete labels; regression outputs continuous values.",
        "B": "Classification uses learning; regression does not.",
        "C": "Regression is only for 2D data; classification for 3D.",
        "D": "There is no difference; both are identical."
      },
      "correct_option": "A",
      "difficulty": "medium",
      "learning_objective": "Distinguish between discrete and continuous prediction tasks in recognition.",
      "slide_refs": [
        12
      ]
    },
    {
      "id": "pkg3_mcq1",
      "question": "In the context of 2D model fitting, what is the primary goal of choosing a parametric model for a set of image features?",
      "options": {
        "A": "To increase computational complexity",
        "B": "To represent object geometry with a compact mathematical description",
        "C": "To eliminate all outliers automatically",
        "D": "To perform edge detection"
      },
      "correct_option": "B",
      "difficulty": "easy",
      "learning_objective": "Understand the purpose of parametric modeling in object representation.",
      "slide_refs": [
        12
      ]
    },
    {
      "id": "pkg3_mcq2",
      "question": "Which of the following is NOT listed as a common geometric primitive for 2D object modeling?",
      "options": {
        "A": "Point",
        "B": "Line",
        "C": "Triangle",
        "D": "Ellipse"
      },
      "correct_option": "C",
      "difficulty": "easy",
      "learning_objective": "Recall basic 2D geometric primitives used in model fitting.",
      "slide_refs": [
        13
      ]
    },
    {
      "id": "pkg3_mcq3",
      "question": "In the case study on line detection shown in the lecture, which of the following is identified as a source of 'missing data'?",
      "options": {
        "A": "Noise in feature locations",
        "B": "Multiple overlapping lines",
        "C": "Occlusions",
        "D": "Clutter from background textures"
      },
      "correct_option": "C",
      "difficulty": "medium",
      "learning_objective": "Identify real-world challenges in model fitting due to missing data.",
      "slide_refs": [
        15
      ]
    },
    {
      "id": "pkg3_mcq4",
      "question": "What type of data issue is illustrated by the presence of multiple coins in the 'simple model: circles' example?",
      "options": {
        "A": "Noise",
        "B": "Extraneous data (clutter)",
        "C": "Missing data",
        "D": "Model mismatch"
      },
      "correct_option": "B",
      "difficulty": "medium",
      "learning_objective": "Distinguish between noise, clutter, and missing data in fitting scenarios.",
      "slide_refs": [
        14
      ]
    },
    {
      "id": "pkg3_mcq5",
      "question": "When fitting a complex model like 'car' using multiple ellipses, the system must handle:",
      "options": {
        "A": "Only position and scale parameters",
        "B": "Pose, articulation, and part-based alignment",
        "C": "Only global rotation",
        "D": "Only color consistency"
      },
      "correct_option": "B",
      "difficulty": "hard",
      "learning_objective": "Recognize the increased complexity in fitting articulated or multi-part models.",
      "slide_refs": [
        14
      ]
    },
    {
      "id": "pkg3_mcq6",
      "question": "Which of the following best describes the role of model parameters in 2D fitting?",
      "options": {
        "A": "They define only the color of the object",
        "B": "They specify size, position, and orientation of the geometric primitive",
        "C": "They are used only for line detection",
        "D": "They replace the need for feature extraction"
      },
      "correct_option": "B",
      "difficulty": "easy",
      "learning_objective": "Understand the semantic meaning of model parameters in geometric fitting.",
      "slide_refs": [
        13
      ]
    },
    {
      "id": "pkg3_mcq7",
      "question": "In the presence of occlusion, a robust fitting algorithm should:",
      "options": {
        "A": "Fail completely if any part is occluded",
        "B": "Use only visible parts to estimate model parameters",
        "C": "Assume all data is missing",
        "D": "Increase sensitivity to noise"
      },
      "correct_option": "B",
      "difficulty": "medium",
      "learning_objective": "Apply robustness principles to partial visibility in model fitting.",
      "slide_refs": [
        15
      ]
    },
    {
      "id": "pkg3_mcq8",
      "question": "Which image in the lecture demonstrates the use of a 'complicated model: car' with deformable parts?",
      "options": {
        "A": "The image with multiple coins",
        "B": "The image with a red microcar and yellow contour overlay",
        "C": "The line detection case study with a house",
        "D": "The structured light example"
      },
      "correct_option": "B",
      "difficulty": "easy",
      "learning_objective": "Identify examples of complex model fitting in real images.",
      "slide_refs": [
        14
      ]
    },
    {
      "id": "pkg1_mcq1",
      "question": "In the homogeneous coordinate representation of 3D affine transformations, the transformation matrix mapping a point \\(P = (x, y, z)\\) to \\(P' = (x', y', z')\\) is a 4×4 matrix with the bottom row fixed as:",
      "options": {
        "A": "[1 0 0 0]",
        "B": "[0 1 0 0]",
        "C": "[0 0 1 0]",
        "D": "[0 0 0 1]"
      },
      "correct_option": "D",
      "difficulty": "easy",
      "learning_objective": "Recall the structure of the 4×4 affine transformation matrix in homogeneous coordinates.",
      "slide_refs": [
        11
      ]
    },
    {
      "id": "pkg1_mcq2",
      "question": "A rigid 3D transformation consists of rotation followed by translation. How many degrees of freedom does it have, and what is the minimum number of point correspondences required to estimate it uniquely?",
      "options": {
        "A": "6 degrees of freedom; at least 3 point correspondences",
        "B": "6 degrees of freedom; at least 4 point correspondences",
        "C": "7 degrees of freedom; at least 3 point correspondences",
        "D": "12 degrees of freedom; at least 6 point correspondences"
      },
      "correct_option": "A",
      "difficulty": "medium",
      "learning_objective": "Identify the degrees of freedom of rigid transformation and the minimum correspondences for pose estimation.",
      "slide_refs": [
        12,
        13,
        34
      ]
    },
    {
      "id": "pkg1_mcq3",
      "question": "Given a calibrated camera with known intrinsic matrix \\(K\\), the projection equation can be simplified using normalized image coordinates. Which matrix represents the simplified projection model \\(M\\) when \\(f = 1\\) and principal point \\((c_0, r_0) = (0, 0)\\)?",
      "options": {
        "A": "\\(M = \\begin{bmatrix} K & 0 \\end{bmatrix} [R \\; t]\\)",
        "B": "\\(M = [R \\; t]\\)",
        "C": "\\(M = K [R \\; t]\\)",
        "D": "\\(M = [I \\; 0] [R \\; t]\\)"
      },
      "correct_option": "B",
      "difficulty": "hard",
      "learning_objective": "Derive the projection matrix under normalized image coordinates for a calibrated camera.",
      "slide_refs": [
        24
      ]
    },
    {
      "id": "pkg16_mcq1",
      "question": "Radial distortion causes:",
      "options": {
        "A": "Straight lines to curve in images",
        "B": "Color shifts",
        "C": "Motion blur",
        "D": "Texture loss"
      },
      "correct_option": "A",
      "difficulty": "easy",
      "learning_objective": "Define radial lens distortion.",
      "slide_refs": [
        1
      ]
    },
    {
      "id": "pkg16_mcq2",
      "question": "RGB color space is:",
      "options": {
        "A": "Additive primaries for displays",
        "B": "Perceptually uniform",
        "C": "Device-independent",
        "D": "Cylindrical coordinates"
      },
      "correct_option": "A",
      "difficulty": "easy",
      "learning_objective": "Describe RGB model.",
      "slide_refs": [
        1
      ]
    },
    {
      "id": "pkg16_mcq3",
      "question": "An image histogram shows:",
      "options": {
        "A": "Intensity frequency distribution",
        "B": "Spatial positions",
        "C": "Gradients",
        "D": "3D depths"
      },
      "correct_option": "A",
      "difficulty": "easy",
      "learning_objective": "Explain histograms.",
      "slide_refs": [
        1
      ]
    },
    {
      "id": "pkg16_mcq4",
      "question": "A feature descriptor is:",
      "options": {
        "A": "Vector summarizing local patch",
        "B": "Global statistic",
        "C": "Camera parameter",
        "D": "Light model"
      },
      "correct_option": "A",
      "difficulty": "medium",
      "learning_objective": "Define descriptors.",
      "slide_refs": [
        1
      ]
    },
    {
      "id": "pkg16_mcq5",
      "question": "The aperture problem arises because:",
      "options": {
        "A": "Local motion is ambiguous along contours",
        "B": "Aperture size affects light",
        "C": "Sensors have limited resolution",
        "D": "Colors vary"
      },
      "correct_option": "A",
      "difficulty": "medium",
      "learning_objective": "Aperture problem in flow.",
      "slide_refs": [
        1
      ]
    },
    {
      "id": "pkg16_mcq6",
      "question": "Structure-from-motion is:",
      "options": {
        "A": "3D from moving camera images",
        "B": "2D alignment",
        "C": "Color recovery",
        "D": "Texture synthesis"
      },
      "correct_option": "A",
      "difficulty": "medium",
      "learning_objective": "Define SfM.",
      "slide_refs": [
        1
      ]
    },
    {
      "id": "pkg16_mcq7",
      "question": "K-means is used for:",
      "options": {
        "A": "Unsupervised clustering",
        "B": "Supervised classification",
        "C": "Regression",
        "D": "Dimensionality reduction"
      },
      "correct_option": "A",
      "difficulty": "easy",
      "learning_objective": "K-means principle.",
      "slide_refs": [
        1
      ]
    },
    {
      "id": "pkg16_mcq8",
      "question": "Hough transform uses:",
      "options": {
        "A": "Accumulator voting for parameters",
        "B": "Gradient descent",
        "C": "Neural networks",
        "D": "Histogram equalization"
      },
      "correct_option": "A",
      "difficulty": "medium",
      "learning_objective": "Hough voting.",
      "slide_refs": [
        1
      ]
    },
    {
      "id": "pkg16_mcq9",
      "question": "In stereo, depth z =:",
      "options": {
        "A": "f b / d",
        "B": "d b / f",
        "C": "f d / b",
        "D": "b / (f d)"
      },
      "correct_option": "A",
      "difficulty": "medium",
      "learning_objective": "Stereo depth formula.",
      "slide_refs": [
        1
      ]
    },
    {
      "id": "pkg16_mcq10",
      "question": "Stereo matching differs from flow by:",
      "options": {
        "A": "Known geometry vs. unknown motion",
        "B": "Both seek correspondences",
        "C": "Stereo uses rectification",
        "D": "All of the above"
      },
      "correct_option": "D",
      "difficulty": "hard",
      "learning_objective": "Compare stereo and flow.",
      "slide_refs": [
        1
      ]
    },
    {
      "id": "pkg20_mcq1",
      "question": "Pinhole model assumes:",
      "options": {
        "A": "Infinite depth of field, perspective projection",
        "B": "Orthographic projection",
        "C": "Lens distortion",
        "D": "Finite aperture"
      },
      "correct_option": "A",
      "difficulty": "easy",
      "learning_objective": "Pinhole basics.",
      "slide_refs": [
        1
      ]
    },
    {
      "id": "pkg20_mcq2",
      "question": "Mahalanobis distance is useful for:",
      "options": {
        "A": "Anisotropic data with covariance",
        "B": "Binary codes",
        "C": "Image filtering",
        "D": "3D reconstruction"
      },
      "correct_option": "A",
      "difficulty": "hard",
      "learning_objective": "Mahalanobis vs Euclidean.",
      "slide_refs": [
        1
      ]
    },
    {
      "id": "pkg20_mcq3",
      "question": "Confusion matrix is for:",
      "options": {
        "A": "Multi-class error analysis",
        "B": "Regression",
        "C": "Clustering",
        "D": "Optimization"
      },
      "correct_option": "A",
      "difficulty": "medium",
      "learning_objective": "Confusion matrix.",
      "slide_refs": [
        1
      ]
    },
    {
      "id": "pkg20_mcq4",
      "question": "Radial distortion is:",
      "options": {
        "A": "Barrel or pincushion warping",
        "B": "Tangential shift",
        "C": "Color aberration",
        "D": "Vignetting"
      },
      "correct_option": "A",
      "difficulty": "medium",
      "learning_objective": "Distortion types.",
      "slide_refs": [
        1
      ]
    },
    {
      "id": "pkg20_mcq5",
      "question": "Unsupervised learning includes:",
      "options": {
        "A": "k-means, PCA",
        "B": "SVM, NN",
        "C": "RL",
        "D": "All supervised"
      },
      "correct_option": "A",
      "difficulty": "easy",
      "learning_objective": "Unsupervised examples.",
      "slide_refs": [
        1
      ]
    },
    {
      "id": "pkg20_mcq6",
      "question": "Multi-view stereo is:",
      "options": {
        "A": "Dense 3D from multiple images",
        "B": "Single view depth",
        "C": "Motion only",
        "D": "Texture only"
      },
      "correct_option": "A",
      "difficulty": "medium",
      "learning_objective": "MVS.",
      "slide_refs": [
        1
      ]
    },
    {
      "id": "pkg20_mcq7",
      "question": "Hough transform:",
      "options": {
        "A": "Parametric detection",
        "B": "Region growing",
        "C": "Flow estimation",
        "D": "Color segmentation"
      },
      "correct_option": "A",
      "difficulty": "medium",
      "learning_objective": "Hough.",
      "slide_refs": [
        1
      ]
    },
    {
      "id": "pkg20_mcq8",
      "question": "MSER detects:",
      "options": {
        "A": "Stable regions across thresholds",
        "B": "Corners only",
        "C": "Lines",
        "D": "Motion"
      },
      "correct_option": "A",
      "difficulty": "hard",
      "learning_objective": "MSER principle.",
      "slide_refs": [
        1
      ]
    },
    {
      "id": "pkg20_mcq9",
      "question": "Optical flow applications include:",
      "options": {
        "A": "Video stabilization, tracking",
        "B": "Static image classification",
        "C": "Lens calibration",
        "D": "Histogram equalization"
      },
      "correct_option": "A",
      "difficulty": "medium",
      "learning_objective": "Flow usage.",
      "slide_refs": [
        1
      ]
    },
    {
      "id": "pkg20_mcq10",
      "question": "Roberts masks are for:",
      "options": {
        "A": "Diagonal edge detection",
        "B": "Smoothing",
        "C": "Sharpening",
        "D": "Color conversion"
      },
      "correct_option": "A",
      "difficulty": "medium",
      "learning_objective": "Roberts operator."
    },
    {
      "id": "pkg18_mcq1",
      "question": "HSV color space is useful for:",
      "options": {
        "A": "Intuitive color manipulation separating hue from intensity",
        "B": "Device calibration",
        "C": "3D modeling",
        "D": "Motion detection"
      },
      "correct_option": "A",
      "difficulty": "easy",
      "learning_objective": "HSV advantages.",
      "slide_refs": [
        1
      ]
    },
    {
      "id": "pkg18_mcq2",
      "question": "Radial distortion is modeled as:",
      "options": {
        "A": "Polynomial function of radius",
        "B": "Linear shear",
        "C": "Color shift",
        "D": "Noise addition"
      },
      "correct_option": "A",
      "difficulty": "medium",
      "learning_objective": "Radial distortion model.",
      "slide_refs": [
        1
      ]
    },
    {
      "id": "pkg18_mcq3",
      "question": "Aperture problem in flow:",
      "options": {
        "A": "One equation, two unknowns per pixel",
        "B": "Global illumination change",
        "C": "Sensor saturation",
        "D": "Lens flare"
      },
      "correct_option": "A",
      "difficulty": "hard",
      "learning_objective": "Flow ambiguity.",
      "slide_refs": [
        1
      ]
    },
    {
      "id": "pkg18_mcq4",
      "question": "Unsupervised learning finds:",
      "options": {
        "A": "Hidden structures without labels",
        "B": "Labeled predictions",
        "C": "Optimal paths",
        "D": "Camera poses"
      },
      "correct_option": "A",
      "difficulty": "easy",
      "learning_objective": "Unsupervised definition.",
      "slide_refs": [
        1
      ]
    },
    {
      "id": "pkg18_mcq5",
      "question": "A feature descriptor should be:",
      "options": {
        "A": "Invariant to transformations, discriminative",
        "B": "Global only",
        "C": "Color-dependent",
        "D": "High-dimensional always"
      },
      "correct_option": "A",
      "difficulty": "medium",
      "learning_objective": "Descriptor properties.",
      "slide_refs": [
        1
      ]
    },
    {
      "id": "pkg18_mcq6",
      "question": "Diffuse reflection is:",
      "options": {
        "A": "Lambertian, view-independent",
        "B": "Specular, mirror-like",
        "C": "Transparent",
        "D": "Emissive"
      },
      "correct_option": "A",
      "difficulty": "medium",
      "learning_objective": "Reflection types.",
      "slide_refs": [
        1
      ]
    },
    {
      "id": "pkg18_mcq7",
      "question": "Epipolar constraint reduces:",
      "options": {
        "A": "2D search to 1D line",
        "B": "3D to 2D",
        "C": "Color space",
        "D": "Texture dimensions"
      },
      "correct_option": "A",
      "difficulty": "medium",
      "learning_objective": "Epipolar utility.",
      "slide_refs": [
        1
      ]
    },
    {
      "id": "pkg18_mcq8",
      "question": "Optical flow assumes:",
      "options": {
        "A": "Brightness constancy and small motion",
        "B": "Large displacements",
        "C": "No occlusion",
        "D": "Known depths"
      },
      "correct_option": "A",
      "difficulty": "hard",
      "learning_objective": "Flow assumptions.",
      "slide_refs": [
        1
      ]
    },
    {
      "id": "pkg18_mcq9",
      "question": "LBP is rotation invariant when:",
      "options": {
        "A": "Using uniform patterns and rotation shifting",
        "B": "No modifications",
        "C": "With color",
        "D": "In 3D"
      },
      "correct_option": "A",
      "difficulty": "hard",
      "learning_objective": "LBP variants.",
      "slide_refs": [
        1
      ]
    },
    {
      "id": "pkg18_mcq10",
      "question": "Texture synthesis generates:",
      "options": {
        "A": "New samples from model",
        "B": "Only analysis",
        "C": "Depth maps",
        "D": "Motion fields"
      },
      "correct_option": "A",
      "difficulty": "medium",
      "learning_objective": "Texture tasks.",
      "slide_refs": [
        1
      ]
    },
    {
      "id": "pkg19_mcq1",
      "question": "Optical flow computes:",
      "options": {
        "A": "Pixel displacements over time",
        "B": "Static features",
        "C": "Color models",
        "D": "3D poses"
      },
      "correct_option": "A",
      "difficulty": "easy",
      "learning_objective": "Optical flow.",
      "slide_refs": [
        1
      ]
    },
    {
      "id": "pkg19_mcq2",
      "question": "SIFT detects:",
      "options": {
        "A": "Scale-invariant keypoints",
        "B": "Lines only",
        "C": "Colors",
        "D": "Motion"
      },
      "correct_option": "A",
      "difficulty": "medium",
      "learning_objective": "SIFT features.",
      "slide_refs": [
        1
      ]
    },
    {
      "id": "pkg19_mcq3",
      "question": "HSV is:",
      "options": {
        "A": "Cylindrical color model",
        "B": "Additive primaries",
        "C": "Opponent space",
        "D": "Device space"
      },
      "correct_option": "A",
      "difficulty": "easy",
      "learning_objective": "HSV.",
      "slide_refs": [
        1
      ]
    },
    {
      "id": "pkg19_mcq4",
      "question": "Aperture problem:",
      "options": {
        "A": "Local motion ambiguity",
        "B": "Global consistency",
        "C": "Color variation",
        "D": "Depth estimation"
      },
      "correct_option": "A",
      "difficulty": "medium",
      "learning_objective": "Aperture issue.",
      "slide_refs": [
        1
      ]
    },
    {
      "id": "pkg19_mcq5",
      "question": "Confusion matrix evaluates:",
      "options": {
        "A": "Classification errors",
        "B": "Regression fit",
        "C": "Clustering",
        "D": "Compression"
      },
      "correct_option": "A",
      "difficulty": "medium",
      "learning_objective": "Confusion matrix.",
      "slide_refs": [
        1
      ]
    },
    {
      "id": "pkg19_mcq6",
      "question": "Unsupervised learning is:",
      "options": {
        "A": "Pattern discovery without labels",
        "B": "Labeled training",
        "C": "Reward-based",
        "D": "Supervised validation"
      },
      "correct_option": "A",
      "difficulty": "easy",
      "learning_objective": "Unsupervised.",
      "slide_refs": [
        1
      ]
    },
    {
      "id": "pkg19_mcq7",
      "question": "Hough transform:",
      "options": {
        "A": "Detects lines via voting",
        "B": "Segments regions",
        "C": "Estimates flow",
        "D": "Balances colors"
      },
      "correct_option": "A",
      "difficulty": "medium",
      "learning_objective": "Hough.",
      "slide_refs": [
        1
      ]
    },
    {
      "id": "pkg19_mcq8",
      "question": "Bag of words in vision:",
      "options": {
        "A": "Histogram of visual words for classification",
        "B": "Text recognition",
        "C": "3D modeling",
        "D": "Motion analysis"
      },
      "correct_option": "A",
      "difficulty": "hard",
      "learning_objective": "BoW model.",
      "slide_refs": [
        1
      ]
    },
    {
      "id": "pkg19_mcq9",
      "question": "Harris detects:",
      "options": {
        "A": "Corners via structure tensor",
        "B": "Blobs",
        "C": "Lines",
        "D": "Circles"
      },
      "correct_option": "A",
      "difficulty": "medium",
      "learning_objective": "Harris detector.",
      "slide_refs": [
        1
      ]
    },
    {
      "id": "pkg19_mcq10",
      "question": "Morphology improves segmentation by:",
      "options": {
        "A": "Erosion/dilation for noise/holes",
        "B": "Gradient computation",
        "C": "Feature matching",
        "D": "Color conversion"
      },
      "correct_option": "A",
      "difficulty": "medium",
      "learning_objective": "Morphology usage."
    },
    {
      "id": "pkg21_mcq1",
      "question": "A feature descriptor is used to:",
      "options": {
        "A": "Match points across images",
        "B": "Calibrate cameras",
        "C": "Segment regions",
        "D": "Estimate depth"
      },
      "correct_option": "A",
      "difficulty": "easy",
      "learning_objective": "Descriptor role.",
      "slide_refs": [
        1
      ]
    },
    {
      "id": "pkg21_mcq2",
      "question": "Bayer filter arranges:",
      "options": {
        "A": "RGBG pattern for color capture",
        "B": "Edge filters",
        "C": "Motion sensors",
        "D": "Depth pixels"
      },
      "correct_option": "A",
      "difficulty": "medium",
      "learning_objective": "Bayer demosaicing.",
      "slide_refs": [
        1
      ]
    },
    {
      "id": "pkg21_mcq3",
      "question": "Aperture problem:",
      "options": {
        "A": "Local flow underdetermined",
        "B": "Global flow",
        "C": "Color problem",
        "D": "Distortion"
      },
      "correct_option": "A",
      "difficulty": "medium",
      "learning_objective": "Aperture.",
      "slide_refs": [
        1
      ]
    },
    {
      "id": "pkg21_mcq4",
      "question": "Hough transform:",
      "options": {
        "A": "Votes for lines/curves",
        "B": "Clusters data",
        "C": "Thresholds images",
        "D": "Detects corners"
      },
      "correct_option": "A",
      "difficulty": "medium",
      "learning_objective": "Hough.",
      "slide_refs": [
        1
      ]
    },
    {
      "id": "pkg21_mcq5",
      "question": "Mahalanobis distance:",
      "options": {
        "A": "Scales by covariance",
        "B": "Is Euclidean",
        "C": "For binary",
        "D": "For histograms"
      },
      "correct_option": "A",
      "difficulty": "hard",
      "learning_objective": "Mahalanobis.",
      "slide_refs": [
        1
      ]
    },
    {
      "id": "pkg21_mcq6",
      "question": "Grayscale histogram:",
      "options": {
        "A": "Intensity counts",
        "B": "Spatial map",
        "C": "Gradient hist",
        "D": "Color hist"
      },
      "correct_option": "A",
      "difficulty": "easy",
      "learning_objective": "Histogram.",
      "slide_refs": [
        1
      ]
    },
    {
      "id": "pkg21_mcq7",
      "question": "K-means:",
      "options": {
        "A": "Iterative centroid assignment",
        "B": "Hierarchical",
        "C": "Density-based",
        "D": "Spectral"
      },
      "correct_option": "A",
      "difficulty": "medium",
      "learning_objective": "k-means.",
      "slide_refs": [
        1
      ]
    },
    {
      "id": "pkg21_mcq8",
      "question": "Otsu's method:",
      "options": {
        "A": "Optimal threshold by variance",
        "B": "Adaptive threshold",
        "C": "Multi-threshold",
        "D": "Color threshold"
      },
      "correct_option": "A",
      "difficulty": "medium",
      "learning_objective": "Otsu.",
      "slide_refs": [
        1
      ]
    },
    {
      "id": "pkg21_mcq9",
      "question": "Harris corner uses:",
      "options": {
        "A": "Eigenvalues of structure tensor",
        "B": "Laplacian",
        "C": "Hessian",
        "D": "DoG"
      },
      "correct_option": "A",
      "difficulty": "hard",
      "learning_objective": "Harris.",
      "slide_refs": [
        1
      ]
    },
    {
      "id": "pkg21_mcq10",
      "question": "In stereo, z = f b / d, for d=2, b=6, f=1, z=:",
      "options": {
        "A": "3 cm",
        "B": "0.33 cm",
        "C": "12 cm",
        "D": "6 cm"
      },
      "correct_option": "A",
      "difficulty": "easy",
      "learning_objective": "Compute depth."
    },
    {
      "id": "pkg17_mcq1",
      "question": "Optical flow is the:",
      "options": {
        "A": "Apparent motion of pixels between frames",
        "B": "3D velocity field",
        "C": "Camera trajectory",
        "D": "Light direction"
      },
      "correct_option": "A",
      "difficulty": "easy",
      "learning_objective": "Define optical flow.",
      "slide_refs": [
        1
      ]
    },
    {
      "id": "pkg17_mcq2",
      "question": "SIFT is a:",
      "options": {
        "A": "Scale-invariant feature transform for detection and description",
        "B": "Color space",
        "C": "Clustering algorithm",
        "D": "Distance metric"
      },
      "correct_option": "A",
      "difficulty": "medium",
      "learning_objective": "Recall SIFT.",
      "slide_refs": [
        1
      ]
    },
    {
      "id": "pkg17_mcq3",
      "question": "A histogram is:",
      "options": {
        "A": "Frequency count of values",
        "B": "Spatial map",
        "C": "Gradient filter",
        "D": "Transformation matrix"
      },
      "correct_option": "A",
      "difficulty": "easy",
      "learning_objective": "Histogram definition.",
      "slide_refs": [
        1
      ]
    },
    {
      "id": "pkg17_mcq4",
      "question": "K-nearest neighbor classifies by:",
      "options": {
        "A": "Majority vote of k closest training examples",
        "B": "Centroid calculation",
        "C": "Linear regression",
        "D": "Neural activation"
      },
      "correct_option": "A",
      "difficulty": "medium",
      "learning_objective": "kNN principle.",
      "slide_refs": [
        1
      ]
    },
    {
      "id": "pkg17_mcq5",
      "question": "HSV separates:",
      "options": {
        "A": "Hue, saturation, value",
        "B": "Red, green, blue",
        "C": "Luminance, chrominance",
        "D": "Opponent colors"
      },
      "correct_option": "A",
      "difficulty": "easy",
      "learning_objective": "HSV components.",
      "slide_refs": [
        1
      ]
    },
    {
      "id": "pkg17_mcq6",
      "question": "Hough transform is for:",
      "options": {
        "A": "Parametric shape detection",
        "B": "Texture analysis",
        "C": "Color correction",
        "D": "Motion estimation"
      },
      "correct_option": "A",
      "difficulty": "medium",
      "learning_objective": "Hough usage.",
      "slide_refs": [
        1
      ]
    },
    {
      "id": "pkg17_mcq7",
      "question": "RANSAC handles:",
      "options": {
        "A": "Outliers in model fitting",
        "B": "Image noise only",
        "C": "Color balancing",
        "D": "Clustering"
      },
      "correct_option": "A",
      "difficulty": "medium",
      "learning_objective": "RANSAC principle.",
      "slide_refs": [
        1
      ]
    },
    {
      "id": "pkg17_mcq8",
      "question": "Mahalanobis distance is:",
      "options": {
        "A": "Covariance-weighted Euclidean",
        "B": "Manhattan distance",
        "C": "Cosine similarity",
        "D": "Hamming distance"
      },
      "correct_option": "A",
      "difficulty": "hard",
      "learning_objective": "Define Mahalanobis.",
      "slide_refs": [
        1
      ]
    },
    {
      "id": "pkg17_mcq9",
      "question": "Morphological operations can:",
      "options": {
        "A": "Clean segmentation by removing noise, filling holes",
        "B": "Compute gradients",
        "C": "Estimate motion",
        "D": "Classify colors"
      },
      "correct_option": "A",
      "difficulty": "medium",
      "learning_objective": "Morphology in segmentation.",
      "slide_refs": [
        1
      ]
    },
    {
      "id": "pkg17_mcq10",
      "question": "For texture classification, filter banks provide:",
      "options": {
        "A": "Multi-scale responses for features",
        "B": "Only edge detection",
        "C": "Color features",
        "D": "3D depths"
      },
      "correct_option": "A",
      "difficulty": "hard",
      "learning_objective": "Filter banks for texture.",
      "slide_refs": [
        1
      ]
    },
    {
      "id": "pkg2_mcq1",
      "question": "Which of the following is a monocular 3D cue that relies on the assumption that parallel lines in the real world converge to a vanishing point in the image?",
      "options": {
        "A": "Texture gradient",
        "B": "Interposition",
        "C": "Vanishing points",
        "D": "Motion parallax"
      },
      "correct_option": "C",
      "difficulty": "easy",
      "learning_objective": "Identify monocular 3D cues available in single 2D images.",
      "slide_refs": [
        3
      ]
    },
    {
      "id": "pkg2_mcq2",
      "question": "In the context of shape from shading, the image intensity \\(I\\) at a point \\(P\\) under diffuse reflection and unit albedo is given by \\(I = \\mathbf{N} \\cdot \\mathbf{L}\\). What does \\(\\mathbf{L}\\) represent?",
      "options": {
        "A": "Surface normal at point \\(P\\)",
        "B": "Incident light direction",
        "C": "Camera viewing direction",
        "D": "Reflected light intensity"
      },
      "correct_option": "B",
      "difficulty": "easy",
      "learning_objective": "Recall the Lambertian reflectance model for shape from shading.",
      "slide_refs": [
        6
      ]
    },
    {
      "id": "pkg2_mcq3",
      "question": "Which of the following statements about T-junctions is correct?",
      "options": {
        "A": "They indicate that the occluding object is farther away.",
        "B": "Two aligning T-junctions provide stronger evidence of occlusion than a single T-junction.",
        "C": "They occur when two surfaces have the same depth.",
        "D": "They are irrelevant to relative depth estimation."
      },
      "correct_option": "B",
      "difficulty": "medium",
      "learning_objective": "Understand the role of T-junctions in occlusion and relative depth perception.",
      "slide_refs": [
        2
      ]
    },
    {
      "id": "pkg2_mcq4",
      "question": "In shape from texture, the surface normal \\(\\mathbf{N}\\) is related to the slant \\(\\phi\\) and tilt \\(\\tau\\). Which angle represents the orientation of the surface normal projected into the image plane?",
      "options": {
        "A": "Slant \\(\\phi\\)",
        "B": "Tilt \\(\\tau\\)",
        "C": "Both slant and tilt",
        "D": "Neither; it requires 3D coordinates"
      },
      "correct_option": "B",
      "difficulty": "medium",
      "learning_objective": "Differentiate between slant and tilt in surface orientation estimation from texture.",
      "slide_refs": [
        5
      ]
    },
    {
      "id": "pkg2_mcq5",
      "question": "Under the assumptions of shape from shading with known light source direction \\(\\mathbf{L}\\) and unit albedo, the equation \\(I = \\cos \\theta_i\\) implies that:",
      "options": {
        "A": "Surface orientation is uniquely determined from intensity alone.",
        "B": "Additional constraints (e.g., integrability, smoothness) are required to recover the surface.",
        "C": "Only convex surfaces can be reconstructed.",
        "D": "The light source must be at infinity."
      },
      "correct_option": "B",
      "difficulty": "hard",
      "learning_objective": "Recognize the ill-posed nature of shape from shading and the need for regularization.",
      "slide_refs": [
        7
      ]
    },
    {
      "id": "pkg2_mcq6",
      "question": "In a 3D affine transformation represented in homogeneous coordinates, the bottom row of the 4×4 transformation matrix is:",
      "options": {
        "A": "[1 0 0 0]",
        "B": "[0 1 0 0]",
        "C": "[0 0 1 0]",
        "D": "[0 0 0 1]"
      },
      "correct_option": "D",
      "difficulty": "easy",
      "learning_objective": "Recall the structure of homogeneous coordinates for 3D affine transformations.",
      "slide_refs": [
        9
      ]
    },
    {
      "id": "pkg2_mcq7",
      "question": "A rigid 3D transformation consists of rotation and translation. How many degrees of freedom does it have?",
      "options": {
        "A": "3",
        "B": "6",
        "C": "9",
        "D": "12"
      },
      "correct_option": "B",
      "difficulty": "easy",
      "learning_objective": "Determine the degrees of freedom in rigid body motion.",
      "slide_refs": [
        9
      ]
    },
    {
      "id": "pkg2_mcq8",
      "question": "The rotation matrix \\(R_x(\\psi)\\) for rotation by angle \\(\\psi\\) about the x-axis is:",
      "options": {
        "A": "\\(\\begin{bmatrix} 1 & 0 & 0 \\\\ 0 & \\cos\\psi & -\\sin\\psi \\\\ 0 & \\sin\\psi & \\cos\\psi \\end{bmatrix}\\)",
        "B": "\\(\\begin{bmatrix} \\cos\\psi & 0 & \\sin\\psi \\\\ 0 & 1 & 0 \\\\ -\\sin\\psi & 0 & \\cos\\psi \\end{bmatrix}\\)",
        "C": "\\(\\begin{bmatrix} \\cos\\psi & -\\sin\\psi & 0 \\\\ \\sin\\psi & \\cos\\psi & 0 \\\\ 0 & 0 & 1 \\end{bmatrix}\\)",
        "D": "\\(\\begin{bmatrix} 1 & 0 & 0 \\\\ 0 & \\cos\\psi & \\sin\\psi \\\\ 0 & -\\sin\\psi & \\cos\\psi \\end{bmatrix}\\)"
      },
      "correct_option": "A",
      "difficulty": "medium",
      "learning_objective": "Recall the standard Euler rotation matrix for rotation about the x-axis.",
      "slide_refs": [
        11
      ]
    },
    {
      "id": "pkg2_mcq9",
      "question": "Which of the following is a limitation of the Euler angle representation for 3D rotations?",
      "options": {
        "A": "It uses more than 3 parameters.",
        "B": "It suffers from gimbal lock.",
        "C": "It cannot represent arbitrary rotations.",
        "D": "It is not orthogonal."
      },
      "correct_option": "B",
      "difficulty": "medium",
      "learning_objective": "Understand the drawbacks of Euler angles in rotation representation.",
      "slide_refs": [
        12
      ]
    },
    {
      "id": "pkg2_mcq10",
      "question": "In structured light 3D reconstruction, shadows can reveal surface shape because:",
      "options": {
        "A": "They indicate regions of self-occlusion.",
        "B": "They bend according to the surface normal.",
        "C": "They are independent of lighting direction.",
        "D": "They only appear under diffuse lighting."
      },
      "correct_option": "B",
      "difficulty": "hard",
      "learning_objective": "Explain how structured illumination aids in depth perception.",
      "slide_refs": [
        4
      ]
    },
    {
      "id": "pkg1_mcq1",
      "question": "Which of the following best describes a limitation of 'informed consent' in the context of data mining, as discussed in the lecture slides?",
      "options": {
        "A": "Informed consent is always sufficient because once users agree they waive all privacy rights.",
        "B": "Informed consent may be insufficient because data mining can discover unforeseen sensitive information that could not be fully described to subjects in advance.",
        "C": "Informed consent is unnecessary when datasets are anonymised because anonymisation removes all privacy risk.",
        "D": "Informed consent is only required for data collected in clinical trials, not for routine digital traces."
      },
      "correct_option": "B",
      "difficulty": "medium",
      "learning_objective": "Recognise limits of informed consent for large-scale data mining and privacy implications",
      "slide_refs": [
        10,
        11,
        12,
        26
      ]
    },
    {
      "id": "pkg1_mcq2",
      "question": "Which anonymisation approach (mentioned in the slides) explicitly requires that each released record be indistinguishable from at least k-1 other records with respect to a set of quasi-identifiers?",
      "options": {
        "A": "Pseudonymisation",
        "B": "k-anonymity",
        "C": "Encryption",
        "D": "Differential privacy"
      },
      "correct_option": "B",
      "difficulty": "easy",
      "learning_objective": "Identify k-anonymity as a structural anonymisation technique for protecting privacy",
      "slide_refs": [
        29,
        31
      ]
    },
    {
      "id": "pkg1_mcq3",
      "question": "According to the lecture slides, which statement correctly contrasts relational databases and document-oriented NoSQL databases (e.g., MongoDB)?",
      "options": {
        "A": "Relational databases permit embedded documents and lack a fixed schema, while MongoDB enforces a strict schema across all documents.",
        "B": "Relational databases provide a standardised query language (SQL) and strong schema constraints; document stores offer flexible schemas but less standardisation.",
        "C": "Document stores always provide stronger consistency guarantees than relational databases.",
        "D": "Relational databases are unsuitable for any scalable or distributed use; NoSQL databases are the only scalable option."
      },
      "correct_option": "B",
      "difficulty": "medium",
      "learning_objective": "Compare trade-offs between relational and NoSQL systems (schema, standardisation, consistency, flexibility)",
      "slide_refs": [
        60,
        61,
        63
      ]
    },
    {
      "id": "pkg3_mcq1",
      "question": "Which of the following is **not** a common cause of missing data as described in the lecture?",
      "options": {
        "A": "Equipment malfunction or sensor failure",
        "B": "Data corruption during storage or transfer",
        "C": "Overfitting of predictive models",
        "D": "Non-response in surveys"
      },
      "correct_option": "C",
      "difficulty": "easy",
      "learning_objective": "Identify typical causes of missing values in datasets.",
      "slide_refs": [
        3,
        5
      ]
    },
    {
      "id": "pkg3_mcq2",
      "question": "In the context of missing data mechanisms, which statement best describes **Missing Completely at Random (MCAR)**?",
      "options": {
        "A": "The probability of a value being missing depends on other observed variables but not on the missing value itself.",
        "B": "The probability of a value being missing is related to the value that is missing.",
        "C": "The probability of a value being missing is independent of both observed and unobserved data.",
        "D": "The probability of a value being missing depends on unmeasured latent variables."
      },
      "correct_option": "C",
      "difficulty": "medium",
      "learning_objective": "Distinguish between MCAR, MAR, and MNAR mechanisms in data collection.",
      "slide_refs": [
        6,
        7,
        8
      ]
    },
    {
      "id": "pkg3_mcq3",
      "question": "Which deletion method uses all available data points for each variable pair when computing correlations?",
      "options": {
        "A": "Listwise deletion",
        "B": "Pairwise deletion",
        "C": "Hot deck imputation",
        "D": "Regression imputation"
      },
      "correct_option": "B",
      "difficulty": "medium",
      "learning_objective": "Differentiate between listwise and pairwise deletion strategies for handling missing data.",
      "slide_refs": [
        10,
        11
      ]
    },
    {
      "id": "pkg3_mcq4",
      "question": "Which imputation technique replaces missing values with the mean (numerical) or mode (categorical) of the variable?",
      "options": {
        "A": "Hot deck imputation",
        "B": "Regression imputation",
        "C": "Mean imputation",
        "D": "Multiple imputation"
      },
      "correct_option": "C",
      "difficulty": "easy",
      "learning_objective": "Understand basic imputation methods and their assumptions.",
      "slide_refs": [
        14,
        15
      ]
    },
    {
      "id": "pkg3_mcq5",
      "question": "Which imputation technique involves fitting a regression model on observed data to predict the missing values, assuming MAR holds?",
      "options": {
        "A": "Regression imputation",
        "B": "Hot deck imputation",
        "C": "k-Nearest Neighbour (kNN) imputation",
        "D": "Maximum likelihood estimation"
      },
      "correct_option": "A",
      "difficulty": "medium",
      "learning_objective": "Explain the process and assumptions of regression-based imputation.",
      "slide_refs": [
        16,
        17
      ]
    },
    {
      "id": "pkg3_mcq6",
      "question": "What is the main advantage of **multiple imputation** over single imputation methods?",
      "options": {
        "A": "It produces identical estimates across imputations, simplifying analysis.",
        "B": "It avoids uncertainty estimation and reduces computational complexity.",
        "C": "It accounts for the uncertainty in missing data by generating multiple plausible datasets and pooling results.",
        "D": "It always yields unbiased estimates, regardless of missingness type."
      },
      "correct_option": "C",
      "difficulty": "hard",
      "learning_objective": "Understand the rationale, steps, and advantages of multiple imputation.",
      "slide_refs": [
        21,
        22,
        23
      ]
    },
    {
      "id": "pkg3_mcq7",
      "question": "Which of the following statements about the **Expectation-Maximization (EM)** algorithm is true?",
      "options": {
        "A": "It is mainly used for categorical data sets.",
        "B": "It alternates between estimating missing data expectations and maximizing the likelihood function.",
        "C": "It requires no iteration and converges in a single step.",
        "D": "It always overestimates standard errors compared to multiple imputation."
      },
      "correct_option": "B",
      "difficulty": "hard",
      "learning_objective": "Explain the EM algorithm’s role in maximum likelihood estimation with missing data.",
      "slide_refs": [
        26,
        27,
        28
      ]
    },
    {
      "id": "pkg3_mcq8",
      "question": "In time series or longitudinal data, why can simple deletion lead to biased results?",
      "options": {
        "A": "Because deletion assumes MAR always holds.",
        "B": "Because deletion can disrupt temporal dependencies or autocorrelations.",
        "C": "Because deletion alters class distributions in classification tasks.",
        "D": "Because deletion introduces new missingness mechanisms."
      },
      "correct_option": "B",
      "difficulty": "medium",
      "learning_objective": "Recognize challenges of handling missing data in time-dependent datasets.",
      "slide_refs": [
        29
      ]
    },
    {
      "id": "pkg4_mcq1",
      "question": "When merging data from multiple sources, what is one key requirement for combining sensor data measuring the same phenomenon?",
      "options": {
        "A": "Sensors must all measure in different units for diversity.",
        "B": "Sensors must have identical sampling frequencies.",
        "C": "Sensors must come from the same manufacturer.",
        "D": "Sensors must be used under identical environmental conditions."
      },
      "correct_option": "B",
      "difficulty": "medium",
      "learning_objective": "Understand the technical requirements for merging multisource sensor data.",
      "slide_refs": [
        5,
        6
      ]
    },
    {
      "id": "pkg4_mcq2",
      "question": "Which of the following is a potential issue when merging datasets from different sources?",
      "options": {
        "A": "Identical timestamping and data alignment",
        "B": "Differences in data formats, units, and time representations",
        "C": "Overrepresentation of rare classes",
        "D": "High recall but low precision"
      },
      "correct_option": "B",
      "difficulty": "easy",
      "learning_objective": "Recognize format, unit, and timestamp issues in multisource data integration.",
      "slide_refs": [
        6,
        7
      ]
    },
    {
      "id": "pkg4_mcq3",
      "question": "What is the primary problem introduced by **downsampling** when aligning signals from different sensors?",
      "options": {
        "A": "Loss of temporal synchronization",
        "B": "Increased data noise",
        "C": "Loss of information due to reduced data density",
        "D": "Bias toward high-frequency data"
      },
      "correct_option": "C",
      "difficulty": "medium",
      "learning_objective": "Evaluate trade-offs in downsampling during data fusion.",
      "slide_refs": [
        12
      ]
    },
    {
      "id": "pkg4_mcq4",
      "question": "What is the **main consequence** of oversampling to match the least common multiple (LCM) of sampling rates?",
      "options": {
        "A": "Reduced computational time",
        "B": "Loss of information from low-frequency sensors",
        "C": "Increased data size and computational load",
        "D": "Destruction of temporal structure"
      },
      "correct_option": "C",
      "difficulty": "medium",
      "learning_objective": "Identify computational trade-offs of oversampling in multi-rate data integration.",
      "slide_refs": [
        13
      ]
    },
    {
      "id": "pkg4_mcq5",
      "question": "Which sampling method ensures every observation has an equal chance to be selected but is not replaced once drawn?",
      "options": {
        "A": "SRSWR (Simple Random Sampling With Replacement)",
        "B": "SRSWOR (Simple Random Sampling Without Replacement)",
        "C": "Balanced Sampling",
        "D": "Cluster Sampling"
      },
      "correct_option": "B",
      "difficulty": "easy",
      "learning_objective": "Differentiate between basic random sampling strategies.",
      "slide_refs": [
        17
      ]
    },
    {
      "id": "pkg4_mcq6",
      "question": "In the SMOTE algorithm, how are synthetic samples created?",
      "options": {
        "A": "By duplicating minority class samples exactly",
        "B": "By adding Gaussian noise to majority class data points",
        "C": "By interpolating between a sample and its k nearest neighbors",
        "D": "By randomly assigning new class labels to samples"
      },
      "correct_option": "C",
      "difficulty": "hard",
      "learning_objective": "Understand the SMOTE technique for addressing class imbalance.",
      "slide_refs": [
        23,
        24,
        25
      ]
    },
    {
      "id": "pkg4_mcq7",
      "question": "Which performance metric tends to overestimate model performance on imbalanced datasets?",
      "options": {
        "A": "Precision",
        "B": "Recall",
        "C": "Accuracy",
        "D": "F1 Score"
      },
      "correct_option": "C",
      "difficulty": "medium",
      "learning_objective": "Recognize limitations of accuracy as a performance measure in imbalanced data.",
      "slide_refs": [
        32,
        35
      ]
    },
    {
      "id": "pkg4_mcq8",
      "question": "Which formula correctly defines **balanced accuracy**?",
      "options": {
        "A": "Balanced accuracy = (Precision + Recall)/2",
        "B": "Balanced accuracy = (TP/(TP+FN) + TN/(TN+FP))/2",
        "C": "Balanced accuracy = (TP + TN)/(TP + FP + TN + FN)",
        "D": "Balanced accuracy = 2 * (Precision * Recall)/(Precision + Recall)"
      },
      "correct_option": "B",
      "difficulty": "hard",
      "learning_objective": "Apply the correct formula for balanced accuracy in performance evaluation.",
      "slide_refs": [
        36
      ]
    },
    {
      "id": "pkg4_mcq9",
      "question": "Which of the following metrics captures the balance between precision and recall?",
      "options": {
        "A": "Specificity",
        "B": "Accuracy",
        "C": "F1 Score",
        "D": "Cohen’s Kappa"
      },
      "correct_option": "C",
      "difficulty": "medium",
      "learning_objective": "Interpret and apply F1-score in evaluating classifier performance.",
      "slide_refs": [
        40
      ]
    },
    {
      "id": "pkg4_mcq10",
      "question": "Cohen’s Kappa is primarily used to:",
      "options": {
        "A": "Measure agreement between observed and predicted labels beyond chance",
        "B": "Estimate the percentage of correctly classified samples",
        "C": "Quantify the recall rate in multi-class problems",
        "D": "Compute mean differences between class probabilities"
      },
      "correct_option": "A",
      "difficulty": "hard",
      "learning_objective": "Explain Cohen’s Kappa as an inter-rater and classification agreement metric.",
      "slide_refs": [
        41
      ]
    },
    {
      "id": "pkg2_mcq1",
      "question": "According to the lecture, what was the main cause of the NASA Mars Climate Orbiter mission failure in 1999?",
      "options": {
        "A": "A software virus corrupted the navigation data.",
        "B": "Engineers failed to convert measurement units between English and metric systems.",
        "C": "The lander’s parachute deployed prematurely due to faulty sensors.",
        "D": "Incorrect sample size estimation led to navigation errors."
      },
      "correct_option": "B",
      "difficulty": "easy",
      "learning_objective": "Recognize the importance of consistent units and measurement standards in data collection and engineering.",
      "slide_refs": [
        6,
        7
      ]
    },
    {
      "id": "pkg2_mcq2",
      "question": "Which of the following best defines a non-stationary element in data collection as discussed in the lecture?",
      "options": {
        "A": "An element that maintains constant statistical properties over time.",
        "B": "An element whose characteristics or state change over time due to environmental or human factors.",
        "C": "An element that cannot be measured repeatedly with the same instrument.",
        "D": "An element that is unaffected by sensor drift or external conditions."
      },
      "correct_option": "B",
      "difficulty": "medium",
      "learning_objective": "Differentiate between stationary and non-stationary elements and understand their implications for data collection.",
      "slide_refs": [
        19,
        20
      ]
    },
    {
      "id": "pkg2_mcq3",
      "question": "Which principle from human research ethics emphasizes that individuals should be treated as autonomous agents and given additional protections if autonomy is diminished?",
      "options": {
        "A": "Beneficence",
        "B": "Justice",
        "C": "Respect for persons",
        "D": "Confidentiality"
      },
      "correct_option": "C",
      "difficulty": "medium",
      "learning_objective": "Identify the three core ethical principles governing human data collection and apply them appropriately.",
      "slide_refs": [
        26,
        27,
        28
      ]
    },
    {
      "id": "pkg8_mcq1",
      "question": "In data mining, what does 'generalizability' of a model refer to?",
      "options": {
        "A": "The ability of the model to fit the training data perfectly.",
        "B": "How well the model performs on new or unseen data.",
        "C": "The capacity of the model to memorize patterns from noise.",
        "D": "The degree to which the model parameters can be freely adjusted."
      },
      "correct_option": "B",
      "difficulty": "easy",
      "learning_objective": "Define model generalization and its importance in evaluating model performance.",
      "slide_refs": [
        6,
        7
      ]
    },
    {
      "id": "pkg8_mcq2",
      "question": "Which type of model is primarily used to test causal hypotheses and explain relationships between variables?",
      "options": {
        "A": "Descriptive model",
        "B": "Predictive model",
        "C": "Explanatory model",
        "D": "Diagnostic model"
      },
      "correct_option": "C",
      "difficulty": "medium",
      "learning_objective": "Distinguish between descriptive, predictive, and explanatory models in data mining.",
      "slide_refs": [
        11,
        13
      ]
    },
    {
      "id": "pkg8_mcq3",
      "question": "Predictive modeling focuses on:",
      "options": {
        "A": "Identifying causal mechanisms in data.",
        "B": "Summarizing existing patterns without forecasting.",
        "C": "Using training data to predict outputs for new observations.",
        "D": "Visualizing relationships between features."
      },
      "correct_option": "C",
      "difficulty": "easy",
      "learning_objective": "Understand the purpose and structure of predictive modeling.",
      "slide_refs": [
        12
      ]
    },
    {
      "id": "pkg8_mcq4",
      "question": "Which of the following statements correctly explains why explanatory and predictive models differ?",
      "options": {
        "A": "Explanatory models optimize accuracy; predictive models optimize interpretability.",
        "B": "Explanatory models are data-driven; predictive models are theory-driven.",
        "C": "Explanatory models aim for causal inference, while predictive models aim for accuracy on new data.",
        "D": "There are no practical differences between them."
      },
      "correct_option": "C",
      "difficulty": "medium",
      "learning_objective": "Explain the conceptual difference between explanatory and predictive modeling.",
      "slide_refs": [
        13,
        15
      ]
    },
    {
      "id": "pkg8_mcq5",
      "question": "What problem often occurs when the training dataset is too small?",
      "options": {
        "A": "Underfitting due to excessive model complexity.",
        "B": "Overfitting, leading to poor generalization on test data.",
        "C": "Reduced variance in model predictions.",
        "D": "Higher confidence intervals around estimates."
      },
      "correct_option": "B",
      "difficulty": "medium",
      "learning_objective": "Recognize the impact of sample size on model overfitting and generalizability.",
      "slide_refs": [
        18
      ]
    },
    {
      "id": "pkg8_mcq6",
      "question": "Which data partitioning strategy helps to ensure a model’s generalization ability?",
      "options": {
        "A": "Training only on the full dataset.",
        "B": "Using independent training and test sets with 80-20 or 2/3-1/3 split.",
        "C": "Evaluating on the same data used for training.",
        "D": "Merging training and test sets for better accuracy."
      },
      "correct_option": "B",
      "difficulty": "easy",
      "learning_objective": "Understand the role of data partitioning in training and testing models.",
      "slide_refs": [
        25
      ]
    },
    {
      "id": "pkg8_mcq7",
      "question": "What is the primary purpose of a validation set during model development?",
      "options": {
        "A": "To fine-tune model hyperparameters and prevent overfitting.",
        "B": "To evaluate final model performance on unseen data.",
        "C": "To reduce the size of the training set for faster computation.",
        "D": "To randomly mix samples from the test set."
      },
      "correct_option": "A",
      "difficulty": "medium",
      "learning_objective": "Differentiate between validation and test sets in model selection and tuning.",
      "slide_refs": [
        26
      ]
    },
    {
      "id": "pkg8_mcq8",
      "question": "Which statement about k-fold cross-validation is true?",
      "options": {
        "A": "It is used only when datasets are extremely large.",
        "B": "Each observation is used once for validation and multiple times for training.",
        "C": "It is identical to leave-one-out validation regardless of k.",
        "D": "It prevents any need for a separate test set."
      },
      "correct_option": "B",
      "difficulty": "medium",
      "learning_objective": "Understand how k-fold cross-validation balances training and testing efficiency.",
      "slide_refs": [
        28
      ]
    },
    {
      "id": "pkg8_mcq9",
      "question": "Leave-one-out validation is particularly characterized by:",
      "options": {
        "A": "High computational cost for large datasets.",
        "B": "Using 50% of data for training and 50% for testing.",
        "C": "Strong bias due to limited resampling.",
        "D": "Completely eliminating variance in results."
      },
      "correct_option": "A",
      "difficulty": "medium",
      "learning_objective": "Recognize the strengths and limitations of leave-one-out validation.",
      "slide_refs": [
        29
      ]
    },
    {
      "id": "pkg8_mcq10",
      "question": "Population models differ from individual models in that:",
      "options": {
        "A": "Population models generalize better across individuals.",
        "B": "Individual models require fewer observations.",
        "C": "Population models are prone to overfitting individual-level noise.",
        "D": "Individual models are always less interpretable."
      },
      "correct_option": "A",
      "difficulty": "medium",
      "learning_objective": "Compare individual-level and population-level models regarding generalization.",
      "slide_refs": [
        30
      ]
    },
    {
      "id": "pkg8_mcq11",
      "question": "When handling temporally dependent data, why is random sampling inappropriate for creating training and test sets?",
      "options": {
        "A": "It causes information leakage between temporally related samples.",
        "B": "It increases model bias but not variance.",
        "C": "Temporal data do not require validation.",
        "D": "It ensures test data contain only anomalies."
      },
      "correct_option": "A",
      "difficulty": "hard",
      "learning_objective": "Identify correct data partitioning strategies for time-dependent datasets.",
      "slide_refs": [
        36,
        37
      ]
    },
    {
      "id": "pkg8_mcq12",
      "question": "What is the main goal of feature extraction using sliding windows in time series data?",
      "options": {
        "A": "To transform raw data into fixed-length statistical features.",
        "B": "To increase data redundancy for training.",
        "C": "To segment continuous data into random samples.",
        "D": "To remove all temporal dependencies between samples."
      },
      "correct_option": "A",
      "difficulty": "medium",
      "learning_objective": "Explain feature extraction through segmentation and summarize its role in model efficiency.",
      "slide_refs": [
        37
      ]
    },
    {
      "id": "pkg8_mcq13",
      "question": "According to the lecture, simpler models tend to generalize better because:",
      "options": {
        "A": "They have more parameters to capture complex data patterns.",
        "B": "They reduce overfitting and improve interpretability.",
        "C": "They use less training data and lower variance estimates.",
        "D": "They assume perfect model fit."
      },
      "correct_option": "B",
      "difficulty": "medium",
      "learning_objective": "Understand the relationship between model complexity, overfitting, and generalization.",
      "slide_refs": [
        39
      ]
    },
    {
      "id": "pkg6_mcq1",
      "question": "Which of the following best defines 'attribute noise' in data mining?",
      "options": {
        "A": "Errors or missing values in data features rather than in class labels.",
        "B": "Incorrect class labels due to human bias.",
        "C": "Inconsistencies between data sources during merging.",
        "D": "Random variations in signal saturation levels."
      },
      "correct_option": "A",
      "difficulty": "easy",
      "learning_objective": "Identify different types of noise in data, particularly attribute and label noise.",
      "slide_refs": [
        3
      ]
    },
    {
      "id": "pkg6_mcq2",
      "question": "Which of the following approaches focuses on correcting noisy instances before model training?",
      "options": {
        "A": "Data polishing",
        "B": "Noise filtering",
        "C": "Robust learning",
        "D": "Feature normalization"
      },
      "correct_option": "A",
      "difficulty": "medium",
      "learning_objective": "Understand the role of data polishing as a preprocessing technique to handle noise.",
      "slide_refs": [
        4
      ]
    },
    {
      "id": "pkg6_mcq3",
      "question": "Noise filters that target specific frequencies, such as 50 Hz from power lines, primarily operate on which kind of data?",
      "options": {
        "A": "Categorical data",
        "B": "Continuous signals",
        "C": "Textual datasets",
        "D": "Sparse matrices"
      },
      "correct_option": "B",
      "difficulty": "medium",
      "learning_objective": "Recognize noise filtering applications in continuous signal data.",
      "slide_refs": [
        5
      ]
    },
    {
      "id": "pkg6_mcq4",
      "question": "What characterizes signal saturation during data collection?",
      "options": {
        "A": "A signal’s amplitude exceeds its maximum measurable value, causing flat peaks at threshold limits.",
        "B": "Signal values are randomly distributed across the expected range.",
        "C": "The sampling rate is too low to capture full waveform detail.",
        "D": "The sensor produces noisy outputs due to temperature drift."
      },
      "correct_option": "A",
      "difficulty": "easy",
      "learning_objective": "Explain the concept and implications of signal saturation in sensor data.",
      "slide_refs": [
        7
      ]
    },
    {
      "id": "pkg6_mcq5",
      "question": "What distinguishes an anomaly from an outlier according to the lecture?",
      "options": {
        "A": "An anomaly is always erroneous; an outlier is usually valid.",
        "B": "An anomaly represents a rare but valid event, while an outlier may be noise or error.",
        "C": "Anomalies and outliers are statistically identical concepts.",
        "D": "Anomalies occur only in categorical data."
      },
      "correct_option": "B",
      "difficulty": "medium",
      "learning_objective": "Differentiate between anomalies and outliers in the context of data mining.",
      "slide_refs": [
        9
      ]
    },
    {
      "id": "pkg6_mcq6",
      "question": "Which of the following describes the correct workflow for handling harmful outliers?",
      "options": {
        "A": "Detect → Normalize → Delete",
        "B": "Investigate → Correct → Recollect",
        "C": "Detect → Investigate → Correct or treat as missing",
        "D": "Ignore → Smooth → Recluster"
      },
      "correct_option": "C",
      "difficulty": "medium",
      "learning_objective": "Apply systematic procedures for identifying and managing harmful outliers.",
      "slide_refs": [
        12
      ]
    },
    {
      "id": "pkg6_mcq7",
      "question": "A group of points that collectively deviates from the overall dataset, but whose individual members may not be extreme, are called:",
      "options": {
        "A": "Global outliers",
        "B": "Contextual outliers",
        "C": "Collective outliers",
        "D": "Statistical residuals"
      },
      "correct_option": "C",
      "difficulty": "medium",
      "learning_objective": "Classify outlier types and identify examples of collective outliers.",
      "slide_refs": [
        14
      ]
    },
    {
      "id": "pkg6_mcq8",
      "question": "Why is building a universal model for outlier detection often impractical?",
      "options": {
        "A": "Data distributions and contexts vary widely across applications.",
        "B": "All datasets contain perfectly labeled outliers.",
        "C": "Outlier thresholds are globally defined constants.",
        "D": "Outlier detection always requires supervised learning."
      },
      "correct_option": "A",
      "difficulty": "medium",
      "learning_objective": "Understand why application-specific context complicates outlier detection.",
      "slide_refs": [
        15
      ]
    },
    {
      "id": "pkg6_mcq9",
      "question": "Which of the following statements about supervised outlier detection is correct?",
      "options": {
        "A": "It trains only on outliers to classify normal data.",
        "B": "It uses labeled data to distinguish normal from abnormal patterns.",
        "C": "It requires no labeled examples at all.",
        "D": "It assumes all outliers are randomly distributed."
      },
      "correct_option": "B",
      "difficulty": "medium",
      "learning_objective": "Explain how supervised learning can be used for outlier detection.",
      "slide_refs": [
        16
      ]
    },
    {
      "id": "pkg6_mcq10",
      "question": "Which of the following is an example of a nonparametric statistical outlier detection method?",
      "options": {
        "A": "Gaussian distribution modeling",
        "B": "Kernel density estimation",
        "C": "Linear regression residual analysis",
        "D": "Support Vector Machine"
      },
      "correct_option": "B",
      "difficulty": "hard",
      "learning_objective": "Differentiate between parametric and nonparametric statistical approaches to outlier detection.",
      "slide_refs": [
        17
      ]
    },
    {
      "id": "pkg6_mcq11",
      "question": "What is the key idea behind proximity-based outlier detection methods?",
      "options": {
        "A": "Outliers are data points with significantly fewer neighbors within a certain distance threshold.",
        "B": "Outliers form dense clusters that differ from normal data clusters.",
        "C": "Outliers always belong to a specific known distribution.",
        "D": "Outliers are identified using time-series autocorrelation."
      },
      "correct_option": "A",
      "difficulty": "medium",
      "learning_objective": "Describe the principle behind distance and density-based proximity methods.",
      "slide_refs": [
        18
      ]
    },
    {
      "id": "pkg6_mcq12",
      "question": "Which method can identify both global and local outliers by comparing densities in local neighborhoods?",
      "options": {
        "A": "k-Nearest Neighbor (kNN)",
        "B": "Local Outlier Factor (LOF)",
        "C": "Gaussian Mixture Model (GMM)",
        "D": "Principal Component Analysis (PCA)"
      },
      "correct_option": "B",
      "difficulty": "hard",
      "learning_objective": "Explain the Local Outlier Factor method for density-based detection.",
      "slide_refs": [
        18
      ]
    },
    {
      "id": "pkg6_mcq13",
      "question": "In clustering-based outlier detection, normal data typically belong to:",
      "options": {
        "A": "Sparse, scattered clusters",
        "B": "Large and dense clusters",
        "C": "No clusters at all",
        "D": "Temporal patterns in time series"
      },
      "correct_option": "B",
      "difficulty": "medium",
      "learning_objective": "Understand how clustering methods distinguish normal data from outliers.",
      "slide_refs": [
        19
      ]
    },
    {
      "id": "pkg6_mcq14",
      "question": "Which classification-based approach models only the normal class to detect outliers?",
      "options": {
        "A": "Random Forest Classifier",
        "B": "One-Class Support Vector Machine (SVM)",
        "C": "Naïve Bayes Classifier",
        "D": "Logistic Regression"
      },
      "correct_option": "B",
      "difficulty": "medium",
      "learning_objective": "Recognize the function of one-class models in outlier detection.",
      "slide_refs": [
        19
      ]
    },
    {
      "id": "pkg6_mcq15",
      "question": "Which of the following is most applicable for real-time outlier detection in time series data?",
      "options": {
        "A": "Autoregressive models and streaming analysis",
        "B": "Balanced sampling and cross-validation",
        "C": "Principal component projection",
        "D": "Batch-mode K-Means clustering"
      },
      "correct_option": "A",
      "difficulty": "medium",
      "learning_objective": "Identify time-series methods suitable for streaming outlier detection.",
      "slide_refs": [
        20
      ]
    },
    {
      "id": "pkg7_mcq1",
      "question": "Why is normalization often performed after outlier detection and noise removal?",
      "options": {
        "A": "It introduces artificial variation to enhance learning complexity.",
        "B": "It improves model performance, stabilizes learning, and simplifies result interpretation.",
        "C": "It reduces data dimensionality through feature extraction.",
        "D": "It ensures outliers remain visible in data visualization."
      },
      "correct_option": "B",
      "difficulty": "medium",
      "learning_objective": "Understand why normalization follows noise and outlier handling in preprocessing pipelines.",
      "slide_refs": [
        3
      ]
    },
    {
      "id": "pkg7_mcq2",
      "question": "What is the purpose of normalization or scaling in data preprocessing?",
      "options": {
        "A": "To convert categorical data to numeric codes.",
        "B": "To adjust values on different scales to a common range.",
        "C": "To remove multicollinearity among variables.",
        "D": "To enforce Gaussian distribution on non-normal data."
      },
      "correct_option": "B",
      "difficulty": "easy",
      "learning_objective": "Define normalization and identify its primary goal in data preprocessing.",
      "slide_refs": [
        5
      ]
    },
    {
      "id": "pkg7_mcq3",
      "question": "Which normalization technique rescales values to the range [0, 1]?",
      "options": {
        "A": "Z-score standardization",
        "B": "Min-Max normalization",
        "C": "Decimal scaling",
        "D": "Box-Cox transformation"
      },
      "correct_option": "B",
      "difficulty": "easy",
      "learning_objective": "Differentiate between common normalization and scaling methods.",
      "slide_refs": [
        9
      ]
    },
    {
      "id": "pkg7_mcq4",
      "question": "Z-score standardization is particularly suitable when:",
      "options": {
        "A": "Minimum and maximum population values are unknown.",
        "B": "All variables are categorical.",
        "C": "Data contains negative values that must be converted to positive.",
        "D": "The dataset is uniform without outliers."
      },
      "correct_option": "A",
      "difficulty": "medium",
      "learning_objective": "Identify scenarios where Z-score standardization is preferable.",
      "slide_refs": [
        10
      ]
    },
    {
      "id": "pkg7_mcq5",
      "question": "Decimal scaling normalization works by:",
      "options": {
        "A": "Subtracting the mean and dividing by standard deviation.",
        "B": "Shifting the decimal point based on the largest absolute value.",
        "C": "Dividing all data by the mean of the variable.",
        "D": "Applying logarithmic transformation for skewed data."
      },
      "correct_option": "B",
      "difficulty": "medium",
      "learning_objective": "Explain the process of decimal scaling normalization.",
      "slide_refs": [
        11
      ]
    },
    {
      "id": "pkg7_mcq6",
      "question": "When is it appropriate to discretize a continuous variable?",
      "options": {
        "A": "When researchers believe distinct groups exist or categorical methods are required.",
        "B": "When there are no meaningful patterns in the data.",
        "C": "When the data already consists of ordinal variables.",
        "D": "Only when missing values must be replaced."
      },
      "correct_option": "A",
      "difficulty": "medium",
      "learning_objective": "Recognize justifiable uses of discretization in data preprocessing.",
      "slide_refs": [
        15
      ]
    },
    {
      "id": "pkg7_mcq7",
      "question": "What is a major drawback of dichotomizing continuous variables?",
      "options": {
        "A": "It introduces nonlinearity into the dataset.",
        "B": "It causes loss of information and potential misclassification.",
        "C": "It prevents visualizations such as scatter plots.",
        "D": "It increases variance and statistical noise."
      },
      "correct_option": "B",
      "difficulty": "medium",
      "learning_objective": "Evaluate the trade-offs associated with discretization or dichotomization.",
      "slide_refs": [
        19,
        20
      ]
    },
    {
      "id": "pkg7_mcq8",
      "question": "Which of the following methods can handle categorical variables in regression models?",
      "options": {
        "A": "Principal Component Analysis",
        "B": "Box-Cox transformation",
        "C": "Dummy coding",
        "D": "Logarithmic scaling"
      },
      "correct_option": "C",
      "difficulty": "easy",
      "learning_objective": "Understand how dummy variables are used for categorical data in regression.",
      "slide_refs": [
        23
      ]
    },
    {
      "id": "pkg7_mcq9",
      "question": "What is the main goal of data reduction?",
      "options": {
        "A": "To simplify data while maintaining analytical integrity.",
        "B": "To remove all redundant and correlated features.",
        "C": "To discard irrelevant data to fit memory constraints only.",
        "D": "To normalize data to Gaussian distribution."
      },
      "correct_option": "A",
      "difficulty": "medium",
      "learning_objective": "Explain the purpose of data reduction in data mining workflows.",
      "slide_refs": [
        25
      ]
    },
    {
      "id": "pkg7_mcq10",
      "question": "Which data reduction method groups continuous values into intervals to simplify analysis?",
      "options": {
        "A": "Aggregation",
        "B": "Binning",
        "C": "Clustering",
        "D": "Sampling"
      },
      "correct_option": "B",
      "difficulty": "easy",
      "learning_objective": "Identify common value reduction methods such as binning and aggregation.",
      "slide_refs": [
        26
      ]
    },
    {
      "id": "pkg7_mcq11",
      "question": "Principal Component Analysis (PCA) primarily aims to:",
      "options": {
        "A": "Increase the dimensionality of feature space.",
        "B": "Select the most correlated variables.",
        "C": "Reduce dimensionality by identifying orthogonal components explaining maximum variance.",
        "D": "Convert categorical variables into dummy variables."
      },
      "correct_option": "C",
      "difficulty": "hard",
      "learning_objective": "Describe the goal and process of PCA for dimensionality reduction.",
      "slide_refs": [
        31
      ]
    },
    {
      "id": "pkg7_mcq12",
      "question": "Which of the following statements correctly differentiates PCA and Discrete Wavelet Transform (DWT)?",
      "options": {
        "A": "PCA uses variance-based selection; DWT uses coefficient magnitude-based selection.",
        "B": "PCA requires categorical data; DWT works on text data only.",
        "C": "DWT is non-linear; PCA is always non-parametric.",
        "D": "Both require combining samples across all datasets."
      },
      "correct_option": "A",
      "difficulty": "hard",
      "learning_objective": "Compare PCA and DWT as dimension reduction techniques.",
      "slide_refs": [
        32
      ]
    },
    {
      "id": "pkg7_mcq13",
      "question": "Box-Cox transformation is applicable only when:",
      "options": {
        "A": "The data contains both positive and negative values.",
        "B": "The dataset contains only positive values.",
        "C": "The data is already normalized to [0,1].",
        "D": "The data is categorical."
      },
      "correct_option": "B",
      "difficulty": "medium",
      "learning_objective": "Recognize constraints and applicability of Box-Cox transformations.",
      "slide_refs": [
        36
      ]
    },
    {
      "id": "pkg7_mcq14",
      "question": "Which of the following transformations can help achieve normality when data violates Gaussian assumptions?",
      "options": {
        "A": "Min-max scaling",
        "B": "Box-Cox transformation",
        "C": "Dummy coding",
        "D": "Equal-width discretization"
      },
      "correct_option": "B",
      "difficulty": "medium",
      "learning_objective": "Apply transformation techniques to achieve data normality for parametric analysis.",
      "slide_refs": [
        35,
        36
      ]
    },
    {
      "id": "pkg10_mcq1",
      "question": "The pinhole camera model assumes that a 3D world point \\((X, Y, Z)\\) projects to an image point \\((x, y)\\) through perspective projection. Under the standard pinhole model with the camera center at the origin and image plane at \\(Z = f\\), the correct projection equations are:",
      "options": {
        "A": "\\(x = X/Z, \\; y = Y/Z\\)",
        "B": "\\(x = f \\cdot X/Y, \\; y = f \\cdot Z/Y\\)",
        "C": "\\(x = f \\cdot X/Z, \\; y = f \\cdot Y/Z\\)",
        "D": "\\(x = f \\cdot Y/Z, \\; y = f \\cdot X/Z\\)"
      },
      "correct_option": "C",
      "difficulty": "easy",
      "learning_objective": "Recall the basic perspective projection equations of the pinhole camera model.",
      "slide_refs": [
        8
      ]
    },
    {
      "id": "pkg10_mcq2",
      "question": "In homogeneous coordinates, the full 3×4 camera projection matrix \\(P\\) for a pinhole camera with intrinsic matrix \\(K\\) and extrinsic parameters \\([R \\; t]\\) is given by:",
      "options": {
        "A": "\\(P = K [R \\; t]\\)",
        "B": "\\(P = [R \\; t] K\\)",
        "C": "\\(P = K [I \\; 0] [R \\; t]\\)",
        "D": "\\(P = [K \\; 0] [R \\; t]\\)"
      },
      "correct_option": "A",
      "difficulty": "medium",
      "learning_objective": "Derive the general form of the camera projection matrix in homogeneous coordinates.",
      "slide_refs": [
        12
      ]
    },
    {
      "id": "pkg10_mcq3",
      "question": "The intrinsic parameter \\(\\alpha_x = f \\cdot m_x\\) represents:",
      "options": {
        "A": "The skew between image axes",
        "B": "The focal length in horizontal pixel units",
        "C": "The principal point offset in y-direction",
        "D": "The aspect ratio correction factor"
      },
      "correct_option": "B",
      "difficulty": "medium",
      "learning_objective": "Interpret the components of the intrinsic calibration matrix \\(K\\).",
      "slide_refs": [
        10
      ]
    },
    {
      "id": "pkg10_mcq4",
      "question": "A finite camera has exactly how many degrees of freedom in its projection matrix \\(P\\)?",
      "options": {
        "A": "6",
        "B": "9",
        "C": "11",
        "D": "12"
      },
      "correct_option": "C",
      "difficulty": "hard",
      "learning_objective": "Understand the dimensionality of general projective cameras.",
      "slide_refs": [
        14
      ]
    },
    {
      "id": "pkg10_mcq5",
      "question": "Radial distortion in real lenses causes straight lines in the world to appear curved in the image. The distortion model commonly used corrects coordinates \\((x_d, y_d)\\) to undistorted \\((x_u, y_u)\\) via:",
      "options": {
        "A": "\\(x_u = x_d (1 + k_1 r^2 + k_2 r^4)\\)",
        "B": "\\(x_d = x_u (1 + k_1 r^2 + k_2 r^4)\\)",
        "C": "\\(x_u = x_d + t_x, \\; y_u = y_d + t_y\\)",
        "D": "\\(x_u = \\alpha x_d + \\beta y_d\\)"
      },
      "correct_option": "B",
      "difficulty": "medium",
      "learning_objective": "Recall the polynomial radial distortion model and its direction of correction.",
      "slide_refs": [
        18
      ]
    },
    {
      "id": "pkg10_mcq6",
      "question": "The principal point \\((c_x, c_y)\\) is defined as:",
      "options": {
        "A": "The center of the sensor array",
        "B": "The optical axis intersection with the image plane",
        "C": "The pixel with maximum intensity",
        "D": "The vanishing point of parallel lines"
      },
      "correct_option": "B",
      "difficulty": "easy",
      "learning_objective": "Define the role of the principal point in camera calibration.",
      "slide_refs": [
        10
      ]
    },
    {
      "id": "pkg10_mcq7",
      "question": "In the pinhole model, a point at infinity in 3D (direction vector) projects to:",
      "options": {
        "A": "The image origin",
        "B": "A vanishing point in the image",
        "C": "The principal point",
        "D": "No valid image point"
      },
      "correct_option": "B",
      "difficulty": "hard",
      "learning_objective": "Explain the imaging of points at infinity and vanishing points.",
      "slide_refs": [
        15
      ]
    },
    {
      "id": "pkg10_mcq8",
      "question": "Camera calibration using a known 3D calibration object (e.g., checkerboard) typically solves for:",
      "options": {
        "A": "Only intrinsic parameters",
        "B": "Only extrinsic parameters per view",
        "C": "Both intrinsic \\(K\\) and per-view extrinsics \\(R, t\\)",
        "D": "Only radial distortion coefficients"
      },
      "correct_option": "C",
      "difficulty": "medium",
      "learning_objective": "Describe the standard camera calibration pipeline.",
      "slide_refs": [
        20
      ]
    },
    {
      "id": "pkg10_mcq9",
      "question": "The skew parameter \\(s\\) in the intrinsic matrix is zero when:",
      "options": {
        "A": "Pixel aspect ratio is 1:1",
        "B": "Image axes are orthogonal (no shearing)",
        "C": "Focal length equals sensor size",
        "D": "Lens has no radial distortion"
      },
      "correct_option": "B",
      "difficulty": "hard",
      "learning_objective": "Interpret the skew parameter in modern digital cameras.",
      "slide_refs": [
        10
      ]
    },
    {
      "id": "pkg10_mcq10",
      "question": "The weak perspective (scaled orthographic) approximation is valid when:",
      "options": {
        "A": "Scene depth variation is small compared to average distance to camera",
        "B": "Camera is at infinity",
        "C": "Objects are perfectly planar",
        "D": "Lens distortion is negligible"
      },
      "correct_option": "A",
      "difficulty": "medium",
      "learning_objective": "Recognize conditions under which perspective effects can be approximated linearly.",
      "slide_refs": [
        22
      ]
    },
    {
      "id": "pkg7_mcq1",
      "question": "What is the primary goal of texture analysis in computer vision?",
      "options": {
        "A": "To reconstruct 3D geometry from a single image",
        "B": "To characterize local image patterns for segmentation, classification, or shape inference",
        "C": "To detect and track moving objects in video",
        "D": "To calibrate camera intrinsic parameters"
      },
      "correct_option": "B",
      "difficulty": "easy",
      "learning_objective": "Understand the role of texture in image interpretation.",
      "slide_refs": [
        5
      ]
    },
    {
      "id": "pkg7_mcq2",
      "question": "Texture can be used as a monocular cue for 3D shape perception because:",
      "options": {
        "A": "It remains constant under perspective projection",
        "B": "Regular texture elements distort predictably with surface orientation (slant and tilt)",
        "C": "It is invariant to lighting conditions",
        "D": "It encodes depth directly in intensity"
      },
      "correct_option": "B",
      "difficulty": "medium",
      "learning_objective": "Explain how texture gradient provides surface orientation cues.",
      "slide_refs": [
        6
      ]
    },
    {
      "id": "pkg7_mcq3",
      "question": "In the context of shape from texture, the two key parameters describing surface orientation are:",
      "options": {
        "A": "Scale and rotation",
        "B": "Slant (\\(\\phi\\)) and tilt (\\(\\tau\\))",
        "C": "Translation and shear",
        "D": "Foreshortening and compression"
      },
      "correct_option": "B",
      "difficulty": "easy",
      "learning_objective": "Recall the geometric parameters in texture-based surface estimation.",
      "slide_refs": [
        7
      ]
    },
    {
      "id": "pkg7_mcq4",
      "question": "Under the isotropy assumption in shape from texture, the texture elements are:",
      "options": {
        "A": "Perfectly circular in 3D",
        "B": "Statistically identical in all directions on the surface",
        "C": "Aligned with the image axes",
        "D": "Projected with uniform scale"
      },
      "correct_option": "B",
      "difficulty": "medium",
      "learning_objective": "State the isotropy assumption and its implication for foreshortening.",
      "slide_refs": [
        8
      ]
    },
    {
      "id": "pkg7_mcq5",
      "question": "The foreshortening effect in projected texture causes:",
      "options": {
        "A": "Texture elements to appear larger when surface is tilted away",
        "B": "Compression of texture in the direction of maximum slant",
        "C": "Uniform scaling in all directions",
        "D": "Rotation without distortion"
      },
      "correct_option": "B",
      "difficulty": "medium",
      "learning_objective": "Describe how slant affects texture element appearance.",
      "slide_refs": [
        9
      ]
    },
    {
      "id": "pkg7_mcq6",
      "question": "Which of the following is a common method to estimate local texture distortion for shape from texture?",
      "options": {
        "A": "Applying global Fourier transform",
        "B": "Fitting ellipses to the autocorrelation or power spectrum of local patches",
        "C": "Using Harris corner detection",
        "D": "Computing optical flow"
      },
      "correct_option": "B",
      "difficulty": "hard",
      "learning_objective": "Identify spectral or statistical methods for measuring texture anisotropy.",
      "slide_refs": [
        12
      ]
    },
    {
      "id": "pkg7_mcq7",
      "question": "The tilt angle \\(\\tau\\) in shape from texture corresponds to:",
      "options": {
        "A": "The magnitude of surface slant",
        "B": "The direction in the image plane toward which the surface is most slanted",
        "C": "The scale factor of texture elements",
        "D": "The camera focal length"
      },
      "correct_option": "B",
      "difficulty": "medium",
      "learning_objective": "Differentiate between slant and tilt in surface orientation.",
      "slide_refs": [
        7
      ]
    },
    {
      "id": "pkg7_mcq8",
      "question": "A major limitation of shape from texture methods is:",
      "options": {
        "A": "They require stereo image pairs",
        "B": "They assume texture is regular and isotropic in 3D, which is often violated",
        "C": "They cannot handle colored textures",
        "D": "They are computationally more expensive than shape from shading"
      },
      "correct_option": "B",
      "difficulty": "hard",
      "learning_objective": "Recognize real-world violations of texture assumptions.",
      "slide_refs": [
        15
      ]
    },
    {
      "id": "pkg7_mcq9",
      "question": "In texture segmentation, co-occurrence matrices are used to capture:",
      "options": {
        "A": "Global frequency content",
        "B": "Second-order statistics of intensity pairs at specific offsets",
        "C": "Edge orientations only",
        "D": "Motion vectors"
      },
      "correct_option": "B",
      "difficulty": "medium",
      "learning_objective": "Recall classical texture descriptors based on spatial relationships.",
      "slide_refs": [
        18
      ]
    },
    {
      "id": "pkg7_mcq10",
      "question": "Which of the following is NOT a typical application of texture analysis mentioned in the lecture?",
      "options": {
        "A": "Material classification",
        "B": "Image retrieval by content",
        "C": "Absolute camera pose estimation",
        "D": "Surface defect detection"
      },
      "correct_option": "C",
      "difficulty": "easy",
      "learning_objective": "Identify standard use cases of texture in vision systems.",
      "slide_refs": [
        5
      ]
    },
    {
      "id": "pkg8_mcq1",
      "question": "What is the primary purpose of binarization in image processing?",
      "options": {
        "A": "To enhance color saturation",
        "B": "To separate foreground objects from the background using a single threshold",
        "C": "To compute optical flow",
        "D": "To perform 3D reconstruction"
      },
      "correct_option": "B",
      "difficulty": "easy",
      "learning_objective": "Understand the goal of converting a grayscale image into a binary representation.",
      "slide_refs": [
        5
      ]
    },
    {
      "id": "pkg8_mcq2",
      "question": "In global thresholding, the binary image \\(B(x,y)\\) is defined as:",
      "options": {
        "A": "\\(B(x,y) = I(x,y) > T\\)",
        "B": "\\(B(x,y) = I(x,y) \\times T\\)",
        "C": "\\(B(x,y) = \\max(I(x,y), T)\\)",
        "D": "\\(B(x,y) = I(x,y)^T\\)"
      },
      "correct_option": "A",
      "difficulty": "easy",
      "learning_objective": "Recall the mathematical formulation of thresholding.",
      "slide_refs": [
        6
      ]
    },
    {
      "id": "pkg8_mcq3",
      "question": "Which of the following is a common challenge when applying a fixed global threshold?",
      "options": {
        "A": "Perfect separation in all lighting conditions",
        "B": "Non-uniform illumination causing intensity variation across the image",
        "C": "Too many connected components",
        "D": "Loss of texture details only in color images"
      },
      "correct_option": "B",
      "difficulty": "medium",
      "learning_objective": "Identify limitations of global thresholding in real-world images.",
      "slide_refs": [
        7
      ]
    },
    {
      "id": "pkg8_mcq4",
      "question": "The 4-connected neighborhood of a pixel \\((x,y)\\) includes:",
      "options": {
        "A": "Only the pixel itself",
        "B": "Pixels at \\((x\\pm1, y)\\) and \\((x, y\\pm1)\\)",
        "C": "All eight surrounding pixels",
        "D": "Diagonal pixels only"
      },
      "correct_option": "B",
      "difficulty": "easy",
      "learning_objective": "Define pixel connectivity in binary images.",
      "slide_refs": [
        10
      ]
    },
    {
      "id": "pkg8_mcq5",
      "question": "In connected component labeling, a foreground object is defined as:",
      "options": {
        "A": "A maximal set of background pixels",
        "B": "A maximal 4- or 8-connected set of foreground pixels",
        "C": "Any pixel with intensity above 128",
        "D": "The boundary between foreground and background"
      },
      "correct_option": "B",
      "difficulty": "medium",
      "learning_objective": "Understand the concept of connected components in binary segmentation.",
      "slide_refs": [
        12
      ]
    },
    {
      "id": "pkg8_mcq6",
      "question": "Which morphological operation uses a structuring element to remove small noise blobs in a binary image?",
      "options": {
        "A": "Dilation",
        "B": "Erosion",
        "C": "Opening",
        "D": "Closing"
      },
      "correct_option": "C",
      "difficulty": "medium",
      "learning_objective": "Apply morphological operations to clean binary images.",
      "slide_refs": [
        15
      ]
    },
    {
      "id": "pkg8_mcq7",
      "question": "The operation of closing in binary morphology is equivalent to:",
      "options": {
        "A": "Erosion followed by dilation",
        "B": "Dilation followed by erosion",
        "C": "Opening followed by thresholding",
        "D": "Boundary extraction"
      },
      "correct_option": "B",
      "difficulty": "medium",
      "learning_objective": "Recall the sequence of operations in morphological closing.",
      "slide_refs": [
        16
      ]
    },
    {
      "id": "pkg8_mcq8",
      "question": "In the context of document image analysis, binarization is crucial for:",
      "options": {
        "A": "Color restoration",
        "B": "Enabling OCR by isolating text from paper background",
        "C": "Estimating camera motion",
        "D": "3D surface reconstruction"
      },
      "correct_option": "B",
      "difficulty": "easy",
      "learning_objective": "Recognize application-specific importance of binary processing.",
      "slide_refs": [
        18
      ]
    },
    {
      "id": "pkg8_mcq9",
      "question": "The two-pass algorithm for connected component labeling works by:",
      "options": {
        "A": "Scanning the image once and assigning final labels immediately",
        "B": "First assigning provisional labels and resolving equivalences in a second pass using Union-Find",
        "C": "Using depth-first search on the fly",
        "D": "Applying morphological dilation repeatedly"
      },
      "correct_option": "B",
      "difficulty": "hard",
      "learning_objective": "Describe efficient algorithms for labeling connected regions.",
      "slide_refs": [
        13
      ]
    },
    {
      "id": "pkg8_mcq10",
      "question": "Which of the following is NOT a typical post-processing step after binarization?",
      "options": {
        "A": "Morphological opening to remove salt noise",
        "B": "Connected component analysis to extract text lines",
        "C": "Applying Gaussian blur to restore gradients",
        "D": "Hole filling via morphological closing"
      },
      "correct_option": "C",
      "difficulty": "hard",
      "learning_objective": "Distinguish appropriate binary image cleanup operations.",
      "slide_refs": [
        17
      ]
    },
    {
      "id": "pkg9_mcq1",
      "question": "What are the three primary additive colors used in digital color displays?",
      "options": {
        "A": "Cyan, Magenta, Yellow",
        "B": "Red, Green, Blue",
        "C": "Hue, Saturation, Value",
        "D": "Lightness, a, b"
      },
      "correct_option": "B",
      "difficulty": "easy",
      "learning_objective": "Recall the basis of the RGB color model for image representation.",
      "slide_refs": [
        5
      ]
    },
    {
      "id": "pkg9_mcq2",
      "question": "In the CIE 1931 chromaticity diagram, the horseshoe-shaped boundary represents:",
      "options": {
        "A": "All possible RGB values",
        "B": "The locus of pure spectral colors (monochromatic light)",
        "C": "The gamut of typical CMYK printers",
        "D": "Perceptual uniformity in color differences"
      },
      "correct_option": "B",
      "difficulty": "medium",
      "learning_objective": "Understand the spectral locus in the standard chromaticity space.",
      "slide_refs": [
        10
      ]
    },
    {
      "id": "pkg9_mcq3",
      "question": "The dichromatic reflection model separates surface reflectance into:",
      "options": {
        "A": "Diffuse and specular components only",
        "B": "Body (diffuse) reflection and interface (specular) reflection",
        "C": "Ambient and directed lighting",
        "D": "Matte and glossy albedo"
      },
      "correct_option": "B",
      "difficulty": "medium",
      "learning_objective": "Recall the two-term reflection model for color highlight analysis.",
      "slide_refs": [
        15
      ]
    },
    {
      "id": "pkg9_mcq4",
      "question": "In the Shafer dichromatic model, specular highlights from a white light source on a colored surface lie along a line in RGB space that:",
      "options": {
        "A": "Passes through the origin",
        "B": "Is parallel to the lighting color vector",
        "C": "Connects the surface body color to the illuminant color",
        "D": "Is perpendicular to the surface normal"
      },
      "correct_option": "C",
      "difficulty": "hard",
      "learning_objective": "Explain the geometry of specular highlights in RGB color space.",
      "slide_refs": [
        16
      ]
    },
    {
      "id": "pkg9_mcq5",
      "question": "Which color space is designed to be perceptually uniform, meaning equal Euclidean distances correspond roughly to equal perceived color differences?",
      "options": {
        "A": "RGB",
        "B": "HSV",
        "C": "CIE L*a*b*",
        "D": "YCbCr"
      },
      "correct_option": "C",
      "difficulty": "medium",
      "learning_objective": "Identify color spaces intended for perceptual uniformity.",
      "slide_refs": [
        12
      ]
    },
    {
      "id": "pkg9_mcq6",
      "question": "The phenomenon where the perceived color of a surface remains relatively constant under changing illumination is known as:",
      "options": {
        "A": "Color quantization",
        "B": "Color constancy",
        "C": "Metamerism",
        "D": "Gamut mapping"
      },
      "correct_option": "B",
      "difficulty": "easy",
      "learning_objective": "Define the human visual system's ability to discount illuminant changes.",
      "slide_refs": [
        20
      ]
    },
    {
      "id": "pkg9_mcq7",
      "question": "A simple algorithm for computational color constancy estimates the illuminant by assuming:",
      "options": {
        "A": "The brightest pixel is a specular highlight from white light",
        "B": "The average color of the image is gray (Gray World assumption)",
        "C": "The darkest pixel represents black shadow",
        "D": "All colors are equally likely"
      },
      "correct_option": "B",
      "difficulty": "medium",
      "learning_objective": "Recall the Gray World hypothesis for illuminant estimation.",
      "slide_refs": [
        22
      ]
    },
    {
      "id": "pkg9_mcq8",
      "question": "In the context of specular highlight removal, clustering in RGB space works because:",
      "options": {
        "A": "Diffuse pixels form a compact cluster; specular pixels form a separate skewed cluster",
        "B": "All pixels lie on the Planckian locus",
        "C": "Specularities reduce saturation uniformly",
        "D": "Highlights are always white regardless of surface color"
      },
      "correct_option": "A",
      "difficulty": "hard",
      "learning_objective": "Apply the dichromatic model to separate reflection components via clustering.",
      "slide_refs": [
        18
      ]
    },
    {
      "id": "pkg9_mcq9",
      "question": "The conversion from RGB to HSV separates color into:",
      "options": {
        "A": "Luminance, chrominance, and saturation",
        "B": "Hue (dominant wavelength), Saturation (purity), Value (brightness)",
        "C": "Opponent red-green, yellow-blue, and light-dark channels",
        "D": "CIELAB lightness and chromatic opponents"
      },
      "correct_option": "B",
      "difficulty": "easy",
      "learning_objective": "Describe the intuitive components of the HSV color representation.",
      "slide_refs": [
        8
      ]
    },
    {
      "id": "pkg9_mcq10",
      "question": "Metamerism occurs when:",
      "options": {
        "A": "Two different spectral distributions produce the same tristimulus values (RGB)",
        "B": "A single surface appears different under two illuminants",
        "C": "The camera sensor has non-ideal spectral sensitivities",
        "D": "White balancing fails in post-processing"
      },
      "correct_option": "A",
      "difficulty": "hard",
      "learning_objective": "Understand the fundamental reason for color matching ambiguities.",
      "slide_refs": [
        9
      ]
    },
    {
      "id": "pkg15_mcq1",
      "question": "A time-of-flight camera measures:",
      "options": {
        "A": "Depth via light travel time",
        "B": "Color spectra",
        "C": "Motion vectors",
        "D": "Texture patterns"
      },
      "correct_option": "A",
      "difficulty": "easy",
      "learning_objective": "Define ToF depth sensing.",
      "slide_refs": [
        1
      ]
    },
    {
      "id": "pkg15_mcq2",
      "question": "Metamers are spectra that:",
      "options": {
        "A": "Match in color appearance but differ physically",
        "B": "Cause distortion",
        "C": "Define camera calibration",
        "D": "Represent textures"
      },
      "correct_option": "A",
      "difficulty": "medium",
      "learning_objective": "Explain metamerism.",
      "slide_refs": [
        1
      ]
    },
    {
      "id": "pkg15_mcq3",
      "question": "Connected component labeling assigns:",
      "options": {
        "A": "Unique IDs to contiguous foreground regions",
        "B": "Gradients to edges",
        "C": "Colors to pixels",
        "D": "Depths to points"
      },
      "correct_option": "A",
      "difficulty": "medium",
      "learning_objective": "Describe CCL in binary images.",
      "slide_refs": [
        1
      ]
    },
    {
      "id": "pkg15_mcq4",
      "question": "A local descriptor describes:",
      "options": {
        "A": "Neighborhood around a feature point",
        "B": "Global image",
        "C": "Camera pose",
        "D": "Light source"
      },
      "correct_option": "A",
      "difficulty": "easy",
      "learning_objective": "Define local descriptors.",
      "slide_refs": [
        1
      ]
    },
    {
      "id": "pkg15_mcq5",
      "question": "Motion parallax provides:",
      "options": {
        "A": "Depth cues from relative motion",
        "B": "Color constancy",
        "C": "Texture classification",
        "D": "Edge detection"
      },
      "correct_option": "A",
      "difficulty": "medium",
      "learning_objective": "Explain parallax as monocular cue.",
      "slide_refs": [
        1
      ]
    },
    {
      "id": "pkg15_mcq6",
      "question": "An epipolar line is:",
      "options": {
        "A": "The image of the ray from one camera in the other",
        "B": "A motion trajectory",
        "C": "A texture gradient",
        "D": "A color channel"
      },
      "correct_option": "A",
      "difficulty": "hard",
      "learning_objective": "Define epipolar lines.",
      "slide_refs": [
        1
      ]
    },
    {
      "id": "pkg15_mcq7",
      "question": "K-means minimizes:",
      "options": {
        "A": "Within-cluster sum of squares",
        "B": "Between-class variance",
        "C": "Edge strengths",
        "D": "Color differences"
      },
      "correct_option": "A",
      "difficulty": "medium",
      "learning_objective": "Principle of k-means.",
      "slide_refs": [
        1
      ]
    },
    {
      "id": "pkg15_mcq8",
      "question": "Photometric stereo recovers:",
      "options": {
        "A": "Normals from multiple illuminations",
        "B": "Motion from video",
        "C": "Textures from filters",
        "D": "Colors from metamer matching"
      },
      "correct_option": "A",
      "difficulty": "hard",
      "learning_objective": "Describe photometric stereo.",
      "slide_refs": [
        1
      ]
    },
    {
      "id": "pkg15_mcq9",
      "question": "Roberts gradient masks compute:",
      "options": {
        "A": "Diagonal derivatives",
        "B": "Horizontal only",
        "C": "Vertical only",
        "D": "Laplacian"
      },
      "correct_option": "A",
      "difficulty": "medium",
      "learning_objective": "Recall Roberts operator.",
      "slide_refs": [
        1
      ]
    },
    {
      "id": "pkg15_mcq10",
      "question": "LBP for texture classification uses:",
      "options": {
        "A": "Histogram of binary patterns",
        "B": "Mean gradients",
        "C": "Color histograms",
        "D": "3D models"
      },
      "correct_option": "A",
      "difficulty": "medium",
      "learning_objective": "Explain LBP-based recognition.",
      "slide_refs": [
        1
      ]
    },
    {
      "id": "pkg12_mcq1",
      "question": "What is an epipolar line in the context of stereo vision?",
      "options": {
        "A": "The line connecting the camera centers",
        "B": "The projection of the epipolar plane onto one image, constraining the search for corresponding points",
        "C": "A line of constant depth in the scene",
        "D": "The baseline between two cameras"
      },
      "correct_option": "B",
      "difficulty": "easy",
      "learning_objective": "Define the epipolar line and its role in correspondence search.",
      "slide_refs": [
        1
      ]
    },
    {
      "id": "pkg12_mcq2",
      "question": "A grayscale histogram represents:",
      "options": {
        "A": "The spatial distribution of pixels",
        "B": "The frequency of occurrence of each intensity level in the image",
        "C": "The gradient magnitudes at each pixel",
        "D": "The color channels separately"
      },
      "correct_option": "B",
      "difficulty": "easy",
      "learning_objective": "Understand the concept of image histograms.",
      "slide_refs": [
        1
      ]
    },
    {
      "id": "pkg12_mcq3",
      "question": "In HSV color space, 'H' stands for:",
      "options": {
        "A": "Hue, representing the dominant wavelength",
        "B": "Horizontal component",
        "C": "Highlight intensity",
        "D": "Histogram value"
      },
      "correct_option": "A",
      "difficulty": "easy",
      "learning_objective": "Recall the components of HSV color model.",
      "slide_refs": [
        1
      ]
    },
    {
      "id": "pkg12_mcq4",
      "question": "Optical flow estimates:",
      "options": {
        "A": "The 3D motion of the camera",
        "B": "The apparent 2D motion field in the image plane",
        "C": "Absolute depth values",
        "D": "Surface normals"
      },
      "correct_option": "B",
      "difficulty": "medium",
      "learning_objective": "Define optical flow and its computation.",
      "slide_refs": [
        1
      ]
    },
    {
      "id": "pkg12_mcq5",
      "question": "An interest point is typically:",
      "options": {
        "A": "A uniform region with no texture",
        "B": "A distinctive location in the image, such as a corner or blob, repeatable across views",
        "C": "The center of the image",
        "D": "A pixel with maximum intensity"
      },
      "correct_option": "B",
      "difficulty": "easy",
      "learning_objective": "Explain the concept of interest points or keypoints.",
      "slide_refs": [
        1
      ]
    },
    {
      "id": "pkg12_mcq6",
      "question": "Chromatic aberration is:",
      "options": {
        "A": "Distortion due to lens curvature",
        "B": "Color fringing caused by wavelength-dependent refraction in lenses",
        "C": "Noise in the sensor",
        "D": "Vignetting at image corners"
      },
      "correct_option": "B",
      "difficulty": "medium",
      "learning_objective": "Define chromatic aberration in imaging.",
      "slide_refs": [
        1
      ]
    },
    {
      "id": "pkg12_mcq7",
      "question": "RANSAC is primarily used for:",
      "options": {
        "A": "Image segmentation",
        "B": "Robust model fitting in the presence of outliers",
        "C": "Color correction",
        "D": "Edge detection"
      },
      "correct_option": "B",
      "difficulty": "medium",
      "learning_objective": "Describe the principle of RANSAC for robust estimation.",
      "slide_refs": [
        1
      ]
    },
    {
      "id": "pkg12_mcq8",
      "question": "A precision-recall curve evaluates:",
      "options": {
        "A": "Regression models",
        "B": "Binary classifiers by trading off precision and recall at different thresholds",
        "C": "Clustering quality",
        "D": "Image compression ratios"
      },
      "correct_option": "B",
      "difficulty": "medium",
      "learning_objective": "Understand precision-recall for performance evaluation.",
      "slide_refs": [
        1
      ]
    },
    {
      "id": "pkg12_mcq9",
      "question": "The SIFT descriptor is:",
      "options": {
        "A": "A global image feature",
        "B": "A local histogram-based descriptor invariant to scale and rotation",
        "C": "A color space transformation",
        "D": "A motion estimation algorithm"
      },
      "correct_option": "B",
      "difficulty": "medium",
      "learning_objective": "Recall the properties of SIFT descriptors.",
      "slide_refs": [
        1
      ]
    },
    {
      "id": "pkg12_mcq10",
      "question": "In homogeneous coordinates, the similarity transformation matrix has how many degrees of freedom?",
      "options": {
        "A": "3",
        "B": "4",
        "C": "6",
        "D": "8"
      },
      "correct_option": "B",
      "difficulty": "hard",
      "learning_objective": "Determine DOF for 2D transformations.",
      "slide_refs": [
        1
      ]
    },
    {
      "id": "pkg24_mcq1",
      "question": "Depth of field is the:",
      "options": {
        "A": "Range of distances in focus",
        "B": "Sensor size",
        "C": "Focal length",
        "D": "Aperture diameter"
      },
      "correct_option": "A",
      "difficulty": "easy",
      "learning_objective": "DOF definition.",
      "slide_refs": [
        1
      ]
    },
    {
      "id": "pkg24_mcq2",
      "question": "HSV color space:",
      "options": {
        "A": "Hue for color, value for intensity",
        "B": "RGB additive",
        "C": "CMYK subtractive",
        "D": "LAB perceptual"
      },
      "correct_option": "A",
      "difficulty": "easy",
      "learning_objective": "HSV.",
      "slide_refs": [
        1
      ]
    },
    {
      "id": "pkg24_mcq3",
      "question": "Diffuse reflection is view-independent because:",
      "options": {
        "A": "Scatters light equally",
        "B": "Reflects specularly",
        "C": "Absorbs all light",
        "D": "Transmits light"
      },
      "correct_option": "A",
      "difficulty": "medium",
      "learning_objective": "Diffuse vs specular.",
      "slide_refs": [
        1
      ]
    },
    {
      "id": "pkg24_mcq4",
      "question": "Confusion matrix tabulates:",
      "options": {
        "A": "Predictions vs actual classes",
        "B": "Feature correlations",
        "C": "Gradient directions",
        "D": "Motion fields"
      },
      "correct_option": "A",
      "difficulty": "medium",
      "learning_objective": "Matrix use.",
      "slide_refs": [
        1
      ]
    },
    {
      "id": "pkg24_mcq5",
      "question": "Local descriptor is for:",
      "options": {
        "A": "Matching local regions",
        "B": "Global alignment",
        "C": "Color balancing",
        "D": "Depth fusion"
      },
      "correct_option": "A",
      "difficulty": "easy",
      "learning_objective": "Descriptors.",
      "slide_refs": [
        1
      ]
    },
    {
      "id": "pkg24_mcq6",
      "question": "P3P problem solves:",
      "options": {
        "A": "Camera pose from 3 points",
        "B": "Stereo disparity",
        "C": "Texture classification",
        "D": "Color constancy"
      },
      "correct_option": "A",
      "difficulty": "hard",
      "learning_objective": "P3P.",
      "slide_refs": [
        1
      ]
    },
    {
      "id": "pkg24_mcq7",
      "question": "Image segmentation partitions into:",
      "options": {
        "A": "Meaningful regions",
        "B": "Random clusters",
        "C": "Individual pixels",
        "D": "Frequency bands"
      },
      "correct_option": "A",
      "difficulty": "easy",
      "learning_objective": "Segmentation.",
      "slide_refs": [
        1
      ]
    },
    {
      "id": "pkg24_mcq8",
      "question": "Optical flow for:",
      "options": {
        "A": "Motion estimation in video",
        "B": "Static images only",
        "C": "Color analysis",
        "D": "Lens calibration"
      },
      "correct_option": "A",
      "difficulty": "medium",
      "learning_objective": "Flow usage.",
      "slide_refs": [
        1
      ]
    },
    {
      "id": "pkg24_mcq9",
      "question": "Structure from motion uses:",
      "options": {
        "A": "Uncalibrated images for 3D",
        "B": "Known poses only",
        "C": "Single view",
        "D": "Shading cues"
      },
      "correct_option": "A",
      "difficulty": "hard",
      "learning_objective": "SfM.",
      "slide_refs": [
        1
      ]
    },
    {
      "id": "pkg24_mcq10",
      "question": "Essential matrix encodes:",
      "options": {
        "A": "Relative pose between views",
        "B": "Intrinsic parameters",
        "C": "Color transformation",
        "D": "Texture mapping"
      },
      "correct_option": "A",
      "difficulty": "hard",
      "learning_objective": "Essential matrix.",
      "slide_refs": [
        1
      ]
    },
    {
      "id": "pkg23_mcq1",
      "question": "The epipolar constraint states that:",
      "options": {
        "A": "Corresponding points lie on epipolar lines",
        "B": "Motion is rigid",
        "C": "Textures are isotropic",
        "D": "Colors are constant"
      },
      "correct_option": "A",
      "difficulty": "medium",
      "learning_objective": "Epipolar constraint.",
      "slide_refs": [
        1
      ]
    },
    {
      "id": "pkg23_mcq2",
      "question": "A decision tree splits data based on:",
      "options": {
        "A": "Features to minimize impurity",
        "B": "Random assignment",
        "C": "Distance metrics",
        "D": "Gradient descent"
      },
      "correct_option": "A",
      "difficulty": "medium",
      "learning_objective": "Decision trees.",
      "slide_refs": [
        1
      ]
    },
    {
      "id": "pkg23_mcq3",
      "question": "HSV is used for:",
      "options": {
        "A": "Robust color analysis under varying light",
        "B": "Depth estimation",
        "C": "Motion tracking",
        "D": "Texture synthesis"
      },
      "correct_option": "A",
      "difficulty": "easy",
      "learning_objective": "HSV usage.",
      "slide_refs": [
        1
      ]
    },
    {
      "id": "pkg23_mcq4",
      "question": "Radial distortion affects:",
      "options": {
        "A": "Peripheral image points more",
        "B": "Center only",
        "C": "Color channels",
        "D": "Brightness"
      },
      "correct_option": "A",
      "difficulty": "medium",
      "learning_objective": "Distortion effect.",
      "slide_refs": [
        1
      ]
    },
    {
      "id": "pkg23_mcq5",
      "question": "SIFT descriptor uses:",
      "options": {
        "A": "Orientation histograms in subregions",
        "B": "Global means",
        "C": "Color gradients",
        "D": "Binary codes"
      },
      "correct_option": "A",
      "difficulty": "hard",
      "learning_objective": "SIFT details.",
      "slide_refs": [
        1
      ]
    },
    {
      "id": "pkg23_mcq6",
      "question": "Diffuse reflection depends on:",
      "options": {
        "A": "Surface normal and light direction",
        "B": "View direction",
        "C": "Specular angle",
        "D": "Transparency"
      },
      "correct_option": "A",
      "difficulty": "medium",
      "learning_objective": "Lambertian model.",
      "slide_refs": [
        1
      ]
    },
    {
      "id": "pkg23_mcq7",
      "question": "Background subtraction detects:",
      "options": {
        "A": "Moving objects by frame differencing",
        "B": "Static backgrounds only",
        "C": "Color changes",
        "D": "Textures"
      },
      "correct_option": "A",
      "difficulty": "medium",
      "learning_objective": "Bg subtraction.",
      "slide_refs": [
        1
      ]
    },
    {
      "id": "pkg23_mcq8",
      "question": "Structure from motion requires:",
      "options": {
        "A": "Feature correspondences across views",
        "B": "Known depths",
        "C": "Fixed camera",
        "D": "Single image"
      },
      "correct_option": "A",
      "difficulty": "hard",
      "learning_objective": "SfM requirements.",
      "slide_refs": [
        1
      ]
    },
    {
      "id": "pkg23_mcq9",
      "question": "LBP is invariant to:",
      "options": {
        "A": "Monotonic illumination changes",
        "B": "Rotation without extension",
        "C": "Scale",
        "D": "Color"
      },
      "correct_option": "A",
      "difficulty": "medium",
      "learning_objective": "LBP invariance.",
      "slide_refs": [
        1
      ]
    },
    {
      "id": "pkg23_mcq10",
      "question": "In stereo, z = f b / d, for d=2, b=6, f=1, z=:",
      "options": {
        "A": "3",
        "B": "12",
        "C": "0.5",
        "D": "6"
      },
      "correct_option": "A",
      "difficulty": "easy",
      "learning_objective": "Depth calc."
    },
    {
      "id": "pkg22_mcq1",
      "question": "A confusion matrix is used to:",
      "options": {
        "A": "Visualize classification performance showing true and predicted labels",
        "B": "Compute image gradients",
        "C": "Estimate camera pose",
        "D": "Generate textures"
      },
      "correct_option": "A",
      "difficulty": "easy",
      "learning_objective": "Explain confusion matrix for evaluation.",
      "slide_refs": [
        1
      ]
    },
    {
      "id": "pkg22_mcq2",
      "question": "HSV color space decouples:",
      "options": {
        "A": "Color (hue, saturation) from brightness (value)",
        "B": "Red, green, blue channels",
        "C": "Opponent colors",
        "D": "Spectral wavelengths"
      },
      "correct_option": "A",
      "difficulty": "easy",
      "learning_objective": "Describe HSV components.",
      "slide_refs": [
        1
      ]
    },
    {
      "id": "pkg22_mcq3",
      "question": "The pinhole model idealizes:",
      "options": {
        "A": "Perspective projection without lens effects",
        "B": "Orthographic projection",
        "C": "Fisheye distortion",
        "D": "Chromatic effects"
      },
      "correct_option": "A",
      "difficulty": "medium",
      "learning_objective": "Define pinhole camera model.",
      "slide_refs": [
        1
      ]
    },
    {
      "id": "pkg22_mcq4",
      "question": "A feature descriptor provides:",
      "options": {
        "A": "Robust representation of local image patch",
        "B": "Global histogram",
        "C": "3D coordinates",
        "D": "Motion vectors"
      },
      "correct_option": "A",
      "difficulty": "easy",
      "learning_objective": "Role of descriptors.",
      "slide_refs": [
        1
      ]
    },
    {
      "id": "pkg22_mcq5",
      "question": "Chromatic aberration results from:",
      "options": {
        "A": "Different refraction for wavelengths",
        "B": "Sensor Bayer pattern",
        "C": "Motion blur",
        "D": "Radial warping"
      },
      "correct_option": "A",
      "difficulty": "medium",
      "learning_objective": "Cause of chromatic aberration.",
      "slide_refs": [
        1
      ]
    },
    {
      "id": "pkg22_mcq6",
      "question": "Structure-from-motion combines:",
      "options": {
        "A": "Feature tracking and bundle adjustment for 3D and poses",
        "B": "Stereo pairs only",
        "C": "Shading cues",
        "D": "Texture gradients"
      },
      "correct_option": "A",
      "difficulty": "hard",
      "learning_objective": "SfM overview.",
      "slide_refs": [
        1
      ]
    },
    {
      "id": "pkg22_mcq7",
      "question": "Optical flow assumes brightness constancy to estimate:",
      "options": {
        "A": "2D motion field",
        "B": "3D structure",
        "C": "Camera intrinsics",
        "D": "Light sources"
      },
      "correct_option": "A",
      "difficulty": "medium",
      "learning_objective": "Flow principle.",
      "slide_refs": [
        1
      ]
    },
    {
      "id": "pkg22_mcq8",
      "question": "Otsu's method finds threshold by:",
      "options": {
        "A": "Maximizing inter-class variance",
        "B": "Minimizing entropy",
        "C": "Edge detection",
        "D": "Clustering colors"
      },
      "correct_option": "A",
      "difficulty": "medium",
      "learning_objective": "Otsu algorithm.",
      "slide_refs": [
        1
      ]
    },
    {
      "id": "pkg22_mcq9",
      "question": "K-means iteratively:",
      "options": {
        "A": "Assigns points to centroids and updates",
        "B": "Builds decision trees",
        "C": "Fits lines",
        "D": "Computes gradients"
      },
      "correct_option": "A",
      "difficulty": "medium",
      "learning_objective": "k-means steps.",
      "slide_refs": [
        1
      ]
    },
    {
      "id": "pkg22_mcq10",
      "question": "Roberts masks detect edges in:",
      "options": {
        "A": "Diagonal directions",
        "B": "Horizontal only",
        "C": "Vertical only",
        "D": "Circular patterns"
      },
      "correct_option": "A",
      "difficulty": "medium",
      "learning_objective": "Roberts for gradients.",
      "slide_refs": [
        1
      ]
    },
    {
      "id": "pkg25_mcq1",
      "question": "Radial distortion is corrected by:",
      "options": {
        "A": "Polynomial mapping",
        "B": "Linear transformation",
        "C": "Color adjustment",
        "D": "Motion compensation"
      },
      "correct_option": "A",
      "difficulty": "medium",
      "learning_objective": "Distortion correction.",
      "slide_refs": [
        1
      ]
    },
    {
      "id": "pkg25_mcq2",
      "question": "k-nearest neighbor uses:",
      "options": {
        "A": "Distance to k examples for vote",
        "B": "Centroids",
        "C": "Trees",
        "D": "Hyperplanes"
      },
      "correct_option": "A",
      "difficulty": "medium",
      "learning_objective": "kNN.",
      "slide_refs": [
        1
      ]
    },
    {
      "id": "pkg25_mcq3",
      "question": "HSV space is:",
      "options": {
        "A": "Intuitive for color selection",
        "B": "Perceptual uniform",
        "C": "Additive",
        "D": "Subtractive"
      },
      "correct_option": "A",
      "difficulty": "easy",
      "learning_objective": "HSV.",
      "slide_refs": [
        1
      ]
    },
    {
      "id": "pkg25_mcq4",
      "question": "Specular reflection is:",
      "options": {
        "A": "Mirror-like, view-dependent",
        "B": "Diffuse scattering",
        "C": "Absorption",
        "D": "Emission"
      },
      "correct_option": "A",
      "difficulty": "medium",
      "learning_objective": "Reflection types.",
      "slide_refs": [
        1
      ]
    },
    {
      "id": "pkg25_mcq5",
      "question": "Structured light projects:",
      "options": {
        "A": "Patterns for correspondence",
        "B": "Uniform light",
        "C": "Colors only",
        "D": "Shadows"
      },
      "correct_option": "A",
      "difficulty": "hard",
      "learning_objective": "Structured light 3D.",
      "slide_refs": [
        1
      ]
    },
    {
      "id": "pkg25_mcq6",
      "question": "Camera extrinsics are:",
      "options": {
        "A": "Rotation and translation",
        "B": "Focal length, principal point",
        "C": "Distortion coeffs",
        "D": "Sensor size"
      },
      "correct_option": "A",
      "difficulty": "medium",
      "learning_objective": "Extrinsics vs intrinsics.",
      "slide_refs": [
        1
      ]
    },
    {
      "id": "pkg25_mcq7",
      "question": "Optical flow usage example:",
      "options": {
        "A": "Video compression, tracking",
        "B": "Static classification",
        "C": "Lens design",
        "D": "Color printing"
      },
      "correct_option": "A",
      "difficulty": "medium",
      "learning_objective": "Flow applications.",
      "slide_refs": [
        1
      ]
    },
    {
      "id": "pkg25_mcq8",
      "question": "SIFT descriptor is 128D vector of:",
      "options": {
        "A": "Gradient orientation histograms",
        "B": "Pixel intensities",
        "C": "Color values",
        "D": "Edge maps"
      },
      "correct_option": "A",
      "difficulty": "hard",
      "learning_objective": "SIFT details.",
      "slide_refs": [
        1
      ]
    },
    {
      "id": "pkg25_mcq9",
      "question": "Quadtree is a:",
      "options": {
        "A": "Hierarchical image decomposition",
        "B": "Decision tree variant",
        "C": "Graph algorithm",
        "D": "Distance measure"
      },
      "correct_option": "A",
      "difficulty": "hard",
      "learning_objective": "Quadtree for segmentation.",
      "slide_refs": [
        1
      ]
    },
    {
      "id": "pkg25_mcq10",
      "question": "Hough transform example:",
      "options": {
        "A": "Line detection in edges",
        "B": "Region filling",
        "C": "Color quantization",
        "D": "Motion blur removal"
      },
      "correct_option": "A",
      "difficulty": "medium",
      "learning_objective": "Hough usage.",
      "slide_refs": [
        1
      ]
    },
    {
      "id": "pkg13_mcq1",
      "question": "Metamers are:",
      "options": {
        "A": "Different spectral distributions that appear the same color to an observer",
        "B": "Camera sensor artifacts",
        "C": "3D reconstruction errors",
        "D": "Texture patterns"
      },
      "correct_option": "A",
      "difficulty": "easy",
      "learning_objective": "Define metamerism in color perception.",
      "slide_refs": [
        1
      ]
    },
    {
      "id": "pkg13_mcq2",
      "question": "Unsupervised learning involves:",
      "options": {
        "A": "Labeled training data",
        "B": "Discovering patterns without labels, e.g., clustering",
        "C": "Regression prediction",
        "D": "Reinforcement rewards"
      },
      "correct_option": "B",
      "difficulty": "easy",
      "learning_objective": "Distinguish unsupervised from supervised learning.",
      "slide_refs": [
        1
      ]
    },
    {
      "id": "pkg13_mcq3",
      "question": "A confusion matrix shows:",
      "options": {
        "A": "Classifier performance via true/false positives/negatives",
        "B": "Image convolution results",
        "C": "Color space transformations",
        "D": "Feature descriptors"
      },
      "correct_option": "A",
      "difficulty": "medium",
      "learning_objective": "Explain confusion matrix for evaluation.",
      "slide_refs": [
        1
      ]
    },
    {
      "id": "pkg13_mcq4",
      "question": "A feature descriptor encodes:",
      "options": {
        "A": "Global image statistics",
        "B": "Local neighborhood properties around a keypoint",
        "C": "Camera parameters",
        "D": "3D coordinates"
      },
      "correct_option": "B",
      "difficulty": "easy",
      "learning_objective": "Define feature descriptors.",
      "slide_refs": [
        1
      ]
    },
    {
      "id": "pkg13_mcq5",
      "question": "An epipolar line constrains:",
      "options": {
        "A": "The location of corresponding points in stereo images",
        "B": "Motion vectors",
        "C": "Texture gradients",
        "D": "Color histograms"
      },
      "correct_option": "A",
      "difficulty": "medium",
      "learning_objective": "Recall epipolar geometry.",
      "slide_refs": [
        1
      ]
    },
    {
      "id": "pkg13_mcq6",
      "question": "Multi-view stereo reconstructs:",
      "options": {
        "A": "2D panoramas",
        "B": "3D models from multiple images via triangulation",
        "C": "Optical flow fields",
        "D": "Texture maps"
      },
      "correct_option": "B",
      "difficulty": "medium",
      "learning_objective": "Describe multi-view stereo.",
      "slide_refs": [
        1
      ]
    },
    {
      "id": "pkg13_mcq7",
      "question": "Otsu's method optimizes:",
      "options": {
        "A": "Threshold for binary segmentation by maximizing between-class variance",
        "B": "Edge detection filters",
        "C": "Feature matching",
        "D": "Color balance"
      },
      "correct_option": "A",
      "difficulty": "medium",
      "learning_objective": "Explain Otsu's thresholding.",
      "slide_refs": [
        1
      ]
    },
    {
      "id": "pkg13_mcq8",
      "question": "Convolutional neural networks use:",
      "options": {
        "A": "Shared weights and pooling for hierarchical feature learning",
        "B": "Only fully connected layers",
        "C": "No activation functions",
        "D": "Unsupervised pretraining always"
      },
      "correct_option": "A",
      "difficulty": "hard",
      "learning_objective": "Outline CNN principles.",
      "slide_refs": [
        1
      ]
    },
    {
      "id": "pkg13_mcq9",
      "question": "Local binary patterns (LBP) compute:",
      "options": {
        "A": "Binary codes from neighborhood intensity comparisons",
        "B": "Global histograms",
        "C": "3D normals",
        "D": "Motion vectors"
      },
      "correct_option": "A",
      "difficulty": "medium",
      "learning_objective": "Describe LBP operator.",
      "slide_refs": [
        1
      ]
    },
    {
      "id": "pkg13_mcq10",
      "question": "The temporal derivative ∂f/∂t in optical flow is estimated by:",
      "options": {
        "A": "Difference between consecutive frames at the same position",
        "B": "Spatial gradients only",
        "C": "Color changes",
        "D": "Camera motion"
      },
      "correct_option": "A",
      "difficulty": "hard",
      "learning_objective": "Compute derivatives for flow constraint.",
      "slide_refs": [
        1
      ]
    },
    {
      "id": "pkg14_mcq1",
      "question": "Chromatic aberration causes:",
      "options": {
        "A": "Geometric distortion",
        "B": "Color-dependent focusing errors leading to fringing",
        "C": "Sensor noise",
        "D": "Vignetting"
      },
      "correct_option": "B",
      "difficulty": "easy",
      "learning_objective": "Define chromatic aberration.",
      "slide_refs": [
        1
      ]
    },
    {
      "id": "pkg14_mcq2",
      "question": "A Bayer filter is used for:",
      "options": {
        "A": "Demosaicing color images from single-sensor arrays",
        "B": "Edge detection",
        "C": "Motion blur reduction",
        "D": "3D reconstruction"
      },
      "correct_option": "A",
      "difficulty": "medium",
      "learning_objective": "Explain Bayer pattern in color imaging.",
      "slide_refs": [
        1
      ]
    },
    {
      "id": "pkg14_mcq3",
      "question": "The aperture problem refers to:",
      "options": {
        "A": "Ambiguity in motion direction along edges",
        "B": "Lens aperture size affecting depth of field",
        "C": "Camera calibration issues",
        "D": "Color space conversion"
      },
      "correct_option": "A",
      "difficulty": "medium",
      "learning_objective": "Describe aperture problem in flow.",
      "slide_refs": [
        1
      ]
    },
    {
      "id": "pkg14_mcq4",
      "question": "Hough transform detects:",
      "options": {
        "A": "Arbitrary shapes via voting in parameter space",
        "B": "Only circles",
        "C": "Textures",
        "D": "Motion"
      },
      "correct_option": "A",
      "difficulty": "medium",
      "learning_objective": "Principle of Hough for line detection.",
      "slide_refs": [
        1
      ]
    },
    {
      "id": "pkg14_mcq5",
      "question": "Mahalanobis distance accounts for:",
      "options": {
        "A": "Covariance in multivariate data",
        "B": "Only mean differences",
        "C": "Binary features",
        "D": "Image gradients"
      },
      "correct_option": "A",
      "difficulty": "hard",
      "learning_objective": "Compare to Euclidean distance.",
      "slide_refs": [
        1
      ]
    },
    {
      "id": "pkg14_mcq6",
      "question": "Structure-from-motion recovers:",
      "options": {
        "A": "3D structure and camera poses from image sequence",
        "B": "Only 2D homographies",
        "C": "Color constancy",
        "D": "Texture synthesis"
      },
      "correct_option": "A",
      "difficulty": "medium",
      "learning_objective": "Define SfM pipeline.",
      "slide_refs": [
        1
      ]
    },
    {
      "id": "pkg14_mcq7",
      "question": "K-means clustering partitions data into:",
      "options": {
        "A": "K groups by minimizing within-cluster variance",
        "B": "Hierarchical trees",
        "C": "Overlapping sets",
        "D": "Labeled classes"
      },
      "correct_option": "A",
      "difficulty": "medium",
      "learning_objective": "Describe k-means algorithm.",
      "slide_refs": [
        1
      ]
    },
    {
      "id": "pkg14_mcq8",
      "question": "Photometric stereo uses:",
      "options": {
        "A": "Multiple lights to recover surface normals from shading",
        "B": "Stereo pairs for depth",
        "C": "Motion for structure",
        "D": "Texture for classification"
      },
      "correct_option": "A",
      "difficulty": "hard",
      "learning_objective": "Explain photometric stereo.",
      "slide_refs": [
        1
      ]
    },
    {
      "id": "pkg14_mcq9",
      "question": "In stereo, disparity d relates to depth z by:",
      "options": {
        "A": "z = f * b / d",
        "B": "z = d / f",
        "C": "z = b / d",
        "D": "z = f / d"
      },
      "correct_option": "A",
      "difficulty": "medium",
      "learning_objective": "Derive depth from disparity.",
      "slide_refs": [
        1
      ]
    },
    {
      "id": "pkg14_mcq10",
      "question": "Stereo correspondence is similar to optical flow matching but differs in:",
      "options": {
        "A": "Fixed baseline vs. temporal change",
        "B": "Both use block matching",
        "C": "Stereo has epipolar constraint",
        "D": "All of the above"
      },
      "correct_option": "D",
      "difficulty": "hard",
      "learning_objective": "Compare stereo and flow.",
      "slide_refs": [
        1
      ]
    },
    {
      "id": "pkg4_mcq1",
      "question": "Which of the following is NOT a reason why analyzing visual motion is important in computer vision?",
      "options": {
        "A": "To stabilize shaky video caused by camera jitter",
        "B": "To track and analyze object trajectories",
        "C": "To directly compute absolute depth from a single image",
        "D": "To reveal spatial layout via motion parallax"
      },
      "correct_option": "C",
      "difficulty": "easy",
      "learning_objective": "Identify valid applications of motion analysis in vision systems.",
      "slide_refs": [
        8
      ]
    },
    {
      "id": "pkg4_mcq2",
      "question": "Motion parallax refers to the phenomenon where:",
      "options": {
        "A": "Closer objects appear to move faster than distant ones when the camera translates",
        "B": "Objects rotate around their center when the camera moves",
        "C": "Parallel lines converge at the vanishing point due to camera motion",
        "D": "Image brightness changes uniformly across the frame"
      },
      "correct_option": "A",
      "difficulty": "easy",
      "learning_objective": "Recall the definition and visual effect of motion parallax.",
      "slide_refs": [
        8
      ]
    },
    {
      "id": "pkg4_mcq3",
      "question": "In the context of perceptual organization, motion can serve as a powerful cue for grouping when:",
      "options": {
        "A": "All points in the image move at the same velocity",
        "B": "Regions move coherently with a common motion pattern",
        "C": "Motion is completely random across the image",
        "D": "Only static edges are present"
      },
      "correct_option": "B",
      "difficulty": "medium",
      "learning_objective": "Understand motion as a segmentation and grouping cue.",
      "slide_refs": [
        9
      ]
    },
    {
      "id": "pkg4_mcq4",
      "question": "The aperture problem states that:",
      "options": {
        "A": "Local motion measurements through a small aperture are ambiguous",
        "B": "Global optical flow is always unique",
        "C": "Only translational motion can be recovered",
        "D": "Motion perpendicular to an edge is always detectable"
      },
      "correct_option": "A",
      "difficulty": "medium",
      "learning_objective": "Explain the fundamental ambiguity in local motion estimation.",
      "slide_refs": [
        12
      ]
    },
    {
      "id": "pkg4_mcq5",
      "question": "For a rigidly moving object under orthographic projection, the instantaneous 2D motion field is best approximated by:",
      "options": {
        "A": "A pure translation",
        "B": "An affine transformation with 6 parameters",
        "C": "A full projective transformation",
        "D": "A single scale factor"
      },
      "correct_option": "B",
      "difficulty": "hard",
      "learning_objective": "Derive the parametric form of 2D motion under rigid 3D motion and orthographic camera.",
      "slide_refs": [
        15
      ]
    },
    {
      "id": "pkg4_mcq6",
      "question": "The brightness constancy assumption in optical flow estimation states that:",
      "options": {
        "A": "Image intensity at a point remains constant over time if the point moves",
        "B": "All pixels have the same brightness",
        "C": "Motion causes uniform change in intensity",
        "D": "Lighting direction is known and fixed"
      },
      "correct_option": "A",
      "difficulty": "easy",
      "learning_objective": "State the core assumption behind differential optical flow methods.",
      "slide_refs": [
        18
      ]
    },
    {
      "id": "pkg4_mcq7",
      "question": "The Lucas-Kanade method assumes that motion is approximately constant:",
      "options": {
        "A": "Over the entire image",
        "B": "In a small spatial neighborhood around each pixel",
        "C": "Only at feature points",
        "D": "Along epipolar lines"
      },
      "correct_option": "B",
      "difficulty": "medium",
      "learning_objective": "Recall the local smoothness assumption in Lucas-Kanade optical flow.",
      "slide_refs": [
        20
      ]
    },
    {
      "id": "pkg4_mcq8",
      "question": "In the Horn-Schunck optical flow formulation, the smoothness term regularizes the flow field by minimizing:",
      "options": {
        "A": "The magnitude of the flow vectors",
        "B": "The spatial gradients of the flow components \\(\\frac{\\partial u}{\\partial x}, \\frac{\\partial u}{\\partial y}, \\frac{\\partial v}{\\partial x}, \\frac{\\partial v}{\\partial y}\\)",
        "C": "The difference between consecutive frames",
        "D": "The total number of non-zero flow vectors"
      },
      "correct_option": "B",
      "difficulty": "hard",
      "learning_objective": "Understand the global regularization in dense optical flow estimation.",
      "slide_refs": [
        22
      ]
    },
    {
      "id": "pkg4_mcq9",
      "question": "Which of the following is a common failure case of the brightness constancy assumption?",
      "options": {
        "A": "Slow camera motion",
        "B": "Specular reflections or moving shadows",
        "C": "Pure translational motion",
        "D": "High-contrast textures"
      },
      "correct_option": "B",
      "difficulty": "medium",
      "learning_objective": "Identify real-world violations of optical flow assumptions.",
      "slide_refs": [
        19
      ]
    },
    {
      "id": "pkg4_mcq10",
      "question": "To estimate an affine 2D motion model using least squares, the minimum number of point correspondences required is:",
      "options": {
        "A": "2",
        "B": "3",
        "C": "4",
        "D": "6"
      },
      "correct_option": "B",
      "difficulty": "medium",
      "learning_objective": "Determine minimal sample size for parametric motion estimation.",
      "slide_refs": [
        16
      ]
    },
    {
      "id": "pkg6_mcq1",
      "question": "What is the primary purpose of detecting local features in images?",
      "options": {
        "A": "To compress the image for storage",
        "B": "To enable robust image matching by finding corresponding points across views",
        "C": "To directly reconstruct 3D geometry without correspondences",
        "D": "To perform global color correction"
      },
      "correct_option": "B",
      "difficulty": "easy",
      "learning_objective": "Understand the motivation for local feature detection in image matching.",
      "slide_refs": [
        8
      ]
    },
    {
      "id": "pkg6_mcq2",
      "question": "The Harris corner detector identifies points where:",
      "options": {
        "A": "Intensity is locally maximum",
        "B": "The image gradient changes significantly in two orthogonal directions",
        "C": "Edges are perfectly straight",
        "D": "Texture is uniform"
      },
      "correct_option": "B",
      "difficulty": "medium",
      "learning_objective": "Recall the core principle behind Harris corner detection.",
      "slide_refs": [
        7
      ]
    },
    {
      "id": "pkg6_mcq3",
      "question": "In the Harris corner response function, the matrix \\(M\\) is constructed from:",
      "options": {
        "A": "Raw pixel intensities in a local window",
        "B": "Spatial gradients \\(I_x\\) and \\(I_y\\) averaged over a neighborhood",
        "C": "Second derivatives of the image",
        "D": "Laplacian of Gaussian responses"
      },
      "correct_option": "B",
      "difficulty": "medium",
      "learning_objective": "Describe the structure tensor used in Harris corner detection.",
      "slide_refs": [
        10
      ]
    },
    {
      "id": "pkg6_mcq4",
      "question": "The corner response \\(R = \\det(M) - k \\cdot \\trace(M)^2\\) is high when:",
      "options": {
        "A": "Both eigenvalues of \\(M\\) are small",
        "B": "One eigenvalue is large and the other is small",
        "C": "Both eigenvalues are large",
        "D": "The trace is zero"
      },
      "correct_option": "C",
      "difficulty": "hard",
      "learning_objective": "Interpret the eigenvalue analysis of the structure tensor for corner classification.",
      "slide_refs": [
        11
      ]
    },
    {
      "id": "pkg6_mcq5",
      "question": "Which of the following is a key advantage of Harris corners over simple edge detectors?",
      "options": {
        "A": "They are invariant to full projective transformations",
        "B": "They provide precise localization at junctions where two edges meet",
        "C": "They require no gradient computation",
        "D": "They are scale-invariant by default"
      },
      "correct_option": "B",
      "difficulty": "medium",
      "learning_objective": "Evaluate the localization precision of corner features.",
      "slide_refs": [
        9
      ]
    },
    {
      "id": "pkg6_mcq6",
      "question": "In practice, non-maximum suppression is applied to Harris corner responses to:",
      "options": {
        "A": "Increase the number of detected corners",
        "B": "Select the strongest local maxima and avoid clustered responses",
        "C": "Smooth the image before detection",
        "D": "Threshold based on global intensity"
      },
      "correct_option": "B",
      "difficulty": "easy",
      "learning_objective": "Explain post-processing steps in feature detection.",
      "slide_refs": [
        12
      ]
    },
    {
      "id": "pkg6_mcq7",
      "question": "The parameter \\(k\\) in the Harris response function typically ranges between:",
      "options": {
        "A": "0.001 to 0.01",
        "B": "0.04 to 0.06",
        "C": "0.5 to 1.0",
        "D": "10 to 100"
      },
      "correct_option": "B",
      "difficulty": "hard",
      "learning_objective": "Recall empirical values for the Harris detector sensitivity parameter.",
      "slide_refs": [
        11
      ]
    },
    {
      "id": "pkg6_mcq8",
      "question": "Which visual example in the lecture demonstrates successful matching using local features?",
      "options": {
        "A": "Two identical images of a flat wall",
        "B": "Two mountain panorama images with corresponding red lines connecting feature points",
        "C": "A single image with motion blur",
        "D": "A 3D reconstructed point cloud"
      },
      "correct_option": "B",
      "difficulty": "easy",
      "learning_objective": "Identify real-world application of feature matching.",
      "slide_refs": [
        8
      ]
    },
    {
      "id": "pkg6_mcq9",
      "question": "The Harris detector is based on the autocorrelation function of the image. A corner corresponds to a point where this function:",
      "options": {
        "A": "Is flat in all directions",
        "B": "Has a peak in one direction only",
        "C": "Drops rapidly in multiple directions",
        "D": "Is zero"
      },
      "correct_option": "C",
      "difficulty": "hard",
      "learning_objective": "Connect Harris method to the concept of image autocorrelation.",
      "slide_refs": [
        9
      ]
    },
    {
      "id": "pkg6_mcq10",
      "question": "Why are local features preferred over global image descriptors for matching under viewpoint changes?",
      "options": {
        "A": "They are computationally cheaper to extract globally",
        "B": "They allow partial matching and are robust to occlusion and clutter",
        "C": "They encode color information more accurately",
        "D": "They require calibrated cameras"
      },
      "correct_option": "B",
      "difficulty": "medium",
      "learning_objective": "Justify the use of sparse local features in robust correspondence.",
      "slide_refs": [
        8
      ]
    },
    {
      "id": "pkg11_mcq1",
      "question": "The lecture series is structured around which central theme?",
      "options": {
        "A": "Machine learning for image classification",
        "B": "From photons to semantics – building a complete computer vision pipeline",
        "C": "Real-time video processing on embedded devices",
        "D": "Deep neural network architectures for vision"
      },
      "correct_option": "B",
      "difficulty": "easy",
      "learning_objective": "Identify the overarching goal of the course as a full vision pipeline.",
      "slide_refs": [
        3
      ]
    },
    {
      "id": "pkg11_mcq2",
      "question": "Which of the following is NOT listed as a core module in the course outline?",
      "options": {
        "A": "Imaging and the pinhole model",
        "B": "Light and color – specular reflection",
        "C": "Graph cuts and energy minimization",
        "D": "Recognition – basics"
      },
      "correct_option": "C",
      "difficulty": "medium",
      "learning_objective": "Recall the sequence and titles of the 11 main lecture modules.",
      "slide_refs": [
        5
      ]
    },
    {
      "id": "pkg11_mcq3",
      "question": "The course emphasizes that modern computer vision relies heavily on:",
      "options": {
        "A": "Hand-crafted geometric models only",
        "B": "Learning from large labeled datasets",
        "C": "Real-time GPU acceleration",
        "D": "Quantum imaging sensors"
      },
      "correct_option": "B",
      "difficulty": "easy",
      "learning_objective": "Understand the paradigm shift toward data-driven methods.",
      "slide_refs": [
        7
      ]
    },
    {
      "id": "pkg11_mcq4",
      "question": "A key challenge highlighted in the introduction is:",
      "options": {
        "A": "The inverse problem of recovering 3D scene properties from 2D projections",
        "B": "Storing high-resolution images efficiently",
        "C": "Training neural networks in under one second",
        "D": "Designing lenses with zero distortion"
      },
      "correct_option": "A",
      "difficulty": "medium",
      "learning_objective": "Recognize the ill-posed nature of vision as inverse optics.",
      "slide_refs": [
        9
      ]
    },
    {
      "id": "pkg11_mcq5",
      "question": "The lecture on 'Depth from images – 3D cues' (10.1) primarily covers:",
      "options": {
        "A": "Active structured light and time-of-flight",
        "B": "Monocular cues such as texture gradient, shading, and focus",
        "C": "Multi-view stereo reconstruction",
        "D": "Photometric stereo with known light directions"
      },
      "correct_option": "B",
      "difficulty": "medium",
      "learning_objective": "Differentiate passive monocular depth cues from active or stereo methods.",
      "slide_refs": [
        5
      ]
    },
    {
      "id": "pkg11_mcq6",
      "question": "Which module directly follows 'Local features – Harris corners' (6.1)?",
      "options": {
        "A": "Recognition – Basics",
        "B": "2D models – Fitting",
        "C": "Motion – Basics",
        "D": "Texture analysis – Basics"
      },
      "correct_option": "A",
      "difficulty": "easy",
      "learning_objective": "Memorize the chronological order of lecture topics.",
      "slide_refs": [
        5
      ]
    },
    {
      "id": "pkg11_mcq7",
      "question": "The introductory slides claim that vision is 'easy for humans, hard for machines' because:",
      "options": {
        "A": "Humans have higher resolution retinas",
        "B": "The mapping from 3D world to 2D image loses information irreversibly",
        "C": "Machines cannot process color information",
        "D": "Human eyes have larger field of view"
      },
      "correct_option": "B",
      "difficulty": "medium",
      "learning_objective": "Explain the information loss in image formation.",
      "slide_refs": [
        8
      ]
    },
    {
      "id": "pkg11_mcq8",
      "question": "In the course roadmap, '3D – Affine transformation' (11.1) is positioned as:",
      "options": {
        "A": "The final topic before advanced deep learning",
        "B": "A bridge between 2D image processing and full 3D reconstruction",
        "C": "An optional module on linear algebra",
        "D": "A review of camera calibration"
      },
      "correct_option": "B",
      "difficulty": "hard",
      "learning_objective": "Interpret the pedagogical progression from 2D to 3D reasoning.",
      "slide_refs": [
        5
      ]
    },
    {
      "id": "pkg11_mcq9",
      "question": "Which real-world application is explicitly mentioned as a motivation for studying motion analysis?",
      "options": {
        "A": "Autonomous driving and action recognition",
        "B": "Medical image registration",
        "C": "Satellite image mosaicking",
        "D": "All of the above"
      },
      "correct_option": "D",
      "difficulty": "easy",
      "learning_objective": "Connect theoretical modules to practical vision systems.",
      "slide_refs": [
        6
      ]
    },
    {
      "id": "pkg11_mcq10",
      "question": "The slide titled 'What is computer vision?' defines it as:",
      "options": {
        "A": "Making useful decisions about real physical objects and scenes based on sensed images",
        "B": "Digitizing analog photographs",
        "C": "Compressing image files",
        "D": "Designing better camera hardware"
      },
      "correct_option": "A",
      "difficulty": "easy",
      "learning_objective": "State the standard definition of the field.",
      "slide_refs": [
        4
      ]
    },
    {
      "id": "pkg5_mcq1",
      "question": "According to the lecture, pattern recognition is fundamentally the assignment of a label to a given input value. Which of the following is NOT explicitly listed as an example of pattern recognition?",
      "options": {
        "A": "Classification",
        "B": "Regression",
        "C": "Clustering",
        "D": "Handprinted character recognition"
      },
      "correct_option": "C",
      "difficulty": "easy",
      "learning_objective": "Recall the definition and common examples of pattern recognition.",
      "slide_refs": [
        12
      ]
    },
    {
      "id": "pkg5_mcq2",
      "question": "In the context of pattern recognition, the lecture emphasizes that the process typically involves:",
      "options": {
        "A": "Manual rule-based decision making",
        "B": "Learning from data in most practical cases",
        "C": "Hard-coded geometric models only",
        "D": "Direct pixel intensity comparison"
      },
      "correct_option": "B",
      "difficulty": "easy",
      "learning_objective": "Understand the role of learning in modern pattern recognition systems.",
      "slide_refs": [
        12
      ]
    },
    {
      "id": "pkg5_mcq3",
      "question": "The lecture shows binary images of handwritten digits. What is the primary challenge illustrated by these examples?",
      "options": {
        "A": "High-resolution color variations",
        "B": "Large intra-class variability in shape and style",
        "C": "Complex background clutter",
        "D": "3D pose estimation"
      },
      "correct_option": "B",
      "difficulty": "medium",
      "learning_objective": "Identify the core difficulty in recognizing deformable or variable patterns like handwriting.",
      "slide_refs": [
        12
      ]
    },
    {
      "id": "pkg5_mcq4",
      "question": "Which of the following best describes the goal of a pattern recognition system for handwritten character recognition?",
      "options": {
        "A": "To reconstruct the 3D pen trajectory",
        "B": "To assign a discrete class label (e.g., '0'–'9') to the input image",
        "C": "To estimate continuous stroke thickness",
        "D": "To detect ink color"
      },
      "correct_option": "B",
      "difficulty": "easy",
      "learning_objective": "Define the output of a classification-based recognition task.",
      "slide_refs": [
        12
      ]
    },
    {
      "id": "pkg5_mcq5",
      "question": "The lecture contrasts pattern recognition with other vision tasks. Which statement is correct?",
      "options": {
        "A": "Recognition is only about detecting objects, not classifying them.",
        "B": "Recognition often requires learning statistical models from labeled training data.",
        "C": "Recognition can always be solved using geometric fitting alone.",
        "D": "Recognition does not benefit from machine learning techniques."
      },
      "correct_option": "B",
      "difficulty": "medium",
      "learning_objective": "Differentiate recognition from low-level vision tasks and emphasize data-driven approaches.",
      "slide_refs": [
        12
      ]
    },
    {
      "id": "pkg5_mcq6",
      "question": "In the broader context of the course (as referenced in prior lectures), object recognition may build upon earlier modules. Which of the following is a prerequisite for robust recognition in real-world images?",
      "options": {
        "A": "Accurate 2D model fitting under occlusion and clutter",
        "B": "Direct pixel-to-label mapping without preprocessing",
        "C": "Assuming all objects are rigid and fully visible",
        "D": "Using only global image statistics"
      },
      "correct_option": "A",
      "difficulty": "hard",
      "learning_objective": "Connect recognition challenges to model fitting and robustness (cross-reference with Lecture 9).",
      "slide_refs": [
        12,
        14
      ]
    },
    {
      "id": "pkg5_mcq7",
      "question": "The lecture introduces recognition as a decision-making process. What is the input to this decision in the handwritten digit example?",
      "options": {
        "A": "A 3D point cloud",
        "B": "A binary or grayscale image patch containing a single digit",
        "C": "A sequence of video frames",
        "D": "A set of edge maps"
      },
      "correct_option": "B",
      "difficulty": "easy",
      "learning_objective": "Identify the typical input representation in a recognition pipeline.",
      "slide_refs": [
        12
      ]
    },
    {
      "id": "pkg5_mcq8",
      "question": "Which of the following is a key difference between classification and regression in the context of pattern recognition?",
      "options": {
        "A": "Classification outputs discrete labels; regression outputs continuous values.",
        "B": "Classification uses learning; regression does not.",
        "C": "Regression is only for 2D data; classification for 3D.",
        "D": "There is no difference; both are identical."
      },
      "correct_option": "A",
      "difficulty": "medium",
      "learning_objective": "Distinguish between discrete and continuous prediction tasks in recognition.",
      "slide_refs": [
        12
      ]
    },
    {
      "id": "pkg3_mcq1",
      "question": "In the context of 2D model fitting, what is the primary goal of choosing a parametric model for a set of image features?",
      "options": {
        "A": "To increase computational complexity",
        "B": "To represent object geometry with a compact mathematical description",
        "C": "To eliminate all outliers automatically",
        "D": "To perform edge detection"
      },
      "correct_option": "B",
      "difficulty": "easy",
      "learning_objective": "Understand the purpose of parametric modeling in object representation.",
      "slide_refs": [
        12
      ]
    },
    {
      "id": "pkg3_mcq2",
      "question": "Which of the following is NOT listed as a common geometric primitive for 2D object modeling?",
      "options": {
        "A": "Point",
        "B": "Line",
        "C": "Triangle",
        "D": "Ellipse"
      },
      "correct_option": "C",
      "difficulty": "easy",
      "learning_objective": "Recall basic 2D geometric primitives used in model fitting.",
      "slide_refs": [
        13
      ]
    },
    {
      "id": "pkg3_mcq3",
      "question": "In the case study on line detection shown in the lecture, which of the following is identified as a source of 'missing data'?",
      "options": {
        "A": "Noise in feature locations",
        "B": "Multiple overlapping lines",
        "C": "Occlusions",
        "D": "Clutter from background textures"
      },
      "correct_option": "C",
      "difficulty": "medium",
      "learning_objective": "Identify real-world challenges in model fitting due to missing data.",
      "slide_refs": [
        15
      ]
    },
    {
      "id": "pkg3_mcq4",
      "question": "What type of data issue is illustrated by the presence of multiple coins in the 'simple model: circles' example?",
      "options": {
        "A": "Noise",
        "B": "Extraneous data (clutter)",
        "C": "Missing data",
        "D": "Model mismatch"
      },
      "correct_option": "B",
      "difficulty": "medium",
      "learning_objective": "Distinguish between noise, clutter, and missing data in fitting scenarios.",
      "slide_refs": [
        14
      ]
    },
    {
      "id": "pkg3_mcq5",
      "question": "When fitting a complex model like 'car' using multiple ellipses, the system must handle:",
      "options": {
        "A": "Only position and scale parameters",
        "B": "Pose, articulation, and part-based alignment",
        "C": "Only global rotation",
        "D": "Only color consistency"
      },
      "correct_option": "B",
      "difficulty": "hard",
      "learning_objective": "Recognize the increased complexity in fitting articulated or multi-part models.",
      "slide_refs": [
        14
      ]
    },
    {
      "id": "pkg3_mcq6",
      "question": "Which of the following best describes the role of model parameters in 2D fitting?",
      "options": {
        "A": "They define only the color of the object",
        "B": "They specify size, position, and orientation of the geometric primitive",
        "C": "They are used only for line detection",
        "D": "They replace the need for feature extraction"
      },
      "correct_option": "B",
      "difficulty": "easy",
      "learning_objective": "Understand the semantic meaning of model parameters in geometric fitting.",
      "slide_refs": [
        13
      ]
    },
    {
      "id": "pkg3_mcq7",
      "question": "In the presence of occlusion, a robust fitting algorithm should:",
      "options": {
        "A": "Fail completely if any part is occluded",
        "B": "Use only visible parts to estimate model parameters",
        "C": "Assume all data is missing",
        "D": "Increase sensitivity to noise"
      },
      "correct_option": "B",
      "difficulty": "medium",
      "learning_objective": "Apply robustness principles to partial visibility in model fitting.",
      "slide_refs": [
        15
      ]
    },
    {
      "id": "pkg3_mcq8",
      "question": "Which image in the lecture demonstrates the use of a 'complicated model: car' with deformable parts?",
      "options": {
        "A": "The image with multiple coins",
        "B": "The image with a red microcar and yellow contour overlay",
        "C": "The line detection case study with a house",
        "D": "The structured light example"
      },
      "correct_option": "B",
      "difficulty": "easy",
      "learning_objective": "Identify examples of complex model fitting in real images.",
      "slide_refs": [
        14
      ]
    },
    {
      "id": "pkg1_mcq1",
      "question": "In the homogeneous coordinate representation of 3D affine transformations, the transformation matrix mapping a point \\(P = (x, y, z)\\) to \\(P' = (x', y', z')\\) is a 4×4 matrix with the bottom row fixed as:",
      "options": {
        "A": "[1 0 0 0]",
        "B": "[0 1 0 0]",
        "C": "[0 0 1 0]",
        "D": "[0 0 0 1]"
      },
      "correct_option": "D",
      "difficulty": "easy",
      "learning_objective": "Recall the structure of the 4×4 affine transformation matrix in homogeneous coordinates.",
      "slide_refs": [
        11
      ]
    },
    {
      "id": "pkg1_mcq2",
      "question": "A rigid 3D transformation consists of rotation followed by translation. How many degrees of freedom does it have, and what is the minimum number of point correspondences required to estimate it uniquely?",
      "options": {
        "A": "6 degrees of freedom; at least 3 point correspondences",
        "B": "6 degrees of freedom; at least 4 point correspondences",
        "C": "7 degrees of freedom; at least 3 point correspondences",
        "D": "12 degrees of freedom; at least 6 point correspondences"
      },
      "correct_option": "A",
      "difficulty": "medium",
      "learning_objective": "Identify the degrees of freedom of rigid transformation and the minimum correspondences for pose estimation.",
      "slide_refs": [
        12,
        13,
        34
      ]
    },
    {
      "id": "pkg1_mcq3",
      "question": "Given a calibrated camera with known intrinsic matrix \\(K\\), the projection equation can be simplified using normalized image coordinates. Which matrix represents the simplified projection model \\(M\\) when \\(f = 1\\) and principal point \\((c_0, r_0) = (0, 0)\\)?",
      "options": {
        "A": "\\(M = \\begin{bmatrix} K & 0 \\end{bmatrix} [R \\; t]\\)",
        "B": "\\(M = [R \\; t]\\)",
        "C": "\\(M = K [R \\; t]\\)",
        "D": "\\(M = [I \\; 0] [R \\; t]\\)"
      },
      "correct_option": "B",
      "difficulty": "hard",
      "learning_objective": "Derive the projection matrix under normalized image coordinates for a calibrated camera.",
      "slide_refs": [
        24
      ]
    },
    {
      "id": "pkg16_mcq1",
      "question": "Radial distortion causes:",
      "options": {
        "A": "Straight lines to curve in images",
        "B": "Color shifts",
        "C": "Motion blur",
        "D": "Texture loss"
      },
      "correct_option": "A",
      "difficulty": "easy",
      "learning_objective": "Define radial lens distortion.",
      "slide_refs": [
        1
      ]
    },
    {
      "id": "pkg16_mcq2",
      "question": "RGB color space is:",
      "options": {
        "A": "Additive primaries for displays",
        "B": "Perceptually uniform",
        "C": "Device-independent",
        "D": "Cylindrical coordinates"
      },
      "correct_option": "A",
      "difficulty": "easy",
      "learning_objective": "Describe RGB model.",
      "slide_refs": [
        1
      ]
    },
    {
      "id": "pkg16_mcq3",
      "question": "An image histogram shows:",
      "options": {
        "A": "Intensity frequency distribution",
        "B": "Spatial positions",
        "C": "Gradients",
        "D": "3D depths"
      },
      "correct_option": "A",
      "difficulty": "easy",
      "learning_objective": "Explain histograms.",
      "slide_refs": [
        1
      ]
    },
    {
      "id": "pkg16_mcq4",
      "question": "A feature descriptor is:",
      "options": {
        "A": "Vector summarizing local patch",
        "B": "Global statistic",
        "C": "Camera parameter",
        "D": "Light model"
      },
      "correct_option": "A",
      "difficulty": "medium",
      "learning_objective": "Define descriptors.",
      "slide_refs": [
        1
      ]
    },
    {
      "id": "pkg16_mcq5",
      "question": "The aperture problem arises because:",
      "options": {
        "A": "Local motion is ambiguous along contours",
        "B": "Aperture size affects light",
        "C": "Sensors have limited resolution",
        "D": "Colors vary"
      },
      "correct_option": "A",
      "difficulty": "medium",
      "learning_objective": "Aperture problem in flow.",
      "slide_refs": [
        1
      ]
    },
    {
      "id": "pkg16_mcq6",
      "question": "Structure-from-motion is:",
      "options": {
        "A": "3D from moving camera images",
        "B": "2D alignment",
        "C": "Color recovery",
        "D": "Texture synthesis"
      },
      "correct_option": "A",
      "difficulty": "medium",
      "learning_objective": "Define SfM.",
      "slide_refs": [
        1
      ]
    },
    {
      "id": "pkg16_mcq7",
      "question": "K-means is used for:",
      "options": {
        "A": "Unsupervised clustering",
        "B": "Supervised classification",
        "C": "Regression",
        "D": "Dimensionality reduction"
      },
      "correct_option": "A",
      "difficulty": "easy",
      "learning_objective": "K-means principle.",
      "slide_refs": [
        1
      ]
    },
    {
      "id": "pkg16_mcq8",
      "question": "Hough transform uses:",
      "options": {
        "A": "Accumulator voting for parameters",
        "B": "Gradient descent",
        "C": "Neural networks",
        "D": "Histogram equalization"
      },
      "correct_option": "A",
      "difficulty": "medium",
      "learning_objective": "Hough voting.",
      "slide_refs": [
        1
      ]
    },
    {
      "id": "pkg16_mcq9",
      "question": "In stereo, depth z =:",
      "options": {
        "A": "f b / d",
        "B": "d b / f",
        "C": "f d / b",
        "D": "b / (f d)"
      },
      "correct_option": "A",
      "difficulty": "medium",
      "learning_objective": "Stereo depth formula.",
      "slide_refs": [
        1
      ]
    },
    {
      "id": "pkg16_mcq10",
      "question": "Stereo matching differs from flow by:",
      "options": {
        "A": "Known geometry vs. unknown motion",
        "B": "Both seek correspondences",
        "C": "Stereo uses rectification",
        "D": "All of the above"
      },
      "correct_option": "D",
      "difficulty": "hard",
      "learning_objective": "Compare stereo and flow.",
      "slide_refs": [
        1
      ]
    },
    {
      "id": "pkg20_mcq1",
      "question": "Pinhole model assumes:",
      "options": {
        "A": "Infinite depth of field, perspective projection",
        "B": "Orthographic projection",
        "C": "Lens distortion",
        "D": "Finite aperture"
      },
      "correct_option": "A",
      "difficulty": "easy",
      "learning_objective": "Pinhole basics.",
      "slide_refs": [
        1
      ]
    },
    {
      "id": "pkg20_mcq2",
      "question": "Mahalanobis distance is useful for:",
      "options": {
        "A": "Anisotropic data with covariance",
        "B": "Binary codes",
        "C": "Image filtering",
        "D": "3D reconstruction"
      },
      "correct_option": "A",
      "difficulty": "hard",
      "learning_objective": "Mahalanobis vs Euclidean.",
      "slide_refs": [
        1
      ]
    },
    {
      "id": "pkg20_mcq3",
      "question": "Confusion matrix is for:",
      "options": {
        "A": "Multi-class error analysis",
        "B": "Regression",
        "C": "Clustering",
        "D": "Optimization"
      },
      "correct_option": "A",
      "difficulty": "medium",
      "learning_objective": "Confusion matrix.",
      "slide_refs": [
        1
      ]
    },
    {
      "id": "pkg20_mcq4",
      "question": "Radial distortion is:",
      "options": {
        "A": "Barrel or pincushion warping",
        "B": "Tangential shift",
        "C": "Color aberration",
        "D": "Vignetting"
      },
      "correct_option": "A",
      "difficulty": "medium",
      "learning_objective": "Distortion types.",
      "slide_refs": [
        1
      ]
    },
    {
      "id": "pkg20_mcq5",
      "question": "Unsupervised learning includes:",
      "options": {
        "A": "k-means, PCA",
        "B": "SVM, NN",
        "C": "RL",
        "D": "All supervised"
      },
      "correct_option": "A",
      "difficulty": "easy",
      "learning_objective": "Unsupervised examples.",
      "slide_refs": [
        1
      ]
    },
    {
      "id": "pkg20_mcq6",
      "question": "Multi-view stereo is:",
      "options": {
        "A": "Dense 3D from multiple images",
        "B": "Single view depth",
        "C": "Motion only",
        "D": "Texture only"
      },
      "correct_option": "A",
      "difficulty": "medium",
      "learning_objective": "MVS.",
      "slide_refs": [
        1
      ]
    },
    {
      "id": "pkg20_mcq7",
      "question": "Hough transform:",
      "options": {
        "A": "Parametric detection",
        "B": "Region growing",
        "C": "Flow estimation",
        "D": "Color segmentation"
      },
      "correct_option": "A",
      "difficulty": "medium",
      "learning_objective": "Hough.",
      "slide_refs": [
        1
      ]
    },
    {
      "id": "pkg20_mcq8",
      "question": "MSER detects:",
      "options": {
        "A": "Stable regions across thresholds",
        "B": "Corners only",
        "C": "Lines",
        "D": "Motion"
      },
      "correct_option": "A",
      "difficulty": "hard",
      "learning_objective": "MSER principle.",
      "slide_refs": [
        1
      ]
    },
    {
      "id": "pkg20_mcq9",
      "question": "Optical flow applications include:",
      "options": {
        "A": "Video stabilization, tracking",
        "B": "Static image classification",
        "C": "Lens calibration",
        "D": "Histogram equalization"
      },
      "correct_option": "A",
      "difficulty": "medium",
      "learning_objective": "Flow usage.",
      "slide_refs": [
        1
      ]
    },
    {
      "id": "pkg20_mcq10",
      "question": "Roberts masks are for:",
      "options": {
        "A": "Diagonal edge detection",
        "B": "Smoothing",
        "C": "Sharpening",
        "D": "Color conversion"
      },
      "correct_option": "A",
      "difficulty": "medium",
      "learning_objective": "Roberts operator."
    },
    {
      "id": "pkg18_mcq1",
      "question": "HSV color space is useful for:",
      "options": {
        "A": "Intuitive color manipulation separating hue from intensity",
        "B": "Device calibration",
        "C": "3D modeling",
        "D": "Motion detection"
      },
      "correct_option": "A",
      "difficulty": "easy",
      "learning_objective": "HSV advantages.",
      "slide_refs": [
        1
      ]
    },
    {
      "id": "pkg18_mcq2",
      "question": "Radial distortion is modeled as:",
      "options": {
        "A": "Polynomial function of radius",
        "B": "Linear shear",
        "C": "Color shift",
        "D": "Noise addition"
      },
      "correct_option": "A",
      "difficulty": "medium",
      "learning_objective": "Radial distortion model.",
      "slide_refs": [
        1
      ]
    },
    {
      "id": "pkg18_mcq3",
      "question": "Aperture problem in flow:",
      "options": {
        "A": "One equation, two unknowns per pixel",
        "B": "Global illumination change",
        "C": "Sensor saturation",
        "D": "Lens flare"
      },
      "correct_option": "A",
      "difficulty": "hard",
      "learning_objective": "Flow ambiguity.",
      "slide_refs": [
        1
      ]
    },
    {
      "id": "pkg18_mcq4",
      "question": "Unsupervised learning finds:",
      "options": {
        "A": "Hidden structures without labels",
        "B": "Labeled predictions",
        "C": "Optimal paths",
        "D": "Camera poses"
      },
      "correct_option": "A",
      "difficulty": "easy",
      "learning_objective": "Unsupervised definition.",
      "slide_refs": [
        1
      ]
    },
    {
      "id": "pkg18_mcq5",
      "question": "A feature descriptor should be:",
      "options": {
        "A": "Invariant to transformations, discriminative",
        "B": "Global only",
        "C": "Color-dependent",
        "D": "High-dimensional always"
      },
      "correct_option": "A",
      "difficulty": "medium",
      "learning_objective": "Descriptor properties.",
      "slide_refs": [
        1
      ]
    },
    {
      "id": "pkg18_mcq6",
      "question": "Diffuse reflection is:",
      "options": {
        "A": "Lambertian, view-independent",
        "B": "Specular, mirror-like",
        "C": "Transparent",
        "D": "Emissive"
      },
      "correct_option": "A",
      "difficulty": "medium",
      "learning_objective": "Reflection types.",
      "slide_refs": [
        1
      ]
    },
    {
      "id": "pkg18_mcq7",
      "question": "Epipolar constraint reduces:",
      "options": {
        "A": "2D search to 1D line",
        "B": "3D to 2D",
        "C": "Color space",
        "D": "Texture dimensions"
      },
      "correct_option": "A",
      "difficulty": "medium",
      "learning_objective": "Epipolar utility.",
      "slide_refs": [
        1
      ]
    },
    {
      "id": "pkg18_mcq8",
      "question": "Optical flow assumes:",
      "options": {
        "A": "Brightness constancy and small motion",
        "B": "Large displacements",
        "C": "No occlusion",
        "D": "Known depths"
      },
      "correct_option": "A",
      "difficulty": "hard",
      "learning_objective": "Flow assumptions.",
      "slide_refs": [
        1
      ]
    },
    {
      "id": "pkg18_mcq9",
      "question": "LBP is rotation invariant when:",
      "options": {
        "A": "Using uniform patterns and rotation shifting",
        "B": "No modifications",
        "C": "With color",
        "D": "In 3D"
      },
      "correct_option": "A",
      "difficulty": "hard",
      "learning_objective": "LBP variants.",
      "slide_refs": [
        1
      ]
    },
    {
      "id": "pkg18_mcq10",
      "question": "Texture synthesis generates:",
      "options": {
        "A": "New samples from model",
        "B": "Only analysis",
        "C": "Depth maps",
        "D": "Motion fields"
      },
      "correct_option": "A",
      "difficulty": "medium",
      "learning_objective": "Texture tasks.",
      "slide_refs": [
        1
      ]
    },
    {
      "id": "pkg19_mcq1",
      "question": "Optical flow computes:",
      "options": {
        "A": "Pixel displacements over time",
        "B": "Static features",
        "C": "Color models",
        "D": "3D poses"
      },
      "correct_option": "A",
      "difficulty": "easy",
      "learning_objective": "Optical flow.",
      "slide_refs": [
        1
      ]
    },
    {
      "id": "pkg19_mcq2",
      "question": "SIFT detects:",
      "options": {
        "A": "Scale-invariant keypoints",
        "B": "Lines only",
        "C": "Colors",
        "D": "Motion"
      },
      "correct_option": "A",
      "difficulty": "medium",
      "learning_objective": "SIFT features.",
      "slide_refs": [
        1
      ]
    },
    {
      "id": "pkg19_mcq3",
      "question": "HSV is:",
      "options": {
        "A": "Cylindrical color model",
        "B": "Additive primaries",
        "C": "Opponent space",
        "D": "Device space"
      },
      "correct_option": "A",
      "difficulty": "easy",
      "learning_objective": "HSV.",
      "slide_refs": [
        1
      ]
    },
    {
      "id": "pkg19_mcq4",
      "question": "Aperture problem:",
      "options": {
        "A": "Local motion ambiguity",
        "B": "Global consistency",
        "C": "Color variation",
        "D": "Depth estimation"
      },
      "correct_option": "A",
      "difficulty": "medium",
      "learning_objective": "Aperture issue.",
      "slide_refs": [
        1
      ]
    },
    {
      "id": "pkg19_mcq5",
      "question": "Confusion matrix evaluates:",
      "options": {
        "A": "Classification errors",
        "B": "Regression fit",
        "C": "Clustering",
        "D": "Compression"
      },
      "correct_option": "A",
      "difficulty": "medium",
      "learning_objective": "Confusion matrix.",
      "slide_refs": [
        1
      ]
    },
    {
      "id": "pkg19_mcq6",
      "question": "Unsupervised learning is:",
      "options": {
        "A": "Pattern discovery without labels",
        "B": "Labeled training",
        "C": "Reward-based",
        "D": "Supervised validation"
      },
      "correct_option": "A",
      "difficulty": "easy",
      "learning_objective": "Unsupervised.",
      "slide_refs": [
        1
      ]
    },
    {
      "id": "pkg19_mcq7",
      "question": "Hough transform:",
      "options": {
        "A": "Detects lines via voting",
        "B": "Segments regions",
        "C": "Estimates flow",
        "D": "Balances colors"
      },
      "correct_option": "A",
      "difficulty": "medium",
      "learning_objective": "Hough.",
      "slide_refs": [
        1
      ]
    },
    {
      "id": "pkg19_mcq8",
      "question": "Bag of words in vision:",
      "options": {
        "A": "Histogram of visual words for classification",
        "B": "Text recognition",
        "C": "3D modeling",
        "D": "Motion analysis"
      },
      "correct_option": "A",
      "difficulty": "hard",
      "learning_objective": "BoW model.",
      "slide_refs": [
        1
      ]
    },
    {
      "id": "pkg19_mcq9",
      "question": "Harris detects:",
      "options": {
        "A": "Corners via structure tensor",
        "B": "Blobs",
        "C": "Lines",
        "D": "Circles"
      },
      "correct_option": "A",
      "difficulty": "medium",
      "learning_objective": "Harris detector.",
      "slide_refs": [
        1
      ]
    },
    {
      "id": "pkg19_mcq10",
      "question": "Morphology improves segmentation by:",
      "options": {
        "A": "Erosion/dilation for noise/holes",
        "B": "Gradient computation",
        "C": "Feature matching",
        "D": "Color conversion"
      },
      "correct_option": "A",
      "difficulty": "medium",
      "learning_objective": "Morphology usage."
    },
    {
      "id": "pkg21_mcq1",
      "question": "A feature descriptor is used to:",
      "options": {
        "A": "Match points across images",
        "B": "Calibrate cameras",
        "C": "Segment regions",
        "D": "Estimate depth"
      },
      "correct_option": "A",
      "difficulty": "easy",
      "learning_objective": "Descriptor role.",
      "slide_refs": [
        1
      ]
    },
    {
      "id": "pkg21_mcq2",
      "question": "Bayer filter arranges:",
      "options": {
        "A": "RGBG pattern for color capture",
        "B": "Edge filters",
        "C": "Motion sensors",
        "D": "Depth pixels"
      },
      "correct_option": "A",
      "difficulty": "medium",
      "learning_objective": "Bayer demosaicing.",
      "slide_refs": [
        1
      ]
    },
    {
      "id": "pkg21_mcq3",
      "question": "Aperture problem:",
      "options": {
        "A": "Local flow underdetermined",
        "B": "Global flow",
        "C": "Color problem",
        "D": "Distortion"
      },
      "correct_option": "A",
      "difficulty": "medium",
      "learning_objective": "Aperture.",
      "slide_refs": [
        1
      ]
    },
    {
      "id": "pkg21_mcq4",
      "question": "Hough transform:",
      "options": {
        "A": "Votes for lines/curves",
        "B": "Clusters data",
        "C": "Thresholds images",
        "D": "Detects corners"
      },
      "correct_option": "A",
      "difficulty": "medium",
      "learning_objective": "Hough.",
      "slide_refs": [
        1
      ]
    },
    {
      "id": "pkg21_mcq5",
      "question": "Mahalanobis distance:",
      "options": {
        "A": "Scales by covariance",
        "B": "Is Euclidean",
        "C": "For binary",
        "D": "For histograms"
      },
      "correct_option": "A",
      "difficulty": "hard",
      "learning_objective": "Mahalanobis.",
      "slide_refs": [
        1
      ]
    },
    {
      "id": "pkg21_mcq6",
      "question": "Grayscale histogram:",
      "options": {
        "A": "Intensity counts",
        "B": "Spatial map",
        "C": "Gradient hist",
        "D": "Color hist"
      },
      "correct_option": "A",
      "difficulty": "easy",
      "learning_objective": "Histogram.",
      "slide_refs": [
        1
      ]
    },
    {
      "id": "pkg21_mcq7",
      "question": "K-means:",
      "options": {
        "A": "Iterative centroid assignment",
        "B": "Hierarchical",
        "C": "Density-based",
        "D": "Spectral"
      },
      "correct_option": "A",
      "difficulty": "medium",
      "learning_objective": "k-means.",
      "slide_refs": [
        1
      ]
    },
    {
      "id": "pkg21_mcq8",
      "question": "Otsu's method:",
      "options": {
        "A": "Optimal threshold by variance",
        "B": "Adaptive threshold",
        "C": "Multi-threshold",
        "D": "Color threshold"
      },
      "correct_option": "A",
      "difficulty": "medium",
      "learning_objective": "Otsu.",
      "slide_refs": [
        1
      ]
    },
    {
      "id": "pkg21_mcq9",
      "question": "Harris corner uses:",
      "options": {
        "A": "Eigenvalues of structure tensor",
        "B": "Laplacian",
        "C": "Hessian",
        "D": "DoG"
      },
      "correct_option": "A",
      "difficulty": "hard",
      "learning_objective": "Harris.",
      "slide_refs": [
        1
      ]
    },
    {
      "id": "pkg21_mcq10",
      "question": "In stereo, z = f b / d, for d=2, b=6, f=1, z=:",
      "options": {
        "A": "3 cm",
        "B": "0.33 cm",
        "C": "12 cm",
        "D": "6 cm"
      },
      "correct_option": "A",
      "difficulty": "easy",
      "learning_objective": "Compute depth."
    },
    {
      "id": "pkg17_mcq1",
      "question": "Optical flow is the:",
      "options": {
        "A": "Apparent motion of pixels between frames",
        "B": "3D velocity field",
        "C": "Camera trajectory",
        "D": "Light direction"
      },
      "correct_option": "A",
      "difficulty": "easy",
      "learning_objective": "Define optical flow.",
      "slide_refs": [
        1
      ]
    },
    {
      "id": "pkg17_mcq2",
      "question": "SIFT is a:",
      "options": {
        "A": "Scale-invariant feature transform for detection and description",
        "B": "Color space",
        "C": "Clustering algorithm",
        "D": "Distance metric"
      },
      "correct_option": "A",
      "difficulty": "medium",
      "learning_objective": "Recall SIFT.",
      "slide_refs": [
        1
      ]
    },
    {
      "id": "pkg17_mcq3",
      "question": "A histogram is:",
      "options": {
        "A": "Frequency count of values",
        "B": "Spatial map",
        "C": "Gradient filter",
        "D": "Transformation matrix"
      },
      "correct_option": "A",
      "difficulty": "easy",
      "learning_objective": "Histogram definition.",
      "slide_refs": [
        1
      ]
    },
    {
      "id": "pkg17_mcq4",
      "question": "K-nearest neighbor classifies by:",
      "options": {
        "A": "Majority vote of k closest training examples",
        "B": "Centroid calculation",
        "C": "Linear regression",
        "D": "Neural activation"
      },
      "correct_option": "A",
      "difficulty": "medium",
      "learning_objective": "kNN principle.",
      "slide_refs": [
        1
      ]
    },
    {
      "id": "pkg17_mcq5",
      "question": "HSV separates:",
      "options": {
        "A": "Hue, saturation, value",
        "B": "Red, green, blue",
        "C": "Luminance, chrominance",
        "D": "Opponent colors"
      },
      "correct_option": "A",
      "difficulty": "easy",
      "learning_objective": "HSV components.",
      "slide_refs": [
        1
      ]
    },
    {
      "id": "pkg17_mcq6",
      "question": "Hough transform is for:",
      "options": {
        "A": "Parametric shape detection",
        "B": "Texture analysis",
        "C": "Color correction",
        "D": "Motion estimation"
      },
      "correct_option": "A",
      "difficulty": "medium",
      "learning_objective": "Hough usage.",
      "slide_refs": [
        1
      ]
    },
    {
      "id": "pkg17_mcq7",
      "question": "RANSAC handles:",
      "options": {
        "A": "Outliers in model fitting",
        "B": "Image noise only",
        "C": "Color balancing",
        "D": "Clustering"
      },
      "correct_option": "A",
      "difficulty": "medium",
      "learning_objective": "RANSAC principle.",
      "slide_refs": [
        1
      ]
    },
    {
      "id": "pkg17_mcq8",
      "question": "Mahalanobis distance is:",
      "options": {
        "A": "Covariance-weighted Euclidean",
        "B": "Manhattan distance",
        "C": "Cosine similarity",
        "D": "Hamming distance"
      },
      "correct_option": "A",
      "difficulty": "hard",
      "learning_objective": "Define Mahalanobis.",
      "slide_refs": [
        1
      ]
    },
    {
      "id": "pkg17_mcq9",
      "question": "Morphological operations can:",
      "options": {
        "A": "Clean segmentation by removing noise, filling holes",
        "B": "Compute gradients",
        "C": "Estimate motion",
        "D": "Classify colors"
      },
      "correct_option": "A",
      "difficulty": "medium",
      "learning_objective": "Morphology in segmentation.",
      "slide_refs": [
        1
      ]
    },
    {
      "id": "pkg17_mcq10",
      "question": "For texture classification, filter banks provide:",
      "options": {
        "A": "Multi-scale responses for features",
        "B": "Only edge detection",
        "C": "Color features",
        "D": "3D depths"
      },
      "correct_option": "A",
      "difficulty": "hard",
      "learning_objective": "Filter banks for texture.",
      "slide_refs": [
        1
      ]
    },
    {
      "id": "pkg2_mcq1",
      "question": "Which of the following is a monocular 3D cue that relies on the assumption that parallel lines in the real world converge to a vanishing point in the image?",
      "options": {
        "A": "Texture gradient",
        "B": "Interposition",
        "C": "Vanishing points",
        "D": "Motion parallax"
      },
      "correct_option": "C",
      "difficulty": "easy",
      "learning_objective": "Identify monocular 3D cues available in single 2D images.",
      "slide_refs": [
        3
      ]
    },
    {
      "id": "pkg2_mcq2",
      "question": "In the context of shape from shading, the image intensity \\(I\\) at a point \\(P\\) under diffuse reflection and unit albedo is given by \\(I = \\mathbf{N} \\cdot \\mathbf{L}\\). What does \\(\\mathbf{L}\\) represent?",
      "options": {
        "A": "Surface normal at point \\(P\\)",
        "B": "Incident light direction",
        "C": "Camera viewing direction",
        "D": "Reflected light intensity"
      },
      "correct_option": "B",
      "difficulty": "easy",
      "learning_objective": "Recall the Lambertian reflectance model for shape from shading.",
      "slide_refs": [
        6
      ]
    },
    {
      "id": "pkg2_mcq3",
      "question": "Which of the following statements about T-junctions is correct?",
      "options": {
        "A": "They indicate that the occluding object is farther away.",
        "B": "Two aligning T-junctions provide stronger evidence of occlusion than a single T-junction.",
        "C": "They occur when two surfaces have the same depth.",
        "D": "They are irrelevant to relative depth estimation."
      },
      "correct_option": "B",
      "difficulty": "medium",
      "learning_objective": "Understand the role of T-junctions in occlusion and relative depth perception.",
      "slide_refs": [
        2
      ]
    },
    {
      "id": "pkg2_mcq4",
      "question": "In shape from texture, the surface normal \\(\\mathbf{N}\\) is related to the slant \\(\\phi\\) and tilt \\(\\tau\\). Which angle represents the orientation of the surface normal projected into the image plane?",
      "options": {
        "A": "Slant \\(\\phi\\)",
        "B": "Tilt \\(\\tau\\)",
        "C": "Both slant and tilt",
        "D": "Neither; it requires 3D coordinates"
      },
      "correct_option": "B",
      "difficulty": "medium",
      "learning_objective": "Differentiate between slant and tilt in surface orientation estimation from texture.",
      "slide_refs": [
        5
      ]
    },
    {
      "id": "pkg2_mcq5",
      "question": "Under the assumptions of shape from shading with known light source direction \\(\\mathbf{L}\\) and unit albedo, the equation \\(I = \\cos \\theta_i\\) implies that:",
      "options": {
        "A": "Surface orientation is uniquely determined from intensity alone.",
        "B": "Additional constraints (e.g., integrability, smoothness) are required to recover the surface.",
        "C": "Only convex surfaces can be reconstructed.",
        "D": "The light source must be at infinity."
      },
      "correct_option": "B",
      "difficulty": "hard",
      "learning_objective": "Recognize the ill-posed nature of shape from shading and the need for regularization.",
      "slide_refs": [
        7
      ]
    },
    {
      "id": "pkg2_mcq6",
      "question": "In a 3D affine transformation represented in homogeneous coordinates, the bottom row of the 4×4 transformation matrix is:",
      "options": {
        "A": "[1 0 0 0]",
        "B": "[0 1 0 0]",
        "C": "[0 0 1 0]",
        "D": "[0 0 0 1]"
      },
      "correct_option": "D",
      "difficulty": "easy",
      "learning_objective": "Recall the structure of homogeneous coordinates for 3D affine transformations.",
      "slide_refs": [
        9
      ]
    },
    {
      "id": "pkg2_mcq7",
      "question": "A rigid 3D transformation consists of rotation and translation. How many degrees of freedom does it have?",
      "options": {
        "A": "3",
        "B": "6",
        "C": "9",
        "D": "12"
      },
      "correct_option": "B",
      "difficulty": "easy",
      "learning_objective": "Determine the degrees of freedom in rigid body motion.",
      "slide_refs": [
        9
      ]
    },
    {
      "id": "pkg2_mcq8",
      "question": "The rotation matrix \\(R_x(\\psi)\\) for rotation by angle \\(\\psi\\) about the x-axis is:",
      "options": {
        "A": "\\(\\begin{bmatrix} 1 & 0 & 0 \\\\ 0 & \\cos\\psi & -\\sin\\psi \\\\ 0 & \\sin\\psi & \\cos\\psi \\end{bmatrix}\\)",
        "B": "\\(\\begin{bmatrix} \\cos\\psi & 0 & \\sin\\psi \\\\ 0 & 1 & 0 \\\\ -\\sin\\psi & 0 & \\cos\\psi \\end{bmatrix}\\)",
        "C": "\\(\\begin{bmatrix} \\cos\\psi & -\\sin\\psi & 0 \\\\ \\sin\\psi & \\cos\\psi & 0 \\\\ 0 & 0 & 1 \\end{bmatrix}\\)",
        "D": "\\(\\begin{bmatrix} 1 & 0 & 0 \\\\ 0 & \\cos\\psi & \\sin\\psi \\\\ 0 & -\\sin\\psi & \\cos\\psi \\end{bmatrix}\\)"
      },
      "correct_option": "A",
      "difficulty": "medium",
      "learning_objective": "Recall the standard Euler rotation matrix for rotation about the x-axis.",
      "slide_refs": [
        11
      ]
    },
    {
      "id": "pkg2_mcq9",
      "question": "Which of the following is a limitation of the Euler angle representation for 3D rotations?",
      "options": {
        "A": "It uses more than 3 parameters.",
        "B": "It suffers from gimbal lock.",
        "C": "It cannot represent arbitrary rotations.",
        "D": "It is not orthogonal."
      },
      "correct_option": "B",
      "difficulty": "medium",
      "learning_objective": "Understand the drawbacks of Euler angles in rotation representation.",
      "slide_refs": [
        12
      ]
    },
    {
      "id": "pkg2_mcq10",
      "question": "In structured light 3D reconstruction, shadows can reveal surface shape because:",
      "options": {
        "A": "They indicate regions of self-occlusion.",
        "B": "They bend according to the surface normal.",
        "C": "They are independent of lighting direction.",
        "D": "They only appear under diffuse lighting."
      },
      "correct_option": "B",
      "difficulty": "hard",
      "learning_objective": "Explain how structured illumination aids in depth perception.",
      "slide_refs": [
        4
      ]
    }
  ],
  "essay": [
    {
      "id": "pkg1_essay1",
      "prompt": "Write an essay (approximately 400–600 words) that: (1) explains the major ethical, legal, and technical challenges when publishing open datasets containing personal information; and (2) proposes a concrete, defensible workflow an academic researcher could follow to prepare and publish such a dataset while minimising privacy risks and complying with relevant legal requirements.",
      "expected_keywords": [
        "informed consent",
        "data minimisation",
        "anonymisation",
        "pseudonymisation",
        "k-anonymity",
        "GDPR",
        "lawful basis",
        "metadata",
        "FAIR",
        "licensing",
        "CC BY",
        "ODbL",
        "access controls",
        "encryption",
        "data quality"
      ],
      "rubric": {
        "total_points": 100,
        "criteria": [
          {
            "keyword": "informed consent",
            "weight": 15,
            "description": "Discusses the need for informed, specific consent (or alternative lawful basis) and the limits of consent for future/unknown uses."
          },
          {
            "keyword": "GDPR",
            "weight": 15,
            "description": "Explains legal obligations under GDPR (personal data definition, lawful basis, data subject rights, data minimisation) or references equivalent legal principles."
          },
          {
            "keyword": "anonymisation",
            "weight": 12,
            "description": "Describes anonymisation strategies and their limitations; distinguishes anonymisation from pseudonymisation."
          },
          {
            "keyword": "pseudonymisation",
            "weight": 8,
            "description": "Explains separation of identifiers from data and when pseudonymisation is appropriate vs. insufficient."
          },
          {
            "keyword": "k-anonymity",
            "weight": 8,
            "description": "Mentions k-anonymity or similar technical metrics as concrete measures and notes risks of re-identification."
          },
          {
            "keyword": "data minimisation",
            "weight": 8,
            "description": "Argues for collecting and publishing only the data needed and reducing data granularity where possible."
          },
          {
            "keyword": "metadata",
            "weight": 6,
            "description": "Addresses the need for rich metadata for reuse and provenance, balanced with privacy considerations."
          },
          {
            "keyword": "FAIR",
            "weight": 6,
            "description": "References FAIR principles (Findable, Accessible, Interoperable, Re-usable) and discusses balancing FAIR with privacy."
          },
          {
            "keyword": "licensing",
            "weight": 6,
            "description": "Identifies appropriate open-data licenses (e.g., CC0/CC BY/ODbL) and the implications of license choice."
          },
          {
            "keyword": "access controls",
            "weight": 6,
            "description": "Proposes controlled-access mechanisms (embargo, tiered access, data use agreements) when full openness is unsafe."
          },
          {
            "keyword": "encryption",
            "weight": 5,
            "description": "Mentions technical safeguards like encryption for storage and transfer where relevant."
          },
          {
            "keyword": "data quality",
            "weight": 5,
            "description": "Addresses the importance of data quality (bias, accuracy) to ethical reuse and downstream decision-making."
          }
        ],
        "grading_notes": "Scores are additive across criteria. A defensible workflow should be concrete (steps, checks, and responsible actors) and should address legal, ethical, and technical safeguards. Partial credit is given for mentioning concepts without deep explanation; full credit requires correct reasoning and linkage between steps and safeguards."
      }
    },
    {
      "id": "pkg3_essay1",
      "prompt": "Compose a 500–700 word essay critically evaluating the mechanisms, implications, and methods for handling missing data in real-world datasets. Your discussion should: (1) compare MCAR, MAR, and MNAR; (2) analyze the statistical and practical consequences of different handling methods such as deletion, imputation, and maximum likelihood; and (3) argue for best practices in applying multiple imputation and EM-based methods for modern data mining tasks.",
      "expected_keywords": [
        "MCAR",
        "MAR",
        "MNAR",
        "deletion",
        "listwise deletion",
        "pairwise deletion",
        "mean imputation",
        "regression imputation",
        "stochastic regression",
        "multiple imputation",
        "maximum likelihood estimation",
        "EM algorithm",
        "bias",
        "uncertainty",
        "variance",
        "time series",
        "autocorrelation"
      ],
      "rubric": {
        "total_points": 100,
        "criteria": [
          {
            "keyword": "MCAR",
            "weight": 10,
            "description": "Accurately defines MCAR and its implications for unbiasedness when missing values are ignored."
          },
          {
            "keyword": "MAR",
            "weight": 10,
            "description": "Explains MAR with examples and highlights model assumptions for unbiased estimation."
          },
          {
            "keyword": "MNAR",
            "weight": 10,
            "description": "Describes MNAR and explains challenges in modeling missingness depending on unobserved data."
          },
          {
            "keyword": "deletion",
            "weight": 8,
            "description": "Compares listwise and pairwise deletion, addressing data loss and potential bias."
          },
          {
            "keyword": "mean imputation",
            "weight": 8,
            "description": "Discusses the simplicity and limitations (e.g., reduced variability, bias) of mean imputation."
          },
          {
            "keyword": "regression imputation",
            "weight": 8,
            "description": "Explains regression-based approaches and when they produce unbiased estimates."
          },
          {
            "keyword": "multiple imputation",
            "weight": 12,
            "description": "Analyzes multiple imputation’s workflow (imputation, analysis, pooling) and how it manages uncertainty."
          },
          {
            "keyword": "maximum likelihood estimation",
            "weight": 10,
            "description": "Describes how ML estimation and the EM algorithm estimate parameters from incomplete data."
          },
          {
            "keyword": "bias",
            "weight": 8,
            "description": "Evaluates the sources and effects of bias in missing data handling."
          },
          {
            "keyword": "uncertainty",
            "weight": 6,
            "description": "Explains how multiple imputation accounts for imputation uncertainty."
          },
          {
            "keyword": "time series",
            "weight": 5,
            "description": "Mentions longitudinal data challenges such as autocorrelation and timeline disruption."
          },
          {
            "keyword": "best practices",
            "weight": 5,
            "description": "Proposes practical and ethical data management recommendations for modern research."
          }
        ],
        "grading_notes": "High-quality essays integrate conceptual definitions, practical examples, and methodological reasoning. Full credit requires linking statistical methods with data mining implications."
      }
    },
    {
      "id": "pkg4_essay1",
      "prompt": "Write a 600–800 word analytical essay discussing the challenges and solutions for merging, sampling, and balancing data in data mining. Include: (1) methods to synchronize and harmonize multisource datasets, (2) statistical implications of over- and under-sampling, and (3) approaches for evaluating model performance under imbalanced data conditions using metrics such as balanced accuracy, F1-score, and Cohen’s Kappa.",
      "expected_keywords": [
        "data merging",
        "sampling frequency",
        "downsampling",
        "oversampling",
        "SRSWR",
        "SRSWOR",
        "balanced sample",
        "SMOTE",
        "artificial data",
        "noise injection",
        "class imbalance",
        "accuracy paradox",
        "balanced accuracy",
        "precision",
        "recall",
        "specificity",
        "F1 score",
        "Cohen’s Kappa"
      ],
      "rubric": {
        "total_points": 100,
        "criteria": [
          {
            "keyword": "data merging",
            "weight": 10,
            "description": "Explains technical and semantic challenges in integrating multisource datasets."
          },
          {
            "keyword": "sampling frequency",
            "weight": 8,
            "description": "Describes how sampling rates affect signal merging and data consistency."
          },
          {
            "keyword": "downsampling",
            "weight": 6,
            "description": "Discusses the benefits and information loss associated with downsampling."
          },
          {
            "keyword": "oversampling",
            "weight": 6,
            "description": "Explains the computational implications and redundancy risks of oversampling."
          },
          {
            "keyword": "SMOTE",
            "weight": 10,
            "description": "Describes the SMOTE algorithm and its use in addressing class imbalance."
          },
          {
            "keyword": "class imbalance",
            "weight": 10,
            "description": "Analyzes causes and implications of imbalanced datasets in predictive modeling."
          },
          {
            "keyword": "balanced accuracy",
            "weight": 8,
            "description": "Applies balanced accuracy to correctly interpret model fairness under imbalance."
          },
          {
            "keyword": "precision",
            "weight": 6,
            "description": "Defines and interprets precision as a metric sensitive to false positives."
          },
          {
            "keyword": "recall",
            "weight": 6,
            "description": "Explains recall’s role in sensitivity and detection of positive classes."
          },
          {
            "keyword": "F1 score",
            "weight": 8,
            "description": "Evaluates trade-offs between precision and recall via F1 metric."
          },
          {
            "keyword": "Cohen’s Kappa",
            "weight": 8,
            "description": "Explains Kappa’s interpretation beyond chance agreement and its importance in classifier reliability."
          },
          {
            "keyword": "accuracy paradox",
            "weight": 8,
            "description": "Critically examines the misleading nature of accuracy in imbalanced datasets."
          }
        ],
        "grading_notes": "Essays should synthesize technical, statistical, and interpretive aspects of data preparation and model evaluation. Full credit requires clear integration of examples from merging, sampling, and performance assessment."
      }
    },
    {
      "id": "pkg2_essay1",
      "prompt": "Discuss the key methodological, ethical, and practical considerations in planning and conducting a human-centered data collection study (such as the wearable-sensor gym activity project presented in the lecture). Your answer should: (1) describe how to plan for repeatability, reproducibility, and accuracy; (2) outline how ethical concerns such as informed consent, privacy, and risk-benefit balance should be addressed; and (3) suggest strategies for mitigating technical and human-related data collection problems.",
      "expected_keywords": [
        "planning",
        "repeatability",
        "reproducibility",
        "accuracy",
        "informed consent",
        "privacy",
        "confidentiality",
        "risk-benefit balance",
        "sample size",
        "non-stationary data",
        "bias",
        "sensor calibration",
        "documentation",
        "Bluetooth connectivity",
        "volunteer recruitment"
      ],
      "rubric": {
        "total_points": 100,
        "criteria": [
          {
            "keyword": "planning",
            "weight": 10,
            "description": "Clearly outlines planning steps including defining goals, procedures, and methodologies."
          },
          {
            "keyword": "repeatability",
            "weight": 8,
            "description": "Describes procedures to ensure data can be replicated under similar conditions."
          },
          {
            "keyword": "reproducibility",
            "weight": 8,
            "description": "Explains how others can independently reproduce results from the same dataset."
          },
          {
            "keyword": "accuracy",
            "weight": 8,
            "description": "Mentions calibration, signal verification, and error checking to maintain data accuracy."
          },
          {
            "keyword": "informed consent",
            "weight": 12,
            "description": "Discusses voluntary, informed participation and explains consent procedures for human subjects."
          },
          {
            "keyword": "privacy",
            "weight": 8,
            "description": "Addresses protection of personal information and limits on data disclosure."
          },
          {
            "keyword": "confidentiality",
            "weight": 6,
            "description": "Explains how data will be securely stored and shared only with authorized personnel."
          },
          {
            "keyword": "risk-benefit balance",
            "weight": 8,
            "description": "Evaluates participant risks versus benefits and proposes measures to minimize harm."
          },
          {
            "keyword": "sample size",
            "weight": 6,
            "description": "Includes rationale for selecting a statistically significant sample size."
          },
          {
            "keyword": "non-stationary data",
            "weight": 6,
            "description": "Explains how variability in human or environmental conditions affects data collection."
          },
          {
            "keyword": "bias",
            "weight": 6,
            "description": "Discusses researcher and respondent biases and strategies to reduce them."
          },
          {
            "keyword": "sensor calibration",
            "weight": 5,
            "description": "Mentions procedures for checking and aligning sensors before and during experiments."
          },
          {
            "keyword": "documentation",
            "weight": 5,
            "description": "Emphasizes the role of thorough documentation for traceability and quality assurance."
          },
          {
            "keyword": "Bluetooth connectivity",
            "weight": 4,
            "description": "Identifies technical reliability issues and mitigation approaches (e.g., real-time monitoring)."
          },
          {
            "keyword": "volunteer recruitment",
            "weight": 4,
            "description": "Notes practical difficulties in participant recruitment and compliance."
          }
        ],
        "grading_notes": "Award full marks for structured, well-argued essays demonstrating integration of ethical, methodological, and practical aspects. Partial credit given for coverage of individual points without synthesis."
      }
    },
    {
      "id": "pkg8_essay1",
      "prompt": "Write a 700–900 word essay discussing model generalization in data mining. Your essay should (1) compare explanatory, predictive, and descriptive modeling, (2) describe how sample size, data partitioning, and validation strategies affect generalization, and (3) analyze the importance of managing temporal dependence and model complexity when building reliable models.",
      "expected_keywords": [
        "model generalization",
        "descriptive modeling",
        "explanatory modeling",
        "predictive modeling",
        "sample size",
        "training and test sets",
        "validation set",
        "cross-validation",
        "leave-one-out",
        "individual models",
        "population models",
        "temporal dependence",
        "sliding window",
        "overfitting",
        "model selection",
        "bias-variance tradeoff"
      ],
      "rubric": {
        "total_points": 100,
        "criteria": [
          {
            "keyword": "model generalization",
            "weight": 10,
            "description": "Defines generalization and explains its significance for unseen data."
          },
          {
            "keyword": "model types",
            "weight": 12,
            "description": "Compares descriptive, explanatory, and predictive models with real examples."
          },
          {
            "keyword": "sample size",
            "weight": 10,
            "description": "Explains how training sample size impacts bias, variance, and overfitting."
          },
          {
            "keyword": "data partitioning",
            "weight": 10,
            "description": "Describes correct use of training, validation, and test sets to ensure fairness."
          },
          {
            "keyword": "cross-validation",
            "weight": 8,
            "description": "Analyzes k-fold and leave-one-out strategies for robust evaluation."
          },
          {
            "keyword": "temporal dependence",
            "weight": 10,
            "description": "Discusses the pitfalls of random sampling and temporal leakage in time-dependent data."
          },
          {
            "keyword": "overfitting",
            "weight": 10,
            "description": "Explains overfitting prevention using validation and model simplification."
          },
          {
            "keyword": "model complexity",
            "weight": 8,
            "description": "Examines why simpler models often generalize better."
          },
          {
            "keyword": "feature extraction",
            "weight": 6,
            "description": "Describes how sliding windows and feature engineering support generalization."
          },
          {
            "keyword": "bias-variance tradeoff",
            "weight": 6,
            "description": "Connects generalization to balance between bias and variance."
          }
        ],
        "grading_notes": "Essays should demonstrate deep conceptual understanding, practical reasoning, and examples connecting model validation, data partitioning, and generalization theory."
      }
    },
    {
      "id": "pkg6_essay1",
      "prompt": "Compose a 700–900 word essay explaining how noise, signal saturation, and outliers affect data mining workflows. Your essay should (1) classify noise and discuss its impact on supervised learning, (2) explain various outlier detection methods such as statistical, proximity, clustering, and classification-based approaches, and (3) discuss practical strategies for distinguishing anomalies from harmful data errors in real-world contexts like sensor analysis or fraud detection.",
      "expected_keywords": [
        "noise",
        "attribute noise",
        "label noise",
        "data polishing",
        "robust learners",
        "noise filtering",
        "signal saturation",
        "outliers",
        "global outliers",
        "contextual outliers",
        "collective outliers",
        "anomalies",
        "outlier detection",
        "statistical methods",
        "kernel density estimation",
        "kNN",
        "Local Outlier Factor",
        "clustering methods",
        "one-class SVM",
        "time series"
      ],
      "rubric": {
        "total_points": 100,
        "criteria": [
          {
            "keyword": "noise",
            "weight": 10,
            "description": "Defines noise and distinguishes between attribute and label noise."
          },
          {
            "keyword": "data polishing",
            "weight": 8,
            "description": "Explains noise correction techniques and their limitations."
          },
          {
            "keyword": "robust learners",
            "weight": 8,
            "description": "Describes algorithms resilient to data corruption."
          },
          {
            "keyword": "signal saturation",
            "weight": 8,
            "description": "Discusses how sensor saturation distorts data and analysis."
          },
          {
            "keyword": "outliers",
            "weight": 10,
            "description": "Defines outliers and explains their statistical effects."
          },
          {
            "keyword": "outlier detection",
            "weight": 12,
            "description": "Covers multiple outlier detection techniques and their assumptions."
          },
          {
            "keyword": "Local Outlier Factor",
            "weight": 8,
            "description": "Explains the principle of local density comparison in outlier identification."
          },
          {
            "keyword": "clustering methods",
            "weight": 8,
            "description": "Discusses clustering-based approaches and their scalability limits."
          },
          {
            "keyword": "one-class SVM",
            "weight": 8,
            "description": "Describes how one-class SVMs identify anomalies without labeled outlier data."
          },
          {
            "keyword": "anomalies",
            "weight": 6,
            "description": "Differentiates anomalies from noise and outliers with domain examples."
          },
          {
            "keyword": "time series",
            "weight": 6,
            "description": "Explains predictive and unsupervised techniques for detecting temporal anomalies."
          },
          {
            "keyword": "application examples",
            "weight": 8,
            "description": "Illustrates methods through real-world cases like fraud detection or sensor monitoring."
          }
        ],
        "grading_notes": "Essays should integrate theoretical understanding with practical applications and emphasize conceptual distinctions between noise, outliers, and anomalies. Full credit requires linking detection techniques to real-world contexts."
      }
    },
    {
      "id": "pkg7_essay1",
      "prompt": "Write a 700–900 word essay analyzing the role of normalization, discretization, data reduction, and transformations to normality in preparing data for data mining. Discuss (1) when and why each preprocessing step is necessary, (2) compare techniques like Z-score, Min-Max scaling, PCA, and Box-Cox transformation, and (3) evaluate the trade-offs between information preservation, interpretability, and computational efficiency.",
      "expected_keywords": [
        "normalization",
        "Min-Max normalization",
        "Z-score standardization",
        "decimal scaling",
        "discretization",
        "categorical data",
        "dummy coding",
        "data reduction",
        "aggregation",
        "binning",
        "PCA",
        "Discrete Wavelet Transform",
        "Box-Cox transformation",
        "Gaussian distribution",
        "non-parametric methods",
        "information loss",
        "dimensionality reduction"
      ],
      "rubric": {
        "total_points": 100,
        "criteria": [
          {
            "keyword": "normalization",
            "weight": 10,
            "description": "Explains the concept, purpose, and methods of normalization including scaling and standardization."
          },
          {
            "keyword": "discretization",
            "weight": 10,
            "description": "Discusses discretization techniques, their use cases, and drawbacks."
          },
          {
            "keyword": "dummy coding",
            "weight": 8,
            "description": "Describes how categorical variables can be represented numerically."
          },
          {
            "keyword": "data reduction",
            "weight": 10,
            "description": "Explains reduction goals and methods such as aggregation, binning, and sampling."
          },
          {
            "keyword": "PCA",
            "weight": 10,
            "description": "Discusses PCA’s function, advantages, and interpretation limits."
          },
          {
            "keyword": "Box-Cox transformation",
            "weight": 8,
            "description": "Explains transformation for normality and constraints (e.g., positive-only data)."
          },
          {
            "keyword": "information loss",
            "weight": 8,
            "description": "Evaluates trade-offs and risks in discretization and data reduction."
          },
          {
            "keyword": "computational efficiency",
            "weight": 8,
            "description": "Analyzes how preprocessing impacts computational performance and model training time."
          },
          {
            "keyword": "Gaussian distribution",
            "weight": 8,
            "description": "Relates transformation to normality requirements in parametric models."
          },
          {
            "keyword": "interpretability",
            "weight": 10,
            "description": "Assesses how preprocessing steps affect transparency and ease of result interpretation."
          }
        ],
        "grading_notes": "Full credit requires a structured argument linking normalization, reduction, and transformation steps with their mathematical and practical roles in data mining. Examples from engineering or physical sciences should be included where applicable."
      }
    },
    {
      "id": "pkg10_essay1",
      "prompt": "Provide a comprehensive explanation of the pinhole camera model and its extensions in computer vision. Start with the ideal perspective projection geometry and derive the mapping from 3D world points to 2D image points using both Cartesian and homogeneous coordinates. Present the full camera matrix \\(P = K [R \\; t]\\) and explain each component of the intrinsic matrix \\(K\\) (focal length, principal point, skew, pixel aspect ratio). Discuss the role of extrinsic parameters and the decomposition of \\(P\\). Then, cover real-world deviations: radial and tangential lens distortion models, and their parameterization (k1, k2, p1, p2). Describe the standard calibration procedure using multiple views of a checkerboard pattern, including corner detection, homography estimation, and nonlinear optimization. Finally, discuss limitations of the pinhole model (finite aperture blur, rolling shutter, chromatic aberration) and advanced models (rational polynomial, fisheye). Include mathematical formulations and practical implications for downstream tasks like 3D reconstruction and augmented reality.",
      "expected_keywords": [
        "pinhole model",
        "perspective projection",
        "homogeneous coordinates",
        "camera matrix",
        "intrinsic parameters",
        "K matrix",
        "focal length",
        "principal point",
        "skew",
        "extrinsic parameters",
        "rotation R",
        "translation t",
        "projection matrix P",
        "radial distortion",
        "tangential distortion",
        "distortion coefficients",
        "camera calibration",
        "checkerboard",
        "homography",
        "Zhang's method",
        "corner detection",
        "nonlinear optimization",
        "vanishing points",
        "weak perspective",
        "affine camera",
        "rolling shutter",
        "chromatic aberration"
      ],
      "rubric": {
        "total_points": 100,
        "criteria": [
          {
            "keyword": "pinhole model",
            "weight": 6,
            "description": "Defines ideal perspective projection with infinitesimal aperture."
          },
          {
            "keyword": "perspective projection",
            "weight": 7,
            "description": "Derives \\(x = f X/Z, y = f Y/Z\\) geometrically."
          },
          {
            "keyword": "homogeneous coordinates",
            "weight": 7,
            "description": "Shows 3D → 2D via 3×4 matrix and dehomogenization."
          },
          {
            "keyword": "camera matrix",
            "weight": 6,
            "description": "States \\(P = K [R | t]\\) with correct dimensions."
          },
          {
            "keyword": "intrinsic parameters",
            "weight": 6,
            "description": "Lists all five: \\(\\alpha_x, \\alpha_y, s, c_x, c_y\\)."
          },
          {
            "keyword": "K matrix",
            "weight": 6,
            "description": "Writes upper-triangular form explicitly."
          },
          {
            "keyword": "focal length",
            "weight": 5,
            "description": "Relates physical \\(f\\) to pixel units via sensor size."
          },
          {
            "keyword": "principal point",
            "weight": 5,
            "description": "Optical center offset; often near image center."
          },
          {
            "keyword": "skew",
            "weight": 4,
            "description": "Non-zero in scanned film or misaligned sensors."
          },
          {
            "keyword": "extrinsic parameters",
            "weight": 5,
            "description": "6 DOF rigid transformation from world to camera."
          },
          {
            "keyword": "rotation R",
            "weight": 4,
            "description": "3×3 orthogonal matrix with det=1."
          },
          {
            "keyword": "translation t",
            "weight": 4,
            "description": "Camera center in world coordinates."
          },
          {
            "keyword": "projection matrix P",
            "weight": 5,
            "description": "11 DOF (up to scale); QR-like decomposition."
          },
          {
            "keyword": "radial distortion",
            "weight": 6,
            "description": "Polynomial model with \\(k_1, k_2, k_3\\); barrel/pincushion."
          },
          {
            "keyword": "tangential distortion",
            "weight": 5,
            "description": "Parameters \\(p_1, p_2\\) for decentering."
          },
          {
            "keyword": "distortion coefficients",
            "weight": 4,
            "description": "Typically 5 parameters total."
          },
          {
            "keyword": "camera calibration",
            "weight": 6,
            "description": "Multi-view optimization of reprojection error."
          },
          {
            "keyword": "checkerboard",
            "weight": 4,
            "description": "Known 3D corners for correspondences."
          },
          {
            "keyword": "homography",
            "weight": 5,
            "description": "Planar → image mapping per view."
          },
          {
            "keyword": "Zhang's method",
            "weight": 5,
            "description": "Closed-form then nonlinear refinement."
          },
          {
            "keyword": "corner detection",
            "weight": 4,
            "description": "Sub-pixel Harris or saddle-point methods."
          },
          {
            "keyword": "nonlinear optimization",
            "weight": 5,
            "description": "Levenberg-Marquardt on all parameters."
          },
          {
            "keyword": "vanishing points",
            "weight": 3,
            "description": "Bonus: lines at infinity for calibration."
          },
          {
            "keyword": "weak perspective",
            "weight": 3,
            "description": "Bonus: approximation for distant objects."
          },
          {
            "keyword": "rolling shutter",
            "weight": 3,
            "description": "Bonus: CMOS readout distortion."
          },
          {
            "keyword": "chromatic aberration",
            "weight": 3,
            "description": "Bonus: wavelength-dependent focal length."
          }
        ],
        "grading_notes": "Require correct projection equations and full K matrix for full marks. Award partial credit for good geometric intuition without full math. Deduct if intrinsic/extrinsic roles are swapped or distortion model direction is wrong. Bonus for mentioning modern tools (OpenCV calibrateCamera)."
      }
    },
    {
      "id": "pkg7_essay1",
      "prompt": "Explain the role of texture as a monocular cue for 3D shape perception. Define the concepts of slant and tilt and describe how isotropic texture is distorted under perspective projection. Derive the relationship between surface orientation and the aspect ratio/orientation of projected texture elements (e.g., via ellipse fitting in the frequency domain). Discuss the isotropy and homogeneity assumptions, their geometric implications, and common violations in real scenes. Compare shape from texture with shape from shading in terms of input requirements, robustness, and output (qualitative vs. metric). Include a brief mention of texture segmentation using statistical descriptors.",
      "expected_keywords": [
        "texture analysis",
        "shape from texture",
        "monocular cue",
        "slant",
        "tilt",
        "isotropy",
        "homogeneity",
        "foreshortening",
        "perspective projection",
        "ellipse fitting",
        "autocorrelation",
        "power spectrum",
        "frequency domain",
        "aspect ratio",
        "surface normal",
        "shape from shading",
        "statistical descriptors",
        "co-occurrence matrix",
        "segmentation"
      ],
      "rubric": {
        "total_points": 100,
        "criteria": [
          {
            "keyword": "texture analysis",
            "weight": 5,
            "description": "Defines texture as repetitive local pattern; role in segmentation and shape."
          },
          {
            "keyword": "shape from texture",
            "weight": 7,
            "description": "States goal: recover surface orientation from texture distortion."
          },
          {
            "keyword": "monocular cue",
            "weight": 5,
            "description": "Classifies as single-image 3D cue alongside shading, contour."
          },
          {
            "keyword": "slant",
            "weight": 6,
            "description": "Angle between surface normal and viewing direction (0° frontal, 90° edge-on)."
          },
          {
            "keyword": "tilt",
            "weight": 6,
            "description": "Direction of steepest ascent in image plane."
          },
          {
            "keyword": "isotropy",
            "weight": 7,
            "description": "Assumes texture statistically uniform in all 3D directions."
          },
          {
            "keyword": "homogeneity",
            "weight": 5,
            "description": "Texture density constant over surface."
          },
          {
            "keyword": "foreshortening",
            "weight": 7,
            "description": "Compression along tilt direction proportional to \\(\\cos \\phi\\)."
          },
          {
            "keyword": "perspective projection",
            "weight": 6,
            "description": "Links 3D slant to 2D affine distortion of texture elements."
          },
          {
            "keyword": "ellipse fitting",
            "weight": 8,
            "description": "Explains fitting ellipse to spectral peak or autocorrelation to estimate axes."
          },
          {
            "keyword": "autocorrelation",
            "weight": 5,
            "description": "Mentions local ACF reveals texel shape and spacing."
          },
          {
            "keyword": "power spectrum",
            "weight": 5,
            "description": "Frequency domain dual: energy concentrated in elliptical region."
          },
          {
            "keyword": "frequency domain",
            "weight": 5,
            "description": "Advantage: robust to phase, sensitive to scale/orientation."
          },
          {
            "keyword": "aspect ratio",
            "weight": 6,
            "description": "Minor/major axis ratio = \\(\\cos \\phi\\); major axis aligned with tilt."
          },
          {
            "keyword": "surface normal",
            "weight": 6,
            "description": "Recovers normal up to ambiguity (convex/concave)."
          },
          {
            "keyword": "shape from shading",
            "weight": 6,
            "description": "Compares: SfT needs texture, SfS needs smooth surface and known light."
          },
          {
            "keyword": "statistical descriptors",
            "weight": 5,
            "description": "Briefly notes GLCM, LBP for segmentation."
          },
          {
            "keyword": "co-occurrence matrix",
            "weight": 4,
            "description": "Captures spatial gray-level dependencies."
          },
          {
            "keyword": "segmentation",
            "weight": 4,
            "description": "Texture features for region labeling."
          }
        ],
        "grading_notes": "Require geometric derivation (foreshortening → ellipse) for full marks. Allow qualitative explanation of spectrum method. Deduct if slant/tilt confused or assumptions misstated. Partial credit for good intuition without math."
      }
    },
    {
      "id": "pkg8_essay1",
      "prompt": "Explain the complete pipeline for binary image analysis as presented in the lecture. Start with the motivation for binarization and the challenges posed by real-world imaging conditions (e.g., lighting, sensor noise). Describe global and adaptive thresholding methods, their assumptions, and failure cases. Then, detail the role of connectivity (4- vs 8-connected) and the two-pass connected component labeling algorithm using provisional labels and equivalence resolution. Finally, discuss morphological post-processing (opening, closing) with structuring elements to clean up noise and fill holes, and conclude with applications such as document OCR or object counting. Include mathematical definitions where appropriate (e.g., erosion, dilation).",
      "expected_keywords": [
        "binarization",
        "thresholding",
        "global threshold",
        "adaptive threshold",
        "illumination variation",
        "connected component",
        "4-connected",
        "8-connected",
        "labeling algorithm",
        "two-pass",
        "provisional label",
        "equivalence",
        "Union-Find",
        "morphology",
        "structuring element",
        "erosion",
        "dilation",
        "opening",
        "closing",
        "noise removal",
        "hole filling",
        "OCR",
        "document analysis"
      ],
      "rubric": {
        "total_points": 100,
        "criteria": [
          {
            "keyword": "binarization",
            "weight": 6,
            "description": "Defines conversion of grayscale to binary via intensity threshold."
          },
          {
            "keyword": "thresholding",
            "weight": 6,
            "description": "Explains \\(B(x,y) = 1\\) if \\(I(x,y) > T\\), else 0."
          },
          {
            "keyword": "global threshold",
            "weight": 5,
            "description": "Single \\(T\\) for whole image; fails under non-uniform light."
          },
          {
            "keyword": "adaptive threshold",
            "weight": 6,
            "description": "Local \\(T\\) per region (e.g., mean of neighborhood)."
          },
          {
            "keyword": "illumination variation",
            "weight": 5,
            "description": "Identifies shading, shadows as key challenges."
          },
          {
            "keyword": "connected component",
            "weight": 6,
            "description": "Maximal set of foreground pixels connected via adjacency."
          },
          {
            "keyword": "4-connected",
            "weight": 4,
            "description": "Horizontal/vertical neighbors only."
          },
          {
            "keyword": "8-connected",
            "weight": 4,
            "description": "Includes diagonal neighbors."
          },
          {
            "keyword": "labeling algorithm",
            "weight": 6,
            "description": "Assigns unique ID to each component."
          },
          {
            "keyword": "two-pass",
            "weight": 7,
            "description": "First pass: provisional labels + equivalence table; second: resolve."
          },
          {
            "keyword": "provisional label",
            "weight": 5,
            "description": "Temporary IDs during raster scan."
          },
          {
            "keyword": "equivalence",
            "weight": 5,
            "description": "Merged labels when adjacent components connect."
          },
          {
            "keyword": "Union-Find",
            "weight": 5,
            "description": "Efficient data structure for equivalence class management."
          },
          {
            "keyword": "morphology",
            "weight": 5,
            "description": "Set operations using structuring element \\(S\\)."
          },
          {
            "keyword": "structuring element",
            "weight": 5,
            "description": "Small binary mask (e.g., 3×3 cross or disk)."
          },
          {
            "keyword": "erosion",
            "weight": 5,
            "description": "\\(A \\ominus S = \\{z \\mid S_z \\subseteq A\\}\\)."
          },
          {
            "keyword": "dilation",
            "weight": 5,
            "description": "\\(A \\oplus S = \\{z \\mid S_z \\cap A \\neq \\emptyset\\}\\)."
          },
          {
            "keyword": "opening",
            "weight": 5,
            "description": "Erosion then dilation; removes small objects."
          },
          {
            "keyword": "closing",
            "weight": 5,
            "description": "Dilation then erosion; fills small holes."
          },
          {
            "keyword": "noise removal",
            "weight": 4,
            "description": "Opening eliminates salt noise and thin protrusions."
          },
          {
            "keyword": "hole filling",
            "weight": 4,
            "description": "Closing merges gaps in foreground."
          },
          {
            "keyword": "OCR",
            "weight": 4,
            "description": "Clean binary text enables character segmentation."
          },
          {
            "keyword": "document analysis",
            "weight": 4,
            "description": "Layout analysis, table detection, etc."
          }
        ],
        "grading_notes": "Require correct math for erosion/dilation and two-pass labeling logic. Full credit only if adaptive thresholding and morphology sequence are justified. Partial credit for good flow without full formalism. Deduct if connectivity types are confused or applications missing."
      }
    },
    {
      "id": "pkg9_essay1",
      "prompt": "Explain the physics and perception of color in computer vision, focusing on the dichromatic reflection model and its implications for highlight analysis and color constancy. Start with light-surface interaction (body vs. interface reflection) and derive the linear clustering property in RGB space for neutral interface reflection (NIR). Describe how this enables specular highlight detection and removal using pixel clustering or line fitting. Then, discuss computational color constancy: define the problem, explain the Gray World and White Patch assumptions, and their limitations under colored illuminants or non-Lambertian surfaces. Compare RGB-based methods with learning-based approaches (e.g., gamut mapping). Conclude with the role of color in higher-level vision tasks (segmentation, recognition) and challenges posed by sensor metamerism.",
      "expected_keywords": [
        "color perception",
        "additive primaries",
        "RGB",
        "CIE chromaticity",
        "spectral locus",
        "dichromatic model",
        "body reflection",
        "interface reflection",
        "specular highlight",
        "neutral interface",
        "RGB clustering",
        "highlight removal",
        "color constancy",
        "illuminant estimation",
        "Gray World",
        "White Patch",
        "gamut mapping",
        "metamerism",
        "sensor sensitivity",
        "segmentation",
        "recognition"
      ],
      "rubric": {
        "total_points": 100,
        "criteria": [
          {
            "keyword": "color perception",
            "weight": 5,
            "description": "Links human cone responses to trichromatic theory."
          },
          {
            "keyword": "additive primaries",
            "weight": 4,
            "description": "Mentions RGB basis for displays."
          },
          {
            "keyword": "RGB",
            "weight": 4,
            "description": "Standard image representation."
          },
          {
            "keyword": "CIE chromaticity",
            "weight": 5,
            "description": "Normalizes for brightness; shows diagram."
          },
          {
            "keyword": "spectral locus",
            "weight": 5,
            "description": "Boundary of visible monochromatic colors."
          },
          {
            "keyword": "dichromatic model",
            "weight": 7,
            "description": "Two-term: \\(I = I_b + I_s\\)."
          },
          {
            "keyword": "body reflection",
            "weight": 6,
            "description": "Lambertian, colored by surface albedo."
          },
          {
            "keyword": "interface reflection",
            "weight": 6,
            "description": "Mirror-like, colored by light (white for NIR)."
          },
          {
            "keyword": "specular highlight",
            "weight": 6,
            "description": "Bright spots where view aligns with reflection."
          },
          {
            "keyword": "neutral interface",
            "weight": 6,
            "description": "Specular component = illuminant color."
          },
          {
            "keyword": "RGB clustering",
            "weight": 8,
            "description": "Diffuse pixels cluster; specular form line to light color."
          },
          {
            "keyword": "highlight removal",
            "weight": 7,
            "description": "Subtract specular via clustering or inpainting."
          },
          {
            "keyword": "color constancy",
            "weight": 7,
            "description": "Estimate illuminant to recover canonical albedo."
          },
          {
            "keyword": "illuminant estimation",
            "weight": 6,
            "description": "Goal: find light color from image alone."
          },
          {
            "keyword": "Gray World",
            "weight": 6,
            "description": "Average image color = gray → light = average."
          },
          {
            "keyword": "White Patch",
            "weight": 6,
            "description": "Brightest pixel = white under light."
          },
          {
            "keyword": "gamut mapping",
            "weight": 5,
            "description": "Constrain possible illuminants by surface gamut."
          },
          {
            "keyword": "metamerism",
            "weight": 5,
            "description": "Different spectra → same RGB."
          },
          {
            "keyword": "sensor sensitivity",
            "weight": 4,
            "description": "Camera ≠ human cones → color errors."
          },
          {
            "keyword": "segmentation",
            "weight": 4,
            "description": "Color as feature beyond intensity."
          },
          {
            "keyword": "recognition",
            "weight": 4,
            "description": "Material/color diagnostic for object ID."
          }
        ],
        "grading_notes": "Require geometric explanation of dichromatic lines in RGB. Full credit only if both Gray World and White Patch are critiqued (e.g., fails on single-color scenes). Allow modern methods mention for bonus. Deduct if reflection model confused with shading."
      }
    },
    {
      "id": "pkg15_essay1",
      "prompt": "Discuss texture classification using gradient features and LBP. For the given patches, compute Roberts gradients, mean feature vectors, and classify unknown using Euclidean distance. Describe LBP computation and histogram-based classification. Note issues with small patches. Compare to filter banks for texture analysis.",
      "expected_keywords": [
        "texture classification",
        "Roberts masks",
        "feature vectors",
        "Euclidean distance",
        "LBP operator",
        "binary patterns",
        "histogram",
        "patch size issue",
        "filter banks",
        "multi-scale analysis"
      ],
      "rubric": {
        "total_points": 100,
        "criteria": [
          {
            "keyword": "texture classification",
            "weight": 10,
            "description": "Goal: assign class labels."
          },
          {
            "keyword": "Roberts masks",
            "weight": 15,
            "description": "Apply to compute derivatives."
          },
          {
            "keyword": "feature vectors",
            "weight": 15,
            "description": "Means of responses."
          },
          {
            "keyword": "Euclidean distance",
            "weight": 10,
            "description": "Classify to nearest."
          },
          {
            "keyword": "LBP operator",
            "weight": 15,
            "description": "Neighborhood comparisons."
          },
          {
            "keyword": "binary patterns",
            "weight": 10,
            "description": "Code to decimal."
          },
          {
            "keyword": "histogram",
            "weight": 10,
            "description": "Distribution as feature."
          },
          {
            "keyword": "patch size issue",
            "weight": 5,
            "description": "Small samples limit stats."
          },
          {
            "keyword": "filter banks",
            "weight": 5,
            "description": "Multi-filter responses."
          },
          {
            "keyword": "multi-scale analysis",
            "weight": 5,
            "description": "Gabor etc. for texture."
          }
        ],
        "grading_notes": "Require specific computations from patches. Partial if errors but correct steps."
      }
    },
    {
      "id": "pkg12_essay1",
      "prompt": "Discuss 2D transformations in homogeneous coordinates. Write the matrix forms for translation, rotation, similarity (RST), affine, and homography. Specify the minimum point correspondences needed for each. Using the example of a transformed rectangle with given coordinates, explain how to solve for similarity parameters. Extend to noisy affine estimation using least squares. Include advantages of homogeneous representation and applications in vision.",
      "expected_keywords": [
        "homogeneous coordinates",
        "translation",
        "rotation",
        "similarity",
        "affine",
        "homography",
        "point correspondences",
        "least squares",
        "matrix form",
        "degrees of freedom"
      ],
      "rubric": {
        "total_points": 100,
        "criteria": [
          {
            "keyword": "homogeneous coordinates",
            "weight": 10,
            "description": "Explains augmentation to handle projective transforms linearly."
          },
          {
            "keyword": "translation",
            "weight": 8,
            "description": "Matrix and 2 correspondences."
          },
          {
            "keyword": "rotation",
            "weight": 8,
            "description": "Matrix and 2 correspondences."
          },
          {
            "keyword": "similarity",
            "weight": 10,
            "description": "4 DOF, 2 correspondences; solve example."
          },
          {
            "keyword": "affine",
            "weight": 10,
            "description": "6 DOF, 3 correspondences."
          },
          {
            "keyword": "homography",
            "weight": 10,
            "description": "8 DOF, 4 correspondences."
          },
          {
            "keyword": "point correspondences",
            "weight": 10,
            "description": "Minimum for unique solution."
          },
          {
            "keyword": "least squares",
            "weight": 15,
            "description": "Overdetermined system for noisy affine."
          },
          {
            "keyword": "matrix form",
            "weight": 10,
            "description": "Correct 3x3 matrices shown."
          },
          {
            "keyword": "degrees of freedom",
            "weight": 9,
            "description": "Counts for each transform."
          }
        ],
        "grading_notes": "Require math matrices and example calculation. Partial for intuition without full derivation."
      }
    },
    {
      "id": "pkg24_essay1",
      "prompt": "Solve for 3D point from projection matrices using linear system. Compute epipolar line from essential matrix and verify point lies on it. Explain epipolar constraint in stereo for reducing search space.",
      "expected_keywords": [
        "projection matrices",
        "linear system",
        "3D coordinates",
        "essential matrix",
        "epipolar line",
        "verification",
        "constraint usage",
        "stereo search"
      ],
      "rubric": {
        "total_points": 100,
        "criteria": [
          {
            "keyword": "projection matrices",
            "weight": 15,
            "description": "Given C, C'."
          },
          {
            "keyword": "linear system",
            "weight": 20,
            "description": "Homogeneous equations."
          },
          {
            "keyword": "3D coordinates",
            "weight": 15,
            "description": "Solve for X,Y,Z."
          },
          {
            "keyword": "essential matrix",
            "weight": 10,
            "description": "Given E."
          },
          {
            "keyword": "epipolar line",
            "weight": 15,
            "description": "Compute l' = E p."
          },
          {
            "keyword": "verification",
            "weight": 10,
            "description": "p' ^ T l' = 0."
          },
          {
            "keyword": "constraint usage",
            "weight": 10,
            "description": "In stereo."
          },
          {
            "keyword": "stereo search",
            "weight": 5,
            "description": "1D reduction."
          }
        ],
        "grading_notes": "Require math solutions. Partial for calculation errors."
      }
    },
    {
      "id": "pkg23_essay1",
      "prompt": "Explain stereo system with parallel cameras. Derive 3D coordinates from disparity. Compute z for values. Compare disparity search to optical flow matching: similar in finding correspondences, different in stereo geometry vs temporal continuity.",
      "expected_keywords": [
        "stereo system",
        "parallel pinhole",
        "disparity",
        "3D coordinates",
        "depth computation",
        "search similarity",
        "optical flow",
        "differences"
      ],
      "rubric": {
        "total_points": 100,
        "criteria": [
          {
            "keyword": "stereo system",
            "weight": 10,
            "description": "Description."
          },
          {
            "keyword": "parallel pinhole",
            "weight": 10,
            "description": "Setup."
          },
          {
            "keyword": "disparity",
            "weight": 15,
            "description": "xl - xr."
          },
          {
            "keyword": "3D coordinates",
            "weight": 20,
            "description": "Equations."
          },
          {
            "keyword": "depth computation",
            "weight": 10,
            "description": "Example."
          },
          {
            "keyword": "search similarity",
            "weight": 10,
            "description": "To flow."
          },
          {
            "keyword": "optical flow",
            "weight": 10,
            "description": "Comparison."
          },
          {
            "keyword": "differences",
            "weight": 15,
            "description": "Geometry vs time."
          }
        ],
        "grading_notes": "Require equations and calc."
      }
    },
    {
      "id": "pkg22_essay1",
      "prompt": "For texture classification of patches, compute Roberts filter responses, mean features, and classify unknown with Euclidean. Detail LBP procedure, issues with small patches. Discuss filter banks as alternative for texture analysis.",
      "expected_keywords": [
        "texture classification",
        "Roberts filter",
        "mean features",
        "Euclidean distance",
        "LBP procedure",
        "small patch issues",
        "filter banks"
      ],
      "rubric": {
        "total_points": 100,
        "criteria": [
          {
            "keyword": "texture classification",
            "weight": 10,
            "description": "Task overview."
          },
          {
            "keyword": "Roberts filter",
            "weight": 20,
            "description": "Responses calculation."
          },
          {
            "keyword": "mean features",
            "weight": 15,
            "description": "Vectors."
          },
          {
            "keyword": "Euclidean distance",
            "weight": 15,
            "description": "Classification."
          },
          {
            "keyword": "LBP procedure",
            "weight": 20,
            "description": "Steps for texture."
          },
          {
            "keyword": "small patch issues",
            "weight": 10,
            "description": "Reliability."
          },
          {
            "keyword": "filter banks",
            "weight": 10,
            "description": "Alternative."
          }
        ],
        "grading_notes": "Require computations. Partial for errors."
      }
    },
    {
      "id": "pkg25_essay1",
      "prompt": "For deformed rectangle, determine DOF (4) and min correspondences (2) for given nonlinear transform. Derive linear system Ax=b for parameters a,b,c,d from correspondences. Discuss homogeneous coordinates for 2D transforms and applications like image registration.",
      "expected_keywords": [
        "2D deformation",
        "DOF",
        "correspondences",
        "linear system",
        "Ax=b",
        "homogeneous",
        "applications"
      ],
      "rubric": {
        "total_points": 100,
        "criteria": [
          {
            "keyword": "2D deformation",
            "weight": 10,
            "description": "Given function."
          },
          {
            "keyword": "DOF",
            "weight": 15,
            "description": "4 parameters."
          },
          {
            "keyword": "correspondences",
            "weight": 15,
            "description": "Min 2."
          },
          {
            "keyword": "linear system",
            "weight": 20,
            "description": "Setup equations."
          },
          {
            "keyword": "Ax=b",
            "weight": 20,
            "description": "Matrix form."
          },
          {
            "keyword": "homogeneous",
            "weight": 10,
            "description": "For transforms."
          },
          {
            "keyword": "applications",
            "weight": 10,
            "description": "E.g., fitting."
          }
        ],
        "grading_notes": "Require derivation. Partial for missing homogeneous."
      }
    },
    {
      "id": "pkg13_essay1",
      "prompt": "Explain optical flow computation using the brightness constancy assumption. Derive the flow constraint equation involving spatial and temporal derivatives. Using the provided image patches, demonstrate calculation of ∂f/∂x, ∂f/∂y using Prewitt masks and ∂f/∂t at specific points. Discuss limitations like the aperture problem and methods to solve it (e.g., Lucas-Kanade). Include applications in motion segmentation and tracking.",
      "expected_keywords": [
        "optical flow",
        "brightness constancy",
        "constraint equation",
        "spatial derivatives",
        "temporal derivative",
        "Prewitt masks",
        "aperture problem",
        "Lucas-Kanade",
        "neighborhood assumption",
        "motion segmentation"
      ],
      "rubric": {
        "total_points": 100,
        "criteria": [
          {
            "keyword": "optical flow",
            "weight": 10,
            "description": "Defines as apparent motion field."
          },
          {
            "keyword": "brightness constancy",
            "weight": 10,
            "description": "Assumes I(x,y,t) = I(x+u,y+v,t+1)."
          },
          {
            "keyword": "constraint equation",
            "weight": 15,
            "description": "Derives Ix u + Iy v + It = 0."
          },
          {
            "keyword": "spatial derivatives",
            "weight": 15,
            "description": "Computes with Prewitt at given points."
          },
          {
            "keyword": "temporal derivative",
            "weight": 10,
            "description": "Frame difference at points."
          },
          {
            "keyword": "Prewitt masks",
            "weight": 10,
            "description": "Applies correctly to patches."
          },
          {
            "keyword": "aperture problem",
            "weight": 10,
            "description": "Ambiguity in local estimation."
          },
          {
            "keyword": "Lucas-Kanade",
            "weight": 10,
            "description": "Local constancy over window."
          },
          {
            "keyword": "neighborhood assumption",
            "weight": 5,
            "description": "Constant flow in small area."
          },
          {
            "keyword": "motion segmentation",
            "weight": 5,
            "description": "Application example."
          }
        ],
        "grading_notes": "Require accurate derivative calculations from patches. Partial if math errors but correct method."
      }
    },
    {
      "id": "pkg14_essay1",
      "prompt": "Explain stereo vision geometry for parallel cameras. Derive the depth equation z = f b / d from baseline b, focal f, disparity d. Compute z for given values. Compare correspondence search in stereo to optical flow estimation, noting similarities in matching and differences in constraints (epipolar vs. temporal). Discuss challenges like occlusions and methods like semi-global matching.",
      "expected_keywords": [
        "stereo vision",
        "parallel cameras",
        "baseline",
        "focal length",
        "disparity",
        "depth equation",
        "correspondence search",
        "optical flow",
        "epipolar constraint",
        "occlusions"
      ],
      "rubric": {
        "total_points": 100,
        "criteria": [
          {
            "keyword": "stereo vision",
            "weight": 10,
            "description": "Defines binocular depth perception."
          },
          {
            "keyword": "parallel cameras",
            "weight": 10,
            "description": "Assumes aligned optics."
          },
          {
            "keyword": "baseline",
            "weight": 10,
            "description": "Separation b between centers."
          },
          {
            "keyword": "focal length",
            "weight": 10,
            "description": "f in projection."
          },
          {
            "keyword": "disparity",
            "weight": 15,
            "description": "d = xl - xr."
          },
          {
            "keyword": "depth equation",
            "weight": 15,
            "description": "Derives z = f b / d; computes example."
          },
          {
            "keyword": "correspondence search",
            "weight": 10,
            "description": "Finding matches."
          },
          {
            "keyword": "optical flow",
            "weight": 10,
            "description": "Similar matching over time."
          },
          {
            "keyword": "epipolar constraint",
            "weight": 5,
            "description": "Reduces search to lines."
          },
          {
            "keyword": "occlusions",
            "weight": 5,
            "description": "Visibility differences."
          }
        ],
        "grading_notes": "Require derivation and computation. Partial for missing differences."
      }
    },
    {
      "id": "pkg4_essay1",
      "prompt": "Explain the optical flow estimation problem and the two fundamental approaches: Lucas-Kanade (local) and Horn-Schunck (global). Describe the brightness constancy constraint, the aperture problem, and how each method addresses motion ambiguity. Compare their assumptions, advantages, and limitations, particularly in handling occlusions, large motions, and textureless regions. Include a brief discussion on modern extensions (e.g., robust penalties, coarse-to-fine warping).",
      "expected_keywords": [
        "optical flow",
        "brightness constancy",
        "aperture problem",
        "Lucas-Kanade",
        "local method",
        "spatial coherence",
        "weighted least squares",
        "Horn-Schunck",
        "global method",
        "smoothness term",
        "variational formulation",
        "occlusions",
        "large displacement",
        "coarse-to-fine",
        "image pyramid",
        "robust penalty"
      ],
      "rubric": {
        "total_points": 100,
        "criteria": [
          {
            "keyword": "optical flow",
            "weight": 6,
            "description": "Defines optical flow as the apparent 2D motion field induced by 3D scene/camera motion."
          },
          {
            "keyword": "brightness constancy",
            "weight": 8,
            "description": "States \\(I(x,y,t) = I(x+u\\Delta t, y+v\\Delta t, t+\\Delta t)\\) and derives the optical flow constraint equation."
          },
          {
            "keyword": "aperture problem",
            "weight": 8,
            "description": "Explains local ambiguity: only normal flow recoverable from edge-like structures."
          },
          {
            "keyword": "Lucas-Kanade",
            "weight": 8,
            "description": "Describes assumption of constant flow in small window and solution via weighted least squares."
          },
          {
            "keyword": "local method",
            "weight": 5,
            "description": "Notes sparsity (flow computed only at reliable points) and robustness to noise in textured areas."
          },
          {
            "keyword": "spatial coherence",
            "weight": 5,
            "description": "Mentions neighborhood votes to resolve aperture ambiguity."
          },
          {
            "keyword": "weighted least squares",
            "weight": 5,
            "description": "Shows system \\(A^T W A \\mathbf{v} = A^T W \\mathbf{b}\\) for solving local flow."
          },
          {
            "keyword": "Horn-Schunck",
            "weight": 8,
            "description": "Presents energy functional \\(E = \\int (I_x u + I_y v + I_t)^2 + \\alpha (|\\nabla u|^2 + |\\nabla v|^2) \\,dx\\,dy\\)."
          },
          {
            "keyword": "global method",
            "weight": 5,
            "description": "Emphasizes dense flow field and global smoothness propagation."
          },
          {
            "keyword": "smoothness term",
            "weight": 6,
            "description": "Explains role of \\(\\alpha\\) in trading off data fidelity vs. smoothness."
          },
          {
            "keyword": "variational formulation",
            "weight": 5,
            "description": "Mentions Euler-Lagrange equations or iterative solution."
          },
          {
            "keyword": "occlusions",
            "weight": 6,
            "description": "Discusses failure at motion boundaries; LK may skip, HS over-smooths."
          },
          {
            "keyword": "large displacement",
            "weight": 6,
            "description": "Notes linearization fails; both need coarse-to-fine or warping."
          },
          {
            "keyword": "coarse-to-fine",
            "weight": 6,
            "description": "Describes multi-scale pyramid to handle large motions."
          },
          {
            "keyword": "image pyramid",
            "weight": 4,
            "description": "Explains downsampling and incremental flow refinement."
          },
          {
            "keyword": "robust penalty",
            "weight": 4,
            "description": "Mentions modern use of L1 or censored penalties for outlier rejection."
          }
        ],
        "grading_notes": "Require clear distinction between local and global methods. Full credit only if both aperture problem and smoothness regularization are explained. Allow partial credit for intuitive descriptions without full math. Deduct if methods are confused or assumptions misstated."
      }
    },
    {
      "id": "pkg31_essay1",
      "prompt": "(a) Using homogeneous coordinates, write the matrix form of the following 2D transformations: pure translation, pure rotation, similarity (translation + rotation + scale), affine, and homography. How many degrees of freedom does each transformation have? How many 2D point correspondences are needed to estimate each?",
      "expected_keywords": [
        "homogeneous coordinates",
        "translation matrix",
        "rotation matrix",
        "similarity transformation",
        "affine transformation",
        "homography",
        "degrees of freedom",
        "point correspondences",
        "linear equations"
      ],
      "rubric": {
        "total_points": 100,
        "criteria": [
          {
            "keyword": "matrix forms",
            "weight": 40,
            "description": "Correctly writes each transformation in homogeneous matrix form (3x3 for 2D transformations)."
          },
          {
            "keyword": "degrees of freedom",
            "weight": 30,
            "description": "Identifies degrees of freedom for each transformation (translation=2, rotation=1, similarity=4, affine=6, homography=8)."
          },
          {
            "keyword": "point correspondences",
            "weight": 20,
            "description": "States number of point correspondences required to estimate each transformation (translation=1, rotation=2, similarity=2, affine=3, homography=4)."
          },
          {
            "keyword": "clarity",
            "weight": 10,
            "description": "Neat matrix notation and correct use of homogeneous coordinates."
          }
        ],
        "grading_notes": "Full credit for correct matrix representation and corresponding degrees of freedom. Partial credit for correct forms with minor omissions."
      }
    },
    {
      "id": "pkg31_essay2",
      "prompt": "(b) A rectangle with corners A = (−1,1), B = (1,1), C = (1,−1), and D = (−1,−1) undergoes a transformation and the corners are observed at A′ = (1,3), B′ = (3,3), C′ = (−2,1), and D′ = (−6,1). The affine transformation does not perfectly explain the observations, but we assume the transformation is affine and noisy. Estimate the optimal affine transformation using the least squares method.",
      "expected_keywords": [
        "affine transformation",
        "least squares estimation",
        "homogeneous coordinates",
        "overdetermined system",
        "Ax=b",
        "pseudoinverse",
        "normal equations",
        "noise",
        "parameter estimation"
      ],
      "rubric": {
        "total_points": 100,
        "criteria": [
          {
            "keyword": "affine model setup",
            "weight": 40,
            "description": "Forms the affine transformation equations in homogeneous coordinates and constructs the system Ax=b."
          },
          {
            "keyword": "least squares solution",
            "weight": 40,
            "description": "Uses least squares or pseudoinverse to estimate transformation parameters minimizing reprojection error."
          },
          {
            "keyword": "interpretation",
            "weight": 20,
            "description": "Explains the impact of noise and justifies the use of least squares as optimal for Gaussian noise."
          }
        ],
        "grading_notes": "Partial credit for correct system setup without derivation. Full credit requires explicit least-squares formulation or solution steps."
      }
    },
    {
      "id": "pkg31_essay3",
      "prompt": "(c) An affine transformation is the most flexible transformation that is linear in both homogeneous and inhomogeneous coordinates. When represented by a matrix (in homogeneous coordinates), how many elements does the matrix have? How many degrees of freedom does an affine transformation have? How many 2D point matches are necessary to estimate it?",
      "expected_keywords": [
        "affine matrix",
        "3x3 matrix",
        "six degrees of freedom",
        "2D point correspondences",
        "minimum points",
        "linear independence"
      ],
      "rubric": {
        "total_points": 100,
        "criteria": [
          {
            "keyword": "matrix elements",
            "weight": 40,
            "description": "States that the affine matrix has 9 elements in homogeneous form, but one scale factor is redundant."
          },
          {
            "keyword": "degrees of freedom",
            "weight": 30,
            "description": "Correctly identifies 6 degrees of freedom for 2D affine transformations."
          },
          {
            "keyword": "point matches",
            "weight": 30,
            "description": "Explains that 3 non-collinear 2D point correspondences are needed to estimate the affine transformation."
          }
        ],
        "grading_notes": "Partial credit for missing one of the three quantitative answers (elements, DoF, or points). Full credit for correct reasoning with supporting explanation."
      }
    },
    {
      "id": "pkg30_essay1",
      "prompt": "Triangulation using 1D cameras. The projection function for a 1D camera is m ∝ P x, where m = [m, 1]^T (pixel in homogeneous coords), x = [x, y, 1]^T (2D world point in homogeneous coords) and P is a 2×3 projection matrix. (a) Given two cameras P1 and P2 and their measurements m1 and m2 of an unknown point x, derive the constraints on x in the form A x = b (A is 2×2, b is 2×1). Express A and b in terms of P1, P2 and the measurements. (b) Given P1 = [[1,2,0],[2,1,0]] and P2 = [[1,2,3],[4,2,0]] and measurements m1 = 1.25 and m2 = 1, triangulate the point x (solve for x and y). (c) A point is often observed in many images: is there an advantage to estimating the point’s position by considering all images simultaneously? Justify. (d) Many points are observed in an image: is there an advantage to estimating all points jointly instead of independently? Justify.",
      "expected_keywords": [
        "triangulation",
        "homogeneous coordinates",
        "projection matrix",
        "linear constraint",
        "Ax=b",
        "overdetermined system",
        "least squares",
        "bundle adjustment",
        "multi-view",
        "robust estimation",
        "joint estimation",
        "covariance",
        "optimal estimation"
      ],
      "rubric": {
        "total_points": 100,
        "criteria": [
          {
            "keyword": "derivation Ax=b",
            "weight": 40,
            "description": "Correct derivation of A and b from the rows of P1,P2 and measured m1,m2, producing a 2×2 linear system for (x,y)."
          },
          {
            "keyword": "numerical triangulation",
            "weight": 30,
            "description": "Correct solution of the provided numeric example (substituting P1,P2,m1,m2 into derived system and solving for x,y)."
          },
          {
            "keyword": "multi-view advantage",
            "weight": 15,
            "description": "Explains benefits of using many images simultaneously (reduces noise, overdetermined -> least-squares, improved accuracy, outlier handling)."
          },
          {
            "keyword": "joint estimation of many points",
            "weight": 15,
            "description": "Explains advantages of joint estimation (bundle adjustment, enforces consistency, uses image/pose covariances) and notes computational trade-offs."
          }
        ],
        "grading_notes": "Partial credit for correct setup with minor algebraic errors. For (b) accept exact numeric solution or equivalent least-squares result. Award extra credit for mentioning covariance, weighting, or robust methods (e.g., RANSAC) when discussing multi-view or joint estimation."
      }
    },
    {
      "id": "pkg29_essay1",
      "prompt": "Describe an algorithm to segment the seashells from the image. How can morphological operations help to improve segmentation?",
      "expected_keywords": [
        "image segmentation",
        "thresholding",
        "background subtraction",
        "binary mask",
        "morphological operations",
        "erosion",
        "dilation",
        "opening",
        "closing",
        "connected components",
        "noise removal"
      ],
      "rubric": {
        "total_points": 100,
        "criteria": [
          {
            "keyword": "segmentation algorithm",
            "weight": 40,
            "description": "Proposes an algorithm for segmenting non-overlapping shells on a contrasting background using thresholding or color separation."
          },
          {
            "keyword": "morphological operations",
            "weight": 40,
            "description": "Explains how morphological filtering (erosion, dilation, opening, closing) improves segmentation quality and removes noise."
          },
          {
            "keyword": "clarity and application",
            "weight": 20,
            "description": "Provides a clear, step-by-step explanation with reasoning for each operation."
          }
        ],
        "grading_notes": "Partial credit for mentioning morphological operations without explaining their effect. Full credit for integrating them effectively into the segmentation pipeline."
      }
    },
    {
      "id": "pkg29_essay2",
      "prompt": "It has been decided that only texture will be used to classify the seashells. You may use either filter banks, Local Binary Patterns (LBP), or co-occurrence matrices to extract texture properties. Given a segmented image, describe an algorithm to classify a seashell. Where do the concepts of feature vector, distance function, and classifier fit in your algorithm?",
      "expected_keywords": [
        "texture features",
        "filter banks",
        "Gabor filters",
        "LBP",
        "gray-level co-occurrence matrix",
        "feature vector",
        "distance function",
        "Euclidean distance",
        "classifier",
        "training",
        "k-nearest neighbour",
        "support vector machine"
      ],
      "rubric": {
        "total_points": 100,
        "criteria": [
          {
            "keyword": "texture extraction",
            "weight": 30,
            "description": "Describes texture feature extraction using one of the suggested methods and explains its relevance."
          },
          {
            "keyword": "feature vector and distance",
            "weight": 35,
            "description": "Explains how texture features are formed into a feature vector and compared using a distance metric."
          },
          {
            "keyword": "classifier integration",
            "weight": 25,
            "description": "Describes how a classifier (e.g., k-NN or SVM) uses these feature vectors for training and prediction."
          },
          {
            "keyword": "clarity of process",
            "weight": 10,
            "description": "Logical structure and clear algorithmic flow from input to classification output."
          }
        ],
        "grading_notes": "Full credit requires integration of all three concepts—feature extraction, distance measurement, and classification—in a coherent algorithmic pipeline."
      }
    },
    {
      "id": "pkg29_essay3",
      "prompt": "The system should be extended to include shape features. What shape features could be useful for classifying the shells? How should one combine them with the texture features of the previous step? How should the algorithm be adapted to classify using both texture and shape?",
      "expected_keywords": [
        "shape features",
        "contour descriptors",
        "moments",
        "Hu moments",
        "aspect ratio",
        "Fourier descriptors",
        "texture-shape fusion",
        "feature-level fusion",
        "normalization",
        "weighted combination",
        "classifier adaptation"
      ],
      "rubric": {
        "total_points": 100,
        "criteria": [
          {
            "keyword": "shape descriptors",
            "weight": 35,
            "description": "Identifies relevant shape features and explains how they capture geometric differences between shells."
          },
          {
            "keyword": "feature fusion",
            "weight": 35,
            "description": "Describes how to combine texture and shape features via concatenation, normalization, or weighting."
          },
          {
            "keyword": "algorithm adaptation",
            "weight": 20,
            "description": "Proposes classifier or feature scaling modifications to integrate multimodal data."
          },
          {
            "keyword": "explanation clarity",
            "weight": 10,
            "description": "Provides a well-organized, conceptually sound discussion of combining multiple feature types."
          }
        ],
        "grading_notes": "Award partial credit for identifying correct shape features without discussing integration. Full credit for coherent fusion strategy and algorithmic justification."
      }
    },
    {
      "id": "pkg28_essay1",
      "prompt": "Briefly explain the following terms: (a) Optical flow (b) SIFT (c) Histogram (d) K-nearest neighbour classification (e) HSV color space.",
      "expected_keywords": [
        "optical flow",
        "motion estimation",
        "SIFT",
        "keypoints",
        "histogram",
        "feature distribution",
        "k-nearest neighbour",
        "classification",
        "HSV color space",
        "hue",
        "saturation",
        "value"
      ],
      "rubric": {
        "total_points": 100,
        "criteria": [
          {
            "keyword": "optical flow",
            "weight": 20,
            "description": "Defines optical flow as apparent pixel motion between frames and explains its role in motion estimation."
          },
          {
            "keyword": "SIFT",
            "weight": 20,
            "description": "Explains Scale-Invariant Feature Transform for keypoint detection and descriptor matching."
          },
          {
            "keyword": "histogram",
            "weight": 20,
            "description": "Describes histograms as statistical distributions of intensity or color values in an image."
          },
          {
            "keyword": "k-nearest neighbour",
            "weight": 20,
            "description": "Describes KNN as a distance-based classifier using feature similarity."
          },
          {
            "keyword": "HSV color space",
            "weight": 20,
            "description": "Explains the components of hue, saturation, and value and their perceptual significance."
          }
        ],
        "grading_notes": "Award partial credit for accurate but incomplete definitions. Full credit requires both definition and context of use in computer vision."
      }
    },
    {
      "id": "pkg28_essay2",
      "prompt": "Describe the main principles of the following and give one example of their usage: (a) Hough transform (b) RANSAC (c) Mahalanobis distance.",
      "expected_keywords": [
        "Hough transform",
        "parametric space",
        "line detection",
        "RANSAC",
        "robust estimation",
        "outliers",
        "Mahalanobis distance",
        "covariance",
        "multivariate distance",
        "feature matching"
      ],
      "rubric": {
        "total_points": 100,
        "criteria": [
          {
            "keyword": "Hough transform",
            "weight": 30,
            "description": "Describes parameter space voting for detecting geometric shapes such as lines or circles."
          },
          {
            "keyword": "RANSAC",
            "weight": 35,
            "description": "Explains Random Sample Consensus as a method for model estimation in the presence of outliers."
          },
          {
            "keyword": "Mahalanobis distance",
            "weight": 25,
            "description": "Defines Mahalanobis distance as a covariance-scaled metric for multivariate data comparison."
          },
          {
            "keyword": "examples and clarity",
            "weight": 10,
            "description": "Provides clear examples demonstrating real-world usage of each concept."
          }
        ],
        "grading_notes": "Full credit requires a clear example for each method. Partial credit for correct principles without examples."
      }
    },
    {
      "id": "pkg6_essay1",
      "prompt": "Explain the Harris corner detector in detail. Start with the motivation for detecting local features in image matching. Derive the structure tensor \\(M\\) from the image gradients and the autocorrelation approximation. Describe how the corner response \\(R = \\det(M) - k \\trace(M)^2\\) classifies image regions into flat, edge, and corner using eigenvalue analysis. Discuss practical implementation steps (Gaussian smoothing, non-maximum suppression, thresholding) and limitations (scale variance, sensitivity to noise). Conclude with how Harris features enable robust correspondence in stereo, motion, or recognition pipelines.",
      "expected_keywords": [
        "local features",
        "image matching",
        "correspondence",
        "Harris corner",
        "structure tensor",
        "image gradients",
        "I_x",
        "I_y",
        "autocorrelation",
        "corner response",
        "det(M)",
        "trace(M)",
        "eigenvalues",
        "flat region",
        "edge",
        "corner",
        "Gaussian window",
        "non-maximum suppression",
        "thresholding",
        "scale invariance",
        "noise sensitivity",
        "sparse features",
        "robust matching"
      ],
      "rubric": {
        "total_points": 100,
        "criteria": [
          {
            "keyword": "local features",
            "weight": 5,
            "description": "Defines need for repeatable, localizable points for matching."
          },
          {
            "keyword": "image matching",
            "weight": 5,
            "description": "Links to finding correspondences across views."
          },
          {
            "keyword": "correspondence",
            "weight": 5,
            "description": "Mentions applications in stereo, motion, recognition."
          },
          {
            "keyword": "Harris corner",
            "weight": 5,
            "description": "Names the detector and its goal: find L- or X-junctions."
          },
          {
            "keyword": "structure tensor",
            "weight": 8,
            "description": "Shows \\(M = \\begin{bmatrix} \\langle I_x^2 \\rangle & \\langle I_x I_y \\rangle \\\\ \\langle I_x I_y \\rangle & \\langle I_y^2 \\rangle \\end{bmatrix}\\)."
          },
          {
            "keyword": "image gradients",
            "weight": 6,
            "description": "Explains computation of \\(I_x, I_y\\) via finite differences."
          },
          {
            "keyword": "I_x",
            "weight": 3,
            "description": "Used in gradient products."
          },
          {
            "keyword": "I_y",
            "weight": 3,
            "description": "Used in gradient products."
          },
          {
            "keyword": "autocorrelation",
            "weight": 7,
            "description": "Connects to Taylor expansion and shift-induced intensity change."
          },
          {
            "keyword": "corner response",
            "weight": 7,
            "description": "States \\(R = \\det(M) - k \\trace(M)^2\\)."
          },
          {
            "keyword": "det(M)",
            "weight": 5,
            "description": "Interprets as product of eigenvalues."
          },
          {
            "keyword": "trace(M)",
            "weight": 5,
            "description": "Interprets as sum of eigenvalues."
          },
          {
            "keyword": "eigenvalues",
            "weight": 8,
            "description": "Classifies: both small → flat; one large → edge; both large → corner."
          },
          {
            "keyword": "flat region",
            "weight": 4,
            "description": "R ≈ 0 (small negative or positive)."
          },
          {
            "keyword": "edge",
            "weight": 4,
            "description": "R large negative."
          },
          {
            "keyword": "corner",
            "weight": 4,
            "description": "R large positive."
          },
          {
            "keyword": "Gaussian window",
            "weight": 5,
            "description": "Averages gradients to reduce noise."
          },
          {
            "keyword": "non-maximum suppression",
            "weight": 5,
            "description": "Ensures one response per corner."
          },
          {
            "keyword": "thresholding",
            "weight": 4,
            "description": "Filters weak responses."
          },
          {
            "keyword": "scale invariance",
            "weight": 5,
            "description": "Notes Harris is not scale-invariant; needs multi-scale."
          },
          {
            "keyword": "noise sensitivity",
            "weight": 5,
            "description": "Discusses impact of derivative noise."
          },
          {
            "keyword": "sparse features",
            "weight": 4,
            "description": "Contrasts with dense methods."
          },
          {
            "keyword": "robust matching",
            "weight": 5,
            "description": "Enables RANSAC, triangulation, etc."
          }
        ],
        "grading_notes": "Require eigenvalue interpretation and response formula for full credit. Allow partial math if intuition is strong. Deduct if autocorrelation or gradient averaging is missing. Award bonus for mentioning later improvements (e.g., SIFT)."
      }
    },
    {
      "id": "pkg11_essay1",
      "prompt": "Provide an overview of the entire computer vision course as outlined in the introductory lecture. List all 11 modules in order with their exact titles and briefly describe the role of each in building a complete vision system from raw pixels to semantic understanding. Explain how the course progresses from low-level image formation (imaging, light, color) through mid-level feature extraction and geometric reasoning (binary analysis, texture, local features, 2D/3D models, motion, depth cues) to high-level recognition and 3D interpretation. Discuss the pedagogical motivation for this bottom-up structure and how it mirrors both classical and modern vision pipelines. Finally, reflect on the challenges highlighted in the introduction (ambiguity, illumination, occlusion, scale) and how subsequent modules address them.",
      "expected_keywords": [
        "course outline",
        "module sequence",
        "1.1 Introduction",
        "2.1 Imaging – Pinhole model",
        "3.1 Light and color",
        "4.1 Binary image analysis",
        "5.1 Texture analysis",
        "6.1 Local features – Harris",
        "7.1 Recognition – Basics",
        "8.1 Motion – Basics",
        "9.1 2D models – Fitting",
        "10.1 Depth from images – 3D cues",
        "11.1 3D – Affine transformation",
        "low-level processing",
        "mid-level features",
        "high-level recognition",
        "bottom-up pipeline",
        "inverse problem",
        "ambiguity",
        "illumination variation",
        "occlusion",
        "scale changes",
        "photons to semantics"
      ],
      "rubric": {
        "total_points": 100,
        "criteria": [
          {
            "keyword": "course outline",
            "weight": 6,
            "description": "Accurately lists all 11 modules with correct numbering and titles."
          },
          {
            "keyword": "module sequence",
            "weight": 5,
            "description": "Presents modules in exact chronological order."
          },
          {
            "keyword": "1.1 Introduction",
            "weight": 3,
            "description": "Sets context and motivation."
          },
          {
            "keyword": "2.1 Imaging – Pinhole model",
            "weight": 4,
            "description": "Image formation and geometry."
          },
          {
            "keyword": "3.1 Light and color",
            "weight": 4,
            "description": "Physics of light-surface interaction."
          },
          {
            "keyword": "4.1 Binary image analysis",
            "weight": 4,
            "description": "Segmentation and morphology."
          },
          {
            "keyword": "5.1 Texture analysis",
            "weight": 4,
            "description": "Surface properties and shape cues."
          },
          {
            "keyword": "6.1 Local features – Harris",
            "weight": 4,
            "description": "Correspondence and matching."
          },
          {
            "keyword": "7.1 Recognition – Basics",
            "weight": 4,
            "description": "Classification and learning."
          },
          {
            "keyword": "8.1 Motion – Basics",
            "weight": 4,
            "description": "Optical flow and tracking."
          },
          {
            "keyword": "9.1 2D models – Fitting",
            "weight": 4,
            "description": "Lines, conics, RANSAC."
          },
          {
            "keyword": "10.1 Depth from images – 3D cues",
            "weight": 4,
            "description": "Monocular depth estimation."
          },
          {
            "keyword": "11.1 3D – Affine transformation",
            "weight": 4,
            "description": "3D reasoning and reconstruction."
          },
          {
            "keyword": "low-level processing",
            "weight": 5,
            "description": "Modules 2–5: pixels to regions."
          },
          {
            "keyword": "mid-level features",
            "weight": 5,
            "description": "Modules 6–10: structure and relations."
          },
          {
            "keyword": "high-level recognition",
            "weight": 5,
            "description": "Module 7 and beyond: semantics."
          },
          {
            "keyword": "bottom-up pipeline",
            "weight": 6,
            "description": "Mimics classical vision hierarchy."
          },
          {
            "keyword": "inverse problem",
            "weight": 5,
            "description": "Recovering 3D from 2D is ill-posed."
          },
          {
            "keyword": "ambiguity",
            "weight": 4,
            "description": "Multiple 3D scenes → same image."
          },
          {
            "keyword": "illumination variation",
            "weight": 4,
            "description": "Handled in color and recognition."
          },
          {
            "keyword": "occlusion",
            "weight": 4,
            "description": "Addressed in motion and 2D/3D fitting."
          },
          {
            "keyword": "scale changes",
            "weight": 4,
            "description": "Local features and multi-scale analysis."
          },
          {
            "keyword": "photons to semantics",
            "weight": 5,
            "description": "Overall course mantra."
          }
        ],
        "grading_notes": "Require exact module titles and order for full credit. Allow flexible grouping into low/mid/high levels. Deduct if any module is missing or misordered. Award bonus for connecting specific challenges to later modules (e.g., occlusion → motion segmentation)."
      }
    },
    {
      "id": "pkg5_essay1",
      "prompt": "Define pattern recognition and explain its role in computer vision using the handwritten character recognition example from the lecture. Discuss the key challenges: intra-class variability, inter-class similarity, and the need for learning from data. Describe a typical machine learning-based recognition pipeline (preprocessing → feature extraction → classifier training → inference). Contrast this with rule-based or template-matching approaches, highlighting why learning is essential in practice. Finally, connect recognition to prior topics in the course (e.g., 2D model fitting, motion, 3D cues) and explain how robust low-level processing enables high-level recognition.",
      "expected_keywords": [
        "pattern recognition",
        "label assignment",
        "classification",
        "regression",
        "handwritten digits",
        "intra-class variability",
        "inter-class similarity",
        "learning from data",
        "training set",
        "feature extraction",
        "classifier",
        "template matching",
        "rule-based",
        "machine learning",
        "preprocessing",
        "inference",
        "2D model fitting",
        "occlusion",
        "motion segmentation",
        "3D cues"
      ],
      "rubric": {
        "total_points": 100,
        "criteria": [
          {
            "keyword": "pattern recognition",
            "weight": 6,
            "description": "Clearly defines as assigning a label to an input pattern."
          },
          {
            "keyword": "label assignment",
            "weight": 5,
            "description": "Emphasizes decision-making from image to semantic category."
          },
          {
            "keyword": "classification",
            "weight": 5,
            "description": "Identifies discrete output (e.g., digit 0–9)."
          },
          {
            "keyword": "regression",
            "weight": 4,
            "description": "Mentions continuous output as alternative (e.g., bounding box coordinates)."
          },
          {
            "keyword": "handwritten digits",
            "weight": 5,
            "description": "Uses lecture example to illustrate real-world variability."
          },
          {
            "keyword": "intra-class variability",
            "weight": 7,
            "description": "Explains same class (e.g., '1') can look very different."
          },
          {
            "keyword": "inter-class similarity",
            "weight": 6,
            "description": "Notes different classes (e.g., '1' vs '7') can be confusable."
          },
          {
            "keyword": "learning from data",
            "weight": 8,
            "description": "States that rules are hard to write; models must be learned."
          },
          {
            "keyword": "training set",
            "weight": 5,
            "description": "Mentions labeled examples needed for supervised learning."
          },
          {
            "keyword": "feature extraction",
            "weight": 7,
            "description": "Describes transforming raw pixels into discriminative representation."
          },
          {
            "keyword": "classifier",
            "weight": 6,
            "description": "Names examples: k-NN, SVM, neural networks."
          },
          {
            "keyword": "template matching",
            "weight": 6,
            "description": "Critiques as brittle to variation in pose, style, illumination."
          },
          {
            "keyword": "rule-based",
            "weight": 5,
            "description": "Notes difficulty in hand-crafting rules for complex patterns."
          },
          {
            "keyword": "machine learning",
            "weight": 6,
            "description": "Contrasts with traditional AI; data-driven generalization."
          },
          {
            "keyword": "preprocessing",
            "weight": 5,
            "description": "Includes normalization, binarization, noise removal."
          },
          {
            "keyword": "inference",
            "weight": 5,
            "description": "Describes applying trained model to new input."
          },
          {
            "keyword": "2D model fitting",
            "weight": 5,
            "description": "Links to locating object parts despite occlusion/clutter."
          },
          {
            "keyword": "occlusion",
            "weight": 4,
            "description": "Notes recognition must handle partial visibility."
          },
          {
            "keyword": "motion segmentation",
            "weight": 4,
            "description": "Connects to isolating moving objects before recognition."
          },
          {
            "keyword": "3D cues",
            "weight": 4,
            "description": "Mentions depth, perspective aid in disambiguating 2D projections."
          }
        ],
        "grading_notes": "Require pipeline structure and contrast with non-learning methods. Full credit only if challenges (variability) and need for learning are well-explained. Allow flexibility in classifier choice. Deduct if recognition is confused with detection or reconstruction."
      }
    },
    {
      "id": "pkg3_essay1",
      "prompt": "Discuss the key challenges in 2D model fitting as illustrated in the lecture. Categorize the problems into noise, extraneous data (clutter/outliers), and missing data (occlusions). For each category, provide a real-world example from the slides, explain why it violates ideal assumptions, and suggest a computational strategy (e.g., RANSAC, part-based models, robust estimators) to address it. Conclude with a discussion on how increasing model complexity (from lines to articulated cars) impacts both robustness and computational cost.",
      "expected_keywords": [
        "model fitting",
        "parametric model",
        "geometric primitives",
        "noise",
        "extraneous data",
        "clutter",
        "outliers",
        "missing data",
        "occlusion",
        "line detection",
        "circle fitting",
        "car model",
        "RANSAC",
        "robust estimation",
        "part-based model",
        "hypothesis generation",
        "verification",
        "computational cost",
        "articulation"
      ],
      "rubric": {
        "total_points": 100,
        "criteria": [
          {
            "keyword": "model fitting",
            "weight": 5,
            "description": "Clearly defines model fitting as estimating parameters of a geometric model from image features."
          },
          {
            "keyword": "parametric model",
            "weight": 5,
            "description": "Mentions use of mathematical models with parameters (size, position, orientation)."
          },
          {
            "keyword": "geometric primitives",
            "weight": 5,
            "description": "Lists at least two primitives (e.g., line, circle, ellipse)."
          },
          {
            "keyword": "noise",
            "weight": 8,
            "description": "Defines as small random errors in feature locations; links to robust estimators."
          },
          {
            "keyword": "extraneous data",
            "weight": 8,
            "description": "Defines as irrelevant features; example: multiple coins when fitting one circle."
          },
          {
            "keyword": "clutter",
            "weight": 5,
            "description": "Synonym for extraneous data; used in context of background distractors."
          },
          {
            "keyword": "outliers",
            "weight": 5,
            "description": "Refers to data not belonging to the target model instance."
          },
          {
            "keyword": "missing data",
            "weight": 8,
            "description": "Defines as parts of the object not visible; example: occluded car wheel."
          },
          {
            "keyword": "occlusion",
            "weight": 5,
            "description": "Specific cause of missing data in real scenes."
          },
          {
            "keyword": "line detection",
            "weight": 5,
            "description": "Uses house facade example to illustrate multiple lines and occlusion."
          },
          {
            "keyword": "circle fitting",
            "weight": 5,
            "description": "Uses coin example to show clutter/outliers."
          },
          {
            "keyword": "car model",
            "weight": 5,
            "description": "Discusses complexity of articulated, multi-part model."
          },
          {
            "keyword": "RANSAC",
            "weight": 8,
            "description": "Explains hypothesis-and-verify paradigm for handling outliers."
          },
          {
            "keyword": "robust estimation",
            "weight": 6,
            "description": "Mentions M-estimators or least median of squares as alternatives."
          },
          {
            "keyword": "part-based model",
            "weight": 6,
            "description": "Suggests modeling car as assembly of ellipses to handle occlusion."
          },
          {
            "keyword": "hypothesis generation",
            "weight": 4,
            "description": "Briefly notes minimal subsets for parameter estimation."
          },
          {
            "keyword": "verification",
            "weight": 4,
            "description": "Mentions consensus set size or residual error."
          },
          {
            "keyword": "computational cost",
            "weight": 5,
            "description": "Discusses trade-off: more complex models need more hypotheses."
          },
          {
            "keyword": "articulation",
            "weight": 3,
            "description": "Acknowledges deformable or pose-varying components in complex models."
          }
        ],
        "grading_notes": "Require at least one example per challenge category. Full credit for suggesting appropriate algorithm (e.g., RANSAC for outliers). Deduct if strategies are generic or not linked to specific problem. Partial credit for good intuition without naming algorithms."
      }
    },
    {
      "id": "pkg1_essay1",
      "prompt": "Explain the Perspective-3-Point (P3P) problem in the context of absolute pose estimation for a calibrated camera. Describe the geometric formulation, the role of the three point correspondences, and how the problem can be converted into estimating the 3D locations of the points. Discuss the number of possible solutions and practical considerations for disambiguation.",
      "expected_keywords": [
        "P3P",
        "absolute pose",
        "calibrated camera",
        "three point correspondences",
        "unit vectors",
        "distances d_ij",
        "triangulation",
        "up to four solutions",
        "fourth point",
        "RANSAC"
      ],
      "rubric": {
        "total_points": 100,
        "criteria": [
          {
            "keyword": "P3P",
            "weight": 10,
            "description": "Clearly states that P3P solves for camera pose using three 3D-to-2D correspondences."
          },
          {
            "keyword": "absolute pose",
            "weight": 10,
            "description": "Defines absolute pose as estimating rotation R and translation t relative to world frame."
          },
          {
            "keyword": "calibrated camera",
            "weight": 10,
            "description": "Mentions that intrinsic parameters K are known, enabling normalized coordinates."
          },
          {
            "keyword": "three point correspondences",
            "weight": 10,
            "description": "Describes 3D points P_i and their 2D projections Q_i in the image."
          },
          {
            "keyword": "unit vectors",
            "weight": 10,
            "description": "Explains back-projection rays as unit vectors q_i from camera center."
          },
          {
            "keyword": "distances d_ij",
            "weight": 10,
            "description": "Shows computation of distances d_ij between 3D points using cosine law on known 2D angles."
          },
          {
            "keyword": "triangulation",
            "weight": 15,
            "description": "Details conversion to solving for 3D point depths along rays (up to four solutions)."
          },
          {
            "keyword": "up to four solutions",
            "weight": 10,
            "description": "Discusses the algebraic result yielding up to four valid pose configurations."
          },
          {
            "keyword": "fourth point",
            "weight": 10,
            "description": "Explains disambiguation using an additional correspondence or cheirality check."
          },
          {
            "keyword": "RANSAC",
            "weight": 5,
            "description": "Mentions robust estimation in presence of outliers using RANSAC with P3P."
          }
        ],
        "grading_notes": "Award partial credit for correct geometric intuition even if algebraic details are incomplete. Deduct points for confusing absolute vs. relative pose or calibrated vs. uncalibrated settings."
      }
    },
    {
      "id": "pkg16_essay1",
      "prompt": "Describe stereo geometry for parallel pinhole cameras. Derive 3D coordinates from disparity, including depth z = f b / d. Compute for given values. Compare correspondence in stereo to optical flow, similarities in feature matching, differences in constraints and applications.",
      "expected_keywords": [
        "stereo geometry",
        "pinhole cameras",
        "baseline b",
        "focal f",
        "disparity d",
        "3D reconstruction",
        "correspondence",
        "optical flow",
        "similarities",
        "differences"
      ],
      "rubric": {
        "total_points": 100,
        "criteria": [
          {
            "keyword": "stereo geometry",
            "weight": 10,
            "description": "Parallel setup."
          },
          {
            "keyword": "pinhole cameras",
            "weight": 10,
            "description": "Projection model."
          },
          {
            "keyword": "baseline b",
            "weight": 10,
            "description": "Separation."
          },
          {
            "keyword": "focal f",
            "weight": 10,
            "description": "Length."
          },
          {
            "keyword": "disparity d",
            "weight": 15,
            "description": "xl - xr."
          },
          {
            "keyword": "3D reconstruction",
            "weight": 15,
            "description": "Equations for x,y,z."
          },
          {
            "keyword": "correspondence",
            "weight": 10,
            "description": "Matching points."
          },
          {
            "keyword": "optical flow",
            "weight": 10,
            "description": "Temporal matching."
          },
          {
            "keyword": "similarities",
            "weight": 5,
            "description": "Search for matches."
          },
          {
            "keyword": "differences",
            "weight": 5,
            "description": "Geometry vs. dynamics."
          }
        ],
        "grading_notes": "Require derivation and example. Partial for missing comparisons."
      }
    },
    {
      "id": "pkg20_essay1",
      "prompt": "Classify texture patches using Roberts gradients and Euclidean, computing features and distance. Describe LBP procedure for classification, noting small patch issues. Compare to filter banks.",
      "expected_keywords": [
        "texture patches",
        "Roberts gradients",
        "feature means",
        "Euclidean classification",
        "LBP procedure",
        "histogram features",
        "patch size problem",
        "filter banks",
        "comparison"
      ],
      "rubric": {
        "total_points": 100,
        "criteria": [
          {
            "keyword": "texture patches",
            "weight": 10,
            "description": "Given classes."
          },
          {
            "keyword": "Roberts gradients",
            "weight": 15,
            "description": "Compute responses."
          },
          {
            "keyword": "feature means",
            "weight": 15,
            "description": "Vector creation."
          },
          {
            "keyword": "Euclidean classification",
            "weight": 10,
            "description": "Nearest class."
          },
          {
            "keyword": "LBP procedure",
            "weight": 15,
            "description": "Code, histogram."
          },
          {
            "keyword": "histogram features",
            "weight": 10,
            "description": "For comparison."
          },
          {
            "keyword": "patch size problem",
            "weight": 10,
            "description": "Limited stats."
          },
          {
            "keyword": "filter banks",
            "weight": 10,
            "description": "Alternative method."
          },
          {
            "keyword": "comparison",
            "weight": 5,
            "description": "To LBP/gradients."
          }
        ],
        "grading_notes": "Similar to pkg15."
      }
    },
    {
      "id": "pkg18_essay1",
      "prompt": "Explain LBP for texture recognition, including basic operator and uniform/rotation-invariant extensions. Describe texture analysis (filter banks for energy features) and synthesis (e.g., Markov models). For 2D transformations, count DOF and min correspondences. Solve for parameters in given nonlinear example using linear system Ax=b.",
      "expected_keywords": [
        "LBP",
        "binary operator",
        "uniform patterns",
        "texture recognition",
        "filter banks",
        "energy features",
        "texture synthesis",
        "2D transformations",
        "DOF",
        "linear system"
      ],
      "rubric": {
        "total_points": 100,
        "criteria": [
          {
            "keyword": "LBP",
            "weight": 15,
            "description": "Neighborhood thresholding."
          },
          {
            "keyword": "binary operator",
            "weight": 10,
            "description": "Code generation."
          },
          {
            "keyword": "uniform patterns",
            "weight": 10,
            "description": "Reduced labels."
          },
          {
            "keyword": "texture recognition",
            "weight": 10,
            "description": "Histogram comparison."
          },
          {
            "keyword": "filter banks",
            "weight": 10,
            "description": "Gabor etc. for analysis."
          },
          {
            "keyword": "energy features",
            "weight": 10,
            "description": "Response stats."
          },
          {
            "keyword": "texture synthesis",
            "weight": 10,
            "description": "Generating new textures."
          },
          {
            "keyword": "2D transformations",
            "weight": 10,
            "description": "DOF and correspondences."
          },
          {
            "keyword": "linear system",
            "weight": 10,
            "description": "Ax=b setup."
          }
        ],
        "grading_notes": "Require LBP details and transformation equations. Partial for missing synthesis."
      }
    },
    {
      "id": "pkg26_essay1",
      "prompt": "A rectangle is deformed as observed in the figure. It is known that the coordinates of the points deform through a function of the form: x' = ax + by, y' = cx² + dy. (a) How many degrees of freedom does the transformation have? How many point correspondences are needed to estimate the parameters? (b) Derive the equations to estimate the transformation parameters from a set of point correspondences. Hint: the transformation can be estimated linearly through a system of the form Ax = b.",
      "expected_keywords": [
        "degrees of freedom",
        "4",
        "point correspondences",
        "2",
        "linear system",
        "Ax=b",
        "least squares",
        "quadratic term",
        "nonlinear transformation",
        "parameter estimation"
      ],
      "rubric": {
        "total_points": 100,
        "criteria": [
          {
            "keyword": "degrees of freedom",
            "weight": 25,
            "description": "Correctly identifies 4 degrees of freedom (a, b, c, d)."
          },
          {
            "keyword": "point correspondences",
            "weight": 25,
            "description": "States that at least 2 point correspondences are needed for exact solution (4 equations from 2 points)."
          },
          {
            "keyword": "linear system derivation",
            "weight": 40,
            "description": "Correctly derives the linear system Ax = b by substituting correspondences into the transformation equations."
          },
          {
            "keyword": "clarity and structure",
            "weight": 10,
            "description": "Clear mathematical notation and logical flow in derivation."
          }
        ],
        "grading_notes": "Award partial credit for recognizing the quadratic term requires special handling. Deduct for incorrect matrix dimensions or missing terms."
      }
    },
    {
      "id": "pkg26_essay2",
      "prompt": "Using the following measured points before (p) and after (p') deformation, recover the parameters a, b, c, d, while assuming that there is no noise in the measurements. p = [0 2.00 4.00 6.00 8.00 10.00 10.00 8.00 6.00 4.00 2.00 0; 1 1.00 1.00 1.00 1.00 1.00 2.00 2.00 2.00 2.00 2.00 2], p' = [2 4.00 6.00 8.00 10.00 12.00 14.00 12.00 10.00 8.00 6.00 4; 1 1.04 1.16 1.36 1.64 2.00 3.00 2.64 2.36 2.16 2.04 2]. How would your solution be different if there was noise in the measurements?",
      "expected_keywords": [
        "overdetermined system",
        "least squares",
        "pseudoinverse",
        "normal equations",
        "robust estimation",
        "RANSAC",
        "outlier rejection"
      ],
      "rubric": {
        "total_points": 100,
        "criteria": [
          {
            "keyword": "parameter recovery",
            "weight": 40,
            "description": "Correctly computes a, b, c, d using exact data (e.g., via subset selection or full least squares)."
          },
          {
            "keyword": "noise handling",
            "weight": 30,
            "description": "Explains use of least squares on overdetermined system when noise is present."
          },
          {
            "keyword": "robustness",
            "weight": 20,
            "description": "Mentions RANSAC or outlier rejection for noisy real-world data."
          },
          {
            "keyword": "numerical stability",
            "weight": 10,
            "description": "Discusses conditioning or scaling of input coordinates."
          }
        ],
        "grading_notes": "Extra credit for implementing subset selection (e.g., using first two points) and verifying with full set. Deduct for algebraic errors in matrix setup."
      }
    },
    {
      "id": "pkg26_essay3",
      "prompt": "Two pinhole cameras observe a 3D point P = (X, Y, Z)^T from two different viewpoints. The camera projection matrices are C = [[1 0 0 0], [0 1 0 0], [0 0 1 0]] and C' = [[1 0 0 1], [0 0 -1 1], [0 1 0 1]]. The coordinates of the 2D point in the first and second image are p = (-1/2, 1/2)^T and p' = (0, -1/2)^T, respectively. (a) Compute the 3D coordinates X, Y and Z. (Hint: Form a linear system of equations using the projection matrices and homogeneous coordinates.) (b) The essential matrix between the views is E = [[0 1 1], [1 -1 0], [-1 0 -1]]. Find the epipolar line in the second image that corresponds to the point p in the first image. Show that the point p' lies on that epipolar line. (c) How can the epipolar constraint be utilized in stereo imaging?",
      "expected_keywords": [
        "triangulation",
        "homogeneous coordinates",
        "projection equations",
        "epipolar line",
        "epipolar constraint",
        "disparity search",
        "correspondence problem",
        "essential matrix"
      ],
      "rubric": {
        "total_points": 100,
        "criteria": [
          {
            "keyword": "3D reconstruction",
            "weight": 35,
            "description": "Correctly sets up and solves the linear system to find (X, Y, Z)."
          },
          {
            "keyword": "epipolar line",
            "weight": 30,
            "description": "Computes l' = E p and verifies p'^T l' = 0."
          },
          {
            "keyword": "stereo application",
            "weight": 25,
            "description": "Explains how epipolar constraint reduces 2D search to 1D along epipolar line."
          },
          {
            "keyword": "mathematical rigor",
            "weight": 10,
            "description": "Proper use of homogeneous coordinates and matrix notation."
          }
        ],
        "grading_notes": "Partial credit for correct setup even if final algebra has minor errors. Extra credit for discussing scale ambiguity or multiple solutions."
      }
    },
    {
      "id": "pkg26_essay4",
      "prompt": "Let us consider three grayscale image patches. The first two represent texture classes 1 and 2. The third belongs to an unknown class. Classify the unknown sample using: 1. Roberts gradient masks (no padding, valid pixels only), 2. Mean filter responses as feature vectors, 3. Euclidean distance to find closest class. Then, describe in detail a procedure based on Local Binary Patterns (LBP) for solving this texture classification problem. What problem is related to the given image patches?",
      "expected_keywords": [
        "Roberts cross",
        "gradient magnitude",
        "mean response",
        "Euclidean distance",
        "LBP",
        "circular neighborhood",
        "uniform patterns",
        "histogram",
        "rotation invariance",
        "small sample size",
        "border effects"
      ],
      "rubric": {
        "total_points": 100,
        "criteria": [
          {
            "keyword": "Roberts classification",
            "weight": 40,
            "description": "Correctly computes filter responses, means, and classifies via nearest neighbor."
          },
          {
            "keyword": "LBP procedure",
            "weight": 40,
            "description": "Describes LBP encoding, histogram construction, and chi-squared comparison."
          },
          {
            "keyword": "limitation identification",
            "weight": 20,
            "description": "Identifies small patch size, lack of rotation invariance, or border effects as issues."
          }
        ],
        "grading_notes": "Do not require actual computation in LBP part. Award points for clear step-by-step LBP pipeline."
      }
    },
    {
      "id": "pkg26_essay5",
      "prompt": "Given two consecutive frames of imagery with intensity function f(x,y,t), consider pixels with spatial coordinates (3,4) and (4,4) marked at t and t+Δt respectively. Here we assume that the origin is in the upper-left corner so that f(3,4,t) = 7 and f(4,4,t+Δt) = 3. (a) Use the 3x3 Prewitt masks to estimate the spatial derivatives ∂f/∂x and ∂f/∂y at the points (3,4,t) and (4,4,t+Δt). (b) Estimate the temporal derivative ∂f/∂t at the position (3,4,t) and (4,4,t+Δt).",
      "expected_keywords": [
        "Prewitt operator",
        "convolution",
        "spatial gradient",
        "finite difference",
        "temporal derivative",
        "optical flow constraint",
        "aperture problem"
      ],
      "rubric": {
        "total_points": 100,
        "criteria": [
          {
            "keyword": "Prewitt convolution",
            "weight": 50,
            "description": "Correctly applies Prewitt masks to compute fx and fy at both space-time locations."
          },
          {
            "keyword": "temporal derivative",
            "weight": 30,
            "description": "Computes ft using intensity difference and Δt (assumed or stated)."
          },
          {
            "keyword": "clarity of assumptions",
            "weight": 20,
            "description": "States Δt or justifies approximation; discusses limitations."
          }
        ],
        "grading_notes": "Accept either central or forward difference for ft. Deduct for incorrect mask orientation."
      }
    },
    {
      "id": "pkg26_essay6",
      "prompt": "A rectangle with corners A = (-1,1), B = (1,1), C = (1,-1), D = (-1,-1) undergoes an affine transformation and the corners are observed at A' = (1,2), B' = (3,2), C' = (-1,0), D' = (-3,0). Calculate the affine transformation using the least squares method. Then, explain when one should use a homography instead of an affine model. In what scenarios is a homography more expressive and flexible? When should algorithms use one over the other?",
      "expected_keywords": [
        "affine matrix",
        "6 parameters",
        "least squares",
        "overdetermined",
        "homography",
        "8 dof",
        "projective",
        "parallel lines",
        "vanishing point",
        "planar scene"
      ],
      "rubric": {
        "total_points": 100,
        "criteria": [
          {
            "keyword": "affine estimation",
            "weight": 45,
            "description": "Correctly forms and solves the 8x6 system for affine parameters."
          },
          {
            "keyword": "homography comparison",
            "weight": 35,
            "description": "Explains homography models perspective distortion; affine preserves parallelism."
          },
          {
            "keyword": "use cases",
            "weight": 20,
            "description": "Gives examples: affine for parallax-free motion, homography for planar surfaces under perspective."
          }
        ],
        "grading_notes": "Extra credit for verifying transformation on a test point. Accept pseudoinverse or normal equations."
      }
    },
    {
      "id": "pkg19_essay1",
      "prompt": "Develop seashell classifier. Segment using threshold on black background, improve with morphology. Extract texture with filter banks/LBP/co-occurrence for features. Classify with distance/classifier. Add shape like aspect ratio, combine features for joint classification. Use labeled training.",
      "expected_keywords": [
        "seashell classification",
        "segmentation",
        "thresholding",
        "morphology",
        "texture features",
        "filter banks",
        "LBP",
        "co-occurrence",
        "shape features",
        "feature combination",
        "training"
      ],
      "rubric": {
        "total_points": 100,
        "criteria": [
          {
            "keyword": "seashell classification",
            "weight": 5,
            "description": "System overview."
          },
          {
            "keyword": "segmentation",
            "weight": 10,
            "description": "Isolate shells."
          },
          {
            "keyword": "thresholding",
            "weight": 10,
            "description": "Contrast-based."
          },
          {
            "keyword": "morphology",
            "weight": 10,
            "description": "Cleanup."
          },
          {
            "keyword": "texture features",
            "weight": 15,
            "description": "Extraction method."
          },
          {
            "keyword": "filter banks",
            "weight": 5,
            "description": "One option."
          },
          {
            "keyword": "LBP",
            "weight": 5,
            "description": "One option."
          },
          {
            "keyword": "co-occurrence",
            "weight": 5,
            "description": "One option."
          },
          {
            "keyword": "shape features",
            "weight": 15,
            "description": "E.g., moments."
          },
          {
            "keyword": "feature combination",
            "weight": 10,
            "description": "Concatenate or fuse."
          },
          {
            "keyword": "training",
            "weight": 10,
            "description": "Labeled data."
          }
        ],
        "grading_notes": "Similar to pkg17."
      }
    },
    {
      "id": "pkg21_essay1",
      "prompt": "Explain stereo for parallel cameras. Derive 3D from disparity. Compute z for values. Compare stereo matching to flow: similar in correspondence, different in geometry vs time.",
      "expected_keywords": [
        "stereo",
        "parallel",
        "disparity",
        "3D equations",
        "computation",
        "matching",
        "flow comparison",
        "similarities",
        "differences"
      ],
      "rubric": {
        "total_points": 100,
        "criteria": [
          {
            "keyword": "stereo",
            "weight": 10,
            "description": "Setup."
          },
          {
            "keyword": "parallel",
            "weight": 10,
            "description": "Cameras."
          },
          {
            "keyword": "disparity",
            "weight": 15,
            "description": "Definition."
          },
          {
            "keyword": "3D equations",
            "weight": 15,
            "description": "x,y,z."
          },
          {
            "keyword": "computation",
            "weight": 10,
            "description": "Example."
          },
          {
            "keyword": "matching",
            "weight": 10,
            "description": "Correspondence."
          },
          {
            "keyword": "flow comparison",
            "weight": 10,
            "description": "To optical flow."
          },
          {
            "keyword": "similarities",
            "weight": 10,
            "description": "Matching."
          },
          {
            "keyword": "differences",
            "weight": 10,
            "description": "Constraints."
          }
        ],
        "grading_notes": "Similar to pkg14."
      }
    },
    {
      "id": "pkg17_essay1",
      "prompt": "Design a seashell classification system using texture and shape. Describe segmentation from black background using thresholding and morphology for cleanup. Explain texture extraction with filter banks/LBP/co-occurrence and classification with feature vectors, distance, classifier. Suggest shape features like eccentricity, compactness; combine via concatenation or separate classifiers. Discuss training with labeled data.",
      "expected_keywords": [
        "segmentation",
        "thresholding",
        "morphology",
        "texture extraction",
        "filter banks",
        "LBP",
        "co-occurrence",
        "feature vectors",
        "distance function",
        "classifier",
        "shape features",
        "combination"
      ],
      "rubric": {
        "total_points": 100,
        "criteria": [
          {
            "keyword": "segmentation",
            "weight": 10,
            "description": "Threshold on contrast."
          },
          {
            "keyword": "thresholding",
            "weight": 10,
            "description": "Binary separation."
          },
          {
            "keyword": "morphology",
            "weight": 10,
            "description": "Noise removal, hole filling."
          },
          {
            "keyword": "texture extraction",
            "weight": 15,
            "description": "Choose method."
          },
          {
            "keyword": "filter banks",
            "weight": 5,
            "description": "Multi-filter responses."
          },
          {
            "keyword": "LBP",
            "weight": 5,
            "description": "Local patterns."
          },
          {
            "keyword": "co-occurrence",
            "weight": 5,
            "description": "Spatial stats."
          },
          {
            "keyword": "feature vectors",
            "weight": 10,
            "description": "Summarize properties."
          },
          {
            "keyword": "distance function",
            "weight": 10,
            "description": "Similarity measure."
          },
          {
            "keyword": "classifier",
            "weight": 10,
            "description": "kNN or other."
          },
          {
            "keyword": "shape features",
            "weight": 10,
            "description": "Moments, boundaries."
          },
          {
            "keyword": "combination",
            "weight": 10,
            "description": "Fuse features."
          }
        ],
        "grading_notes": "Require full pipeline. Partial if missing shape integration."
      }
    },
    {
      "id": "pkg2_essay1",
      "prompt": "Discuss the challenges and assumptions in recovering 3D shape from shading using a single image under the Lambertian reflectance model. Explain the role of the brightness equation \\(I = \\mathbf{N} \\cdot \\mathbf{L}\\), why it is underconstrained, and how additional priors such as smoothness, integrability, or boundary conditions are used to regularize the solution. Include practical limitations and real-world violations of the model.",
      "expected_keywords": [
        "shape from shading",
        "Lambertian",
        "brightness equation",
        "I = N · L",
        "underconstrained",
        "integrability",
        "smoothness",
        "boundary conditions",
        "albedo variation",
        "cast shadows",
        "specular reflections",
        "unknown lighting"
      ],
      "rubric": {
        "total_points": 100,
        "criteria": [
          {
            "keyword": "shape from shading",
            "weight": 8,
            "description": "Clearly defines the goal: recover surface normals or depth from intensity variations."
          },
          {
            "keyword": "Lambertian",
            "weight": 8,
            "description": "States assumption of diffuse reflection with constant albedo."
          },
          {
            "keyword": "brightness equation",
            "weight": 8,
            "description": "Presents \\(I = \\mathbf{N} \\cdot \\mathbf{L}\\) or \\(I = \\cos \\theta_i\\) correctly."
          },
          {
            "keyword": "I = N · L",
            "weight": 8,
            "description": "Explains that intensity depends on dot product of normal and light direction."
          },
          {
            "keyword": "underconstrained",
            "weight": 10,
            "description": "Shows that one equation per pixel is insufficient to solve for two normal components."
          },
          {
            "keyword": "integrability",
            "weight": 12,
            "description": "Discusses need for normal field to be integrable into a consistent surface."
          },
          {
            "keyword": "smoothness",
            "weight": 10,
            "description": "Mentions regularization via minimizing changes in depth or normals."
          },
          {
            "keyword": "boundary conditions",
            "weight": 8,
            "description": "Notes importance of known depths or normals at occluding boundaries."
          },
          {
            "keyword": "albedo variation",
            "weight": 8,
            "description": "Identifies violation when surface reflectance is not uniform."
          },
          {
            "keyword": "cast shadows",
            "weight": 8,
            "description": "Recognizes that self-shadowing breaks the simple model."
          },
          {
            "keyword": "specular reflections",
            "weight": 6,
            "description": "Acknowledges specular highlights invalidate diffuse assumption."
          },
          {
            "keyword": "unknown lighting",
            "weight": 6,
            "description": "Points out difficulty when light direction is not known."
          }
        ],
        "grading_notes": "Award partial credit for intuitive explanations even if mathematical details are missing. Deduct for confusing shape from shading with stereo or structured light. Require at least two regularization methods for full marks on constraints."
      }
    },
    {
      "id": "pkg31_essay1",
      "prompt": "(a) Using homogeneous coordinates, write the matrix form of the following 2D transformations: pure translation, pure rotation, similarity (translation + rotation + scale), affine, and homography. How many degrees of freedom does each transformation have? How many 2D point correspondences are needed to estimate each?",
      "expected_keywords": [
        "homogeneous coordinates",
        "translation matrix",
        "rotation matrix",
        "similarity transformation",
        "affine transformation",
        "homography",
        "degrees of freedom",
        "point correspondences",
        "linear equations"
      ],
      "rubric": {
        "total_points": 100,
        "criteria": [
          {
            "keyword": "matrix forms",
            "weight": 40,
            "description": "Correctly writes each transformation in homogeneous matrix form (3x3 for 2D transformations)."
          },
          {
            "keyword": "degrees of freedom",
            "weight": 30,
            "description": "Identifies degrees of freedom for each transformation (translation=2, rotation=1, similarity=4, affine=6, homography=8)."
          },
          {
            "keyword": "point correspondences",
            "weight": 20,
            "description": "States number of point correspondences required to estimate each transformation (translation=1, rotation=2, similarity=2, affine=3, homography=4)."
          },
          {
            "keyword": "clarity",
            "weight": 10,
            "description": "Neat matrix notation and correct use of homogeneous coordinates."
          }
        ],
        "grading_notes": "Full credit for correct matrix representation and corresponding degrees of freedom. Partial credit for correct forms with minor omissions."
      }
    },
    {
      "id": "pkg31_essay2",
      "prompt": "(b) A rectangle with corners A = (−1,1), B = (1,1), C = (1,−1), and D = (−1,−1) undergoes a transformation and the corners are observed at A′ = (1,3), B′ = (3,3), C′ = (−2,1), and D′ = (−6,1). The affine transformation does not perfectly explain the observations, but we assume the transformation is affine and noisy. Estimate the optimal affine transformation using the least squares method.",
      "expected_keywords": [
        "affine transformation",
        "least squares estimation",
        "homogeneous coordinates",
        "overdetermined system",
        "Ax=b",
        "pseudoinverse",
        "normal equations",
        "noise",
        "parameter estimation"
      ],
      "rubric": {
        "total_points": 100,
        "criteria": [
          {
            "keyword": "affine model setup",
            "weight": 40,
            "description": "Forms the affine transformation equations in homogeneous coordinates and constructs the system Ax=b."
          },
          {
            "keyword": "least squares solution",
            "weight": 40,
            "description": "Uses least squares or pseudoinverse to estimate transformation parameters minimizing reprojection error."
          },
          {
            "keyword": "interpretation",
            "weight": 20,
            "description": "Explains the impact of noise and justifies the use of least squares as optimal for Gaussian noise."
          }
        ],
        "grading_notes": "Partial credit for correct system setup without derivation. Full credit requires explicit least-squares formulation or solution steps."
      }
    },
    {
      "id": "pkg31_essay3",
      "prompt": "(c) An affine transformation is the most flexible transformation that is linear in both homogeneous and inhomogeneous coordinates. When represented by a matrix (in homogeneous coordinates), how many elements does the matrix have? How many degrees of freedom does an affine transformation have? How many 2D point matches are necessary to estimate it?",
      "expected_keywords": [
        "affine matrix",
        "3x3 matrix",
        "six degrees of freedom",
        "2D point correspondences",
        "minimum points",
        "linear independence"
      ],
      "rubric": {
        "total_points": 100,
        "criteria": [
          {
            "keyword": "matrix elements",
            "weight": 40,
            "description": "States that the affine matrix has 9 elements in homogeneous form, but one scale factor is redundant."
          },
          {
            "keyword": "degrees of freedom",
            "weight": 30,
            "description": "Correctly identifies 6 degrees of freedom for 2D affine transformations."
          },
          {
            "keyword": "point matches",
            "weight": 30,
            "description": "Explains that 3 non-collinear 2D point correspondences are needed to estimate the affine transformation."
          }
        ],
        "grading_notes": "Partial credit for missing one of the three quantitative answers (elements, DoF, or points). Full credit for correct reasoning with supporting explanation."
      }
    },
    {
      "id": "pkg30_essay1",
      "prompt": "Triangulation using 1D cameras. The projection function for a 1D camera is m ∝ P x, where m = [m, 1]^T (pixel in homogeneous coords), x = [x, y, 1]^T (2D world point in homogeneous coords) and P is a 2×3 projection matrix. (a) Given two cameras P1 and P2 and their measurements m1 and m2 of an unknown point x, derive the constraints on x in the form A x = b (A is 2×2, b is 2×1). Express A and b in terms of P1, P2 and the measurements. (b) Given P1 = [[1,2,0],[2,1,0]] and P2 = [[1,2,3],[4,2,0]] and measurements m1 = 1.25 and m2 = 1, triangulate the point x (solve for x and y). (c) A point is often observed in many images: is there an advantage to estimating the point’s position by considering all images simultaneously? Justify. (d) Many points are observed in an image: is there an advantage to estimating all points jointly instead of independently? Justify.",
      "expected_keywords": [
        "triangulation",
        "homogeneous coordinates",
        "projection matrix",
        "linear constraint",
        "Ax=b",
        "overdetermined system",
        "least squares",
        "bundle adjustment",
        "multi-view",
        "robust estimation",
        "joint estimation",
        "covariance",
        "optimal estimation"
      ],
      "rubric": {
        "total_points": 100,
        "criteria": [
          {
            "keyword": "derivation Ax=b",
            "weight": 40,
            "description": "Correct derivation of A and b from the rows of P1,P2 and measured m1,m2, producing a 2×2 linear system for (x,y)."
          },
          {
            "keyword": "numerical triangulation",
            "weight": 30,
            "description": "Correct solution of the provided numeric example (substituting P1,P2,m1,m2 into derived system and solving for x,y)."
          },
          {
            "keyword": "multi-view advantage",
            "weight": 15,
            "description": "Explains benefits of using many images simultaneously (reduces noise, overdetermined -> least-squares, improved accuracy, outlier handling)."
          },
          {
            "keyword": "joint estimation of many points",
            "weight": 15,
            "description": "Explains advantages of joint estimation (bundle adjustment, enforces consistency, uses image/pose covariances) and notes computational trade-offs."
          }
        ],
        "grading_notes": "Partial credit for correct setup with minor algebraic errors. For (b) accept exact numeric solution or equivalent least-squares result. Award extra credit for mentioning covariance, weighting, or robust methods (e.g., RANSAC) when discussing multi-view or joint estimation."
      }
    },
    {
      "id": "pkg29_essay1",
      "prompt": "Describe an algorithm to segment the seashells from the image. How can morphological operations help to improve segmentation?",
      "expected_keywords": [
        "image segmentation",
        "thresholding",
        "background subtraction",
        "binary mask",
        "morphological operations",
        "erosion",
        "dilation",
        "opening",
        "closing",
        "connected components",
        "noise removal"
      ],
      "rubric": {
        "total_points": 100,
        "criteria": [
          {
            "keyword": "segmentation algorithm",
            "weight": 40,
            "description": "Proposes an algorithm for segmenting non-overlapping shells on a contrasting background using thresholding or color separation."
          },
          {
            "keyword": "morphological operations",
            "weight": 40,
            "description": "Explains how morphological filtering (erosion, dilation, opening, closing) improves segmentation quality and removes noise."
          },
          {
            "keyword": "clarity and application",
            "weight": 20,
            "description": "Provides a clear, step-by-step explanation with reasoning for each operation."
          }
        ],
        "grading_notes": "Partial credit for mentioning morphological operations without explaining their effect. Full credit for integrating them effectively into the segmentation pipeline."
      }
    },
    {
      "id": "pkg29_essay2",
      "prompt": "It has been decided that only texture will be used to classify the seashells. You may use either filter banks, Local Binary Patterns (LBP), or co-occurrence matrices to extract texture properties. Given a segmented image, describe an algorithm to classify a seashell. Where do the concepts of feature vector, distance function, and classifier fit in your algorithm?",
      "expected_keywords": [
        "texture features",
        "filter banks",
        "Gabor filters",
        "LBP",
        "gray-level co-occurrence matrix",
        "feature vector",
        "distance function",
        "Euclidean distance",
        "classifier",
        "training",
        "k-nearest neighbour",
        "support vector machine"
      ],
      "rubric": {
        "total_points": 100,
        "criteria": [
          {
            "keyword": "texture extraction",
            "weight": 30,
            "description": "Describes texture feature extraction using one of the suggested methods and explains its relevance."
          },
          {
            "keyword": "feature vector and distance",
            "weight": 35,
            "description": "Explains how texture features are formed into a feature vector and compared using a distance metric."
          },
          {
            "keyword": "classifier integration",
            "weight": 25,
            "description": "Describes how a classifier (e.g., k-NN or SVM) uses these feature vectors for training and prediction."
          },
          {
            "keyword": "clarity of process",
            "weight": 10,
            "description": "Logical structure and clear algorithmic flow from input to classification output."
          }
        ],
        "grading_notes": "Full credit requires integration of all three concepts—feature extraction, distance measurement, and classification—in a coherent algorithmic pipeline."
      }
    },
    {
      "id": "pkg29_essay3",
      "prompt": "The system should be extended to include shape features. What shape features could be useful for classifying the shells? How should one combine them with the texture features of the previous step? How should the algorithm be adapted to classify using both texture and shape?",
      "expected_keywords": [
        "shape features",
        "contour descriptors",
        "moments",
        "Hu moments",
        "aspect ratio",
        "Fourier descriptors",
        "texture-shape fusion",
        "feature-level fusion",
        "normalization",
        "weighted combination",
        "classifier adaptation"
      ],
      "rubric": {
        "total_points": 100,
        "criteria": [
          {
            "keyword": "shape descriptors",
            "weight": 35,
            "description": "Identifies relevant shape features and explains how they capture geometric differences between shells."
          },
          {
            "keyword": "feature fusion",
            "weight": 35,
            "description": "Describes how to combine texture and shape features via concatenation, normalization, or weighting."
          },
          {
            "keyword": "algorithm adaptation",
            "weight": 20,
            "description": "Proposes classifier or feature scaling modifications to integrate multimodal data."
          },
          {
            "keyword": "explanation clarity",
            "weight": 10,
            "description": "Provides a well-organized, conceptually sound discussion of combining multiple feature types."
          }
        ],
        "grading_notes": "Award partial credit for identifying correct shape features without discussing integration. Full credit for coherent fusion strategy and algorithmic justification."
      }
    },
    {
      "id": "pkg28_essay1",
      "prompt": "Briefly explain the following terms: (a) Optical flow (b) SIFT (c) Histogram (d) K-nearest neighbour classification (e) HSV color space.",
      "expected_keywords": [
        "optical flow",
        "motion estimation",
        "SIFT",
        "keypoints",
        "histogram",
        "feature distribution",
        "k-nearest neighbour",
        "classification",
        "HSV color space",
        "hue",
        "saturation",
        "value"
      ],
      "rubric": {
        "total_points": 100,
        "criteria": [
          {
            "keyword": "optical flow",
            "weight": 20,
            "description": "Defines optical flow as apparent pixel motion between frames and explains its role in motion estimation."
          },
          {
            "keyword": "SIFT",
            "weight": 20,
            "description": "Explains Scale-Invariant Feature Transform for keypoint detection and descriptor matching."
          },
          {
            "keyword": "histogram",
            "weight": 20,
            "description": "Describes histograms as statistical distributions of intensity or color values in an image."
          },
          {
            "keyword": "k-nearest neighbour",
            "weight": 20,
            "description": "Describes KNN as a distance-based classifier using feature similarity."
          },
          {
            "keyword": "HSV color space",
            "weight": 20,
            "description": "Explains the components of hue, saturation, and value and their perceptual significance."
          }
        ],
        "grading_notes": "Award partial credit for accurate but incomplete definitions. Full credit requires both definition and context of use in computer vision."
      }
    },
    {
      "id": "pkg28_essay2",
      "prompt": "Describe the main principles of the following and give one example of their usage: (a) Hough transform (b) RANSAC (c) Mahalanobis distance.",
      "expected_keywords": [
        "Hough transform",
        "parametric space",
        "line detection",
        "RANSAC",
        "robust estimation",
        "outliers",
        "Mahalanobis distance",
        "covariance",
        "multivariate distance",
        "feature matching"
      ],
      "rubric": {
        "total_points": 100,
        "criteria": [
          {
            "keyword": "Hough transform",
            "weight": 30,
            "description": "Describes parameter space voting for detecting geometric shapes such as lines or circles."
          },
          {
            "keyword": "RANSAC",
            "weight": 35,
            "description": "Explains Random Sample Consensus as a method for model estimation in the presence of outliers."
          },
          {
            "keyword": "Mahalanobis distance",
            "weight": 25,
            "description": "Defines Mahalanobis distance as a covariance-scaled metric for multivariate data comparison."
          },
          {
            "keyword": "examples and clarity",
            "weight": 10,
            "description": "Provides clear examples demonstrating real-world usage of each concept."
          }
        ],
        "grading_notes": "Full credit requires a clear example for each method. Partial credit for correct principles without examples."
      }
    },
    {
      "id": "pkg1_essay1",
      "prompt": "Write an essay (approximately 400–600 words) that: (1) explains the major ethical, legal, and technical challenges when publishing open datasets containing personal information; and (2) proposes a concrete, defensible workflow an academic researcher could follow to prepare and publish such a dataset while minimising privacy risks and complying with relevant legal requirements.",
      "expected_keywords": [
        "informed consent",
        "data minimisation",
        "anonymisation",
        "pseudonymisation",
        "k-anonymity",
        "GDPR",
        "lawful basis",
        "metadata",
        "FAIR",
        "licensing",
        "CC BY",
        "ODbL",
        "access controls",
        "encryption",
        "data quality"
      ],
      "rubric": {
        "total_points": 100,
        "criteria": [
          {
            "keyword": "informed consent",
            "weight": 15,
            "description": "Discusses the need for informed, specific consent (or alternative lawful basis) and the limits of consent for future/unknown uses."
          },
          {
            "keyword": "GDPR",
            "weight": 15,
            "description": "Explains legal obligations under GDPR (personal data definition, lawful basis, data subject rights, data minimisation) or references equivalent legal principles."
          },
          {
            "keyword": "anonymisation",
            "weight": 12,
            "description": "Describes anonymisation strategies and their limitations; distinguishes anonymisation from pseudonymisation."
          },
          {
            "keyword": "pseudonymisation",
            "weight": 8,
            "description": "Explains separation of identifiers from data and when pseudonymisation is appropriate vs. insufficient."
          },
          {
            "keyword": "k-anonymity",
            "weight": 8,
            "description": "Mentions k-anonymity or similar technical metrics as concrete measures and notes risks of re-identification."
          },
          {
            "keyword": "data minimisation",
            "weight": 8,
            "description": "Argues for collecting and publishing only the data needed and reducing data granularity where possible."
          },
          {
            "keyword": "metadata",
            "weight": 6,
            "description": "Addresses the need for rich metadata for reuse and provenance, balanced with privacy considerations."
          },
          {
            "keyword": "FAIR",
            "weight": 6,
            "description": "References FAIR principles (Findable, Accessible, Interoperable, Re-usable) and discusses balancing FAIR with privacy."
          },
          {
            "keyword": "licensing",
            "weight": 6,
            "description": "Identifies appropriate open-data licenses (e.g., CC0/CC BY/ODbL) and the implications of license choice."
          },
          {
            "keyword": "access controls",
            "weight": 6,
            "description": "Proposes controlled-access mechanisms (embargo, tiered access, data use agreements) when full openness is unsafe."
          },
          {
            "keyword": "encryption",
            "weight": 5,
            "description": "Mentions technical safeguards like encryption for storage and transfer where relevant."
          },
          {
            "keyword": "data quality",
            "weight": 5,
            "description": "Addresses the importance of data quality (bias, accuracy) to ethical reuse and downstream decision-making."
          }
        ],
        "grading_notes": "Scores are additive across criteria. A defensible workflow should be concrete (steps, checks, and responsible actors) and should address legal, ethical, and technical safeguards. Partial credit is given for mentioning concepts without deep explanation; full credit requires correct reasoning and linkage between steps and safeguards."
      }
    },
    {
      "id": "pkg3_essay1",
      "prompt": "Compose a 500–700 word essay critically evaluating the mechanisms, implications, and methods for handling missing data in real-world datasets. Your discussion should: (1) compare MCAR, MAR, and MNAR; (2) analyze the statistical and practical consequences of different handling methods such as deletion, imputation, and maximum likelihood; and (3) argue for best practices in applying multiple imputation and EM-based methods for modern data mining tasks.",
      "expected_keywords": [
        "MCAR",
        "MAR",
        "MNAR",
        "deletion",
        "listwise deletion",
        "pairwise deletion",
        "mean imputation",
        "regression imputation",
        "stochastic regression",
        "multiple imputation",
        "maximum likelihood estimation",
        "EM algorithm",
        "bias",
        "uncertainty",
        "variance",
        "time series",
        "autocorrelation"
      ],
      "rubric": {
        "total_points": 100,
        "criteria": [
          {
            "keyword": "MCAR",
            "weight": 10,
            "description": "Accurately defines MCAR and its implications for unbiasedness when missing values are ignored."
          },
          {
            "keyword": "MAR",
            "weight": 10,
            "description": "Explains MAR with examples and highlights model assumptions for unbiased estimation."
          },
          {
            "keyword": "MNAR",
            "weight": 10,
            "description": "Describes MNAR and explains challenges in modeling missingness depending on unobserved data."
          },
          {
            "keyword": "deletion",
            "weight": 8,
            "description": "Compares listwise and pairwise deletion, addressing data loss and potential bias."
          },
          {
            "keyword": "mean imputation",
            "weight": 8,
            "description": "Discusses the simplicity and limitations (e.g., reduced variability, bias) of mean imputation."
          },
          {
            "keyword": "regression imputation",
            "weight": 8,
            "description": "Explains regression-based approaches and when they produce unbiased estimates."
          },
          {
            "keyword": "multiple imputation",
            "weight": 12,
            "description": "Analyzes multiple imputation’s workflow (imputation, analysis, pooling) and how it manages uncertainty."
          },
          {
            "keyword": "maximum likelihood estimation",
            "weight": 10,
            "description": "Describes how ML estimation and the EM algorithm estimate parameters from incomplete data."
          },
          {
            "keyword": "bias",
            "weight": 8,
            "description": "Evaluates the sources and effects of bias in missing data handling."
          },
          {
            "keyword": "uncertainty",
            "weight": 6,
            "description": "Explains how multiple imputation accounts for imputation uncertainty."
          },
          {
            "keyword": "time series",
            "weight": 5,
            "description": "Mentions longitudinal data challenges such as autocorrelation and timeline disruption."
          },
          {
            "keyword": "best practices",
            "weight": 5,
            "description": "Proposes practical and ethical data management recommendations for modern research."
          }
        ],
        "grading_notes": "High-quality essays integrate conceptual definitions, practical examples, and methodological reasoning. Full credit requires linking statistical methods with data mining implications."
      }
    },
    {
      "id": "pkg4_essay1",
      "prompt": "Write a 600–800 word analytical essay discussing the challenges and solutions for merging, sampling, and balancing data in data mining. Include: (1) methods to synchronize and harmonize multisource datasets, (2) statistical implications of over- and under-sampling, and (3) approaches for evaluating model performance under imbalanced data conditions using metrics such as balanced accuracy, F1-score, and Cohen’s Kappa.",
      "expected_keywords": [
        "data merging",
        "sampling frequency",
        "downsampling",
        "oversampling",
        "SRSWR",
        "SRSWOR",
        "balanced sample",
        "SMOTE",
        "artificial data",
        "noise injection",
        "class imbalance",
        "accuracy paradox",
        "balanced accuracy",
        "precision",
        "recall",
        "specificity",
        "F1 score",
        "Cohen’s Kappa"
      ],
      "rubric": {
        "total_points": 100,
        "criteria": [
          {
            "keyword": "data merging",
            "weight": 10,
            "description": "Explains technical and semantic challenges in integrating multisource datasets."
          },
          {
            "keyword": "sampling frequency",
            "weight": 8,
            "description": "Describes how sampling rates affect signal merging and data consistency."
          },
          {
            "keyword": "downsampling",
            "weight": 6,
            "description": "Discusses the benefits and information loss associated with downsampling."
          },
          {
            "keyword": "oversampling",
            "weight": 6,
            "description": "Explains the computational implications and redundancy risks of oversampling."
          },
          {
            "keyword": "SMOTE",
            "weight": 10,
            "description": "Describes the SMOTE algorithm and its use in addressing class imbalance."
          },
          {
            "keyword": "class imbalance",
            "weight": 10,
            "description": "Analyzes causes and implications of imbalanced datasets in predictive modeling."
          },
          {
            "keyword": "balanced accuracy",
            "weight": 8,
            "description": "Applies balanced accuracy to correctly interpret model fairness under imbalance."
          },
          {
            "keyword": "precision",
            "weight": 6,
            "description": "Defines and interprets precision as a metric sensitive to false positives."
          },
          {
            "keyword": "recall",
            "weight": 6,
            "description": "Explains recall’s role in sensitivity and detection of positive classes."
          },
          {
            "keyword": "F1 score",
            "weight": 8,
            "description": "Evaluates trade-offs between precision and recall via F1 metric."
          },
          {
            "keyword": "Cohen’s Kappa",
            "weight": 8,
            "description": "Explains Kappa’s interpretation beyond chance agreement and its importance in classifier reliability."
          },
          {
            "keyword": "accuracy paradox",
            "weight": 8,
            "description": "Critically examines the misleading nature of accuracy in imbalanced datasets."
          }
        ],
        "grading_notes": "Essays should synthesize technical, statistical, and interpretive aspects of data preparation and model evaluation. Full credit requires clear integration of examples from merging, sampling, and performance assessment."
      }
    },
    {
      "id": "pkg2_essay1",
      "prompt": "Discuss the key methodological, ethical, and practical considerations in planning and conducting a human-centered data collection study (such as the wearable-sensor gym activity project presented in the lecture). Your answer should: (1) describe how to plan for repeatability, reproducibility, and accuracy; (2) outline how ethical concerns such as informed consent, privacy, and risk-benefit balance should be addressed; and (3) suggest strategies for mitigating technical and human-related data collection problems.",
      "expected_keywords": [
        "planning",
        "repeatability",
        "reproducibility",
        "accuracy",
        "informed consent",
        "privacy",
        "confidentiality",
        "risk-benefit balance",
        "sample size",
        "non-stationary data",
        "bias",
        "sensor calibration",
        "documentation",
        "Bluetooth connectivity",
        "volunteer recruitment"
      ],
      "rubric": {
        "total_points": 100,
        "criteria": [
          {
            "keyword": "planning",
            "weight": 10,
            "description": "Clearly outlines planning steps including defining goals, procedures, and methodologies."
          },
          {
            "keyword": "repeatability",
            "weight": 8,
            "description": "Describes procedures to ensure data can be replicated under similar conditions."
          },
          {
            "keyword": "reproducibility",
            "weight": 8,
            "description": "Explains how others can independently reproduce results from the same dataset."
          },
          {
            "keyword": "accuracy",
            "weight": 8,
            "description": "Mentions calibration, signal verification, and error checking to maintain data accuracy."
          },
          {
            "keyword": "informed consent",
            "weight": 12,
            "description": "Discusses voluntary, informed participation and explains consent procedures for human subjects."
          },
          {
            "keyword": "privacy",
            "weight": 8,
            "description": "Addresses protection of personal information and limits on data disclosure."
          },
          {
            "keyword": "confidentiality",
            "weight": 6,
            "description": "Explains how data will be securely stored and shared only with authorized personnel."
          },
          {
            "keyword": "risk-benefit balance",
            "weight": 8,
            "description": "Evaluates participant risks versus benefits and proposes measures to minimize harm."
          },
          {
            "keyword": "sample size",
            "weight": 6,
            "description": "Includes rationale for selecting a statistically significant sample size."
          },
          {
            "keyword": "non-stationary data",
            "weight": 6,
            "description": "Explains how variability in human or environmental conditions affects data collection."
          },
          {
            "keyword": "bias",
            "weight": 6,
            "description": "Discusses researcher and respondent biases and strategies to reduce them."
          },
          {
            "keyword": "sensor calibration",
            "weight": 5,
            "description": "Mentions procedures for checking and aligning sensors before and during experiments."
          },
          {
            "keyword": "documentation",
            "weight": 5,
            "description": "Emphasizes the role of thorough documentation for traceability and quality assurance."
          },
          {
            "keyword": "Bluetooth connectivity",
            "weight": 4,
            "description": "Identifies technical reliability issues and mitigation approaches (e.g., real-time monitoring)."
          },
          {
            "keyword": "volunteer recruitment",
            "weight": 4,
            "description": "Notes practical difficulties in participant recruitment and compliance."
          }
        ],
        "grading_notes": "Award full marks for structured, well-argued essays demonstrating integration of ethical, methodological, and practical aspects. Partial credit given for coverage of individual points without synthesis."
      }
    },
    {
      "id": "pkg8_essay1",
      "prompt": "Write a 700–900 word essay discussing model generalization in data mining. Your essay should (1) compare explanatory, predictive, and descriptive modeling, (2) describe how sample size, data partitioning, and validation strategies affect generalization, and (3) analyze the importance of managing temporal dependence and model complexity when building reliable models.",
      "expected_keywords": [
        "model generalization",
        "descriptive modeling",
        "explanatory modeling",
        "predictive modeling",
        "sample size",
        "training and test sets",
        "validation set",
        "cross-validation",
        "leave-one-out",
        "individual models",
        "population models",
        "temporal dependence",
        "sliding window",
        "overfitting",
        "model selection",
        "bias-variance tradeoff"
      ],
      "rubric": {
        "total_points": 100,
        "criteria": [
          {
            "keyword": "model generalization",
            "weight": 10,
            "description": "Defines generalization and explains its significance for unseen data."
          },
          {
            "keyword": "model types",
            "weight": 12,
            "description": "Compares descriptive, explanatory, and predictive models with real examples."
          },
          {
            "keyword": "sample size",
            "weight": 10,
            "description": "Explains how training sample size impacts bias, variance, and overfitting."
          },
          {
            "keyword": "data partitioning",
            "weight": 10,
            "description": "Describes correct use of training, validation, and test sets to ensure fairness."
          },
          {
            "keyword": "cross-validation",
            "weight": 8,
            "description": "Analyzes k-fold and leave-one-out strategies for robust evaluation."
          },
          {
            "keyword": "temporal dependence",
            "weight": 10,
            "description": "Discusses the pitfalls of random sampling and temporal leakage in time-dependent data."
          },
          {
            "keyword": "overfitting",
            "weight": 10,
            "description": "Explains overfitting prevention using validation and model simplification."
          },
          {
            "keyword": "model complexity",
            "weight": 8,
            "description": "Examines why simpler models often generalize better."
          },
          {
            "keyword": "feature extraction",
            "weight": 6,
            "description": "Describes how sliding windows and feature engineering support generalization."
          },
          {
            "keyword": "bias-variance tradeoff",
            "weight": 6,
            "description": "Connects generalization to balance between bias and variance."
          }
        ],
        "grading_notes": "Essays should demonstrate deep conceptual understanding, practical reasoning, and examples connecting model validation, data partitioning, and generalization theory."
      }
    },
    {
      "id": "pkg6_essay1",
      "prompt": "Compose a 700–900 word essay explaining how noise, signal saturation, and outliers affect data mining workflows. Your essay should (1) classify noise and discuss its impact on supervised learning, (2) explain various outlier detection methods such as statistical, proximity, clustering, and classification-based approaches, and (3) discuss practical strategies for distinguishing anomalies from harmful data errors in real-world contexts like sensor analysis or fraud detection.",
      "expected_keywords": [
        "noise",
        "attribute noise",
        "label noise",
        "data polishing",
        "robust learners",
        "noise filtering",
        "signal saturation",
        "outliers",
        "global outliers",
        "contextual outliers",
        "collective outliers",
        "anomalies",
        "outlier detection",
        "statistical methods",
        "kernel density estimation",
        "kNN",
        "Local Outlier Factor",
        "clustering methods",
        "one-class SVM",
        "time series"
      ],
      "rubric": {
        "total_points": 100,
        "criteria": [
          {
            "keyword": "noise",
            "weight": 10,
            "description": "Defines noise and distinguishes between attribute and label noise."
          },
          {
            "keyword": "data polishing",
            "weight": 8,
            "description": "Explains noise correction techniques and their limitations."
          },
          {
            "keyword": "robust learners",
            "weight": 8,
            "description": "Describes algorithms resilient to data corruption."
          },
          {
            "keyword": "signal saturation",
            "weight": 8,
            "description": "Discusses how sensor saturation distorts data and analysis."
          },
          {
            "keyword": "outliers",
            "weight": 10,
            "description": "Defines outliers and explains their statistical effects."
          },
          {
            "keyword": "outlier detection",
            "weight": 12,
            "description": "Covers multiple outlier detection techniques and their assumptions."
          },
          {
            "keyword": "Local Outlier Factor",
            "weight": 8,
            "description": "Explains the principle of local density comparison in outlier identification."
          },
          {
            "keyword": "clustering methods",
            "weight": 8,
            "description": "Discusses clustering-based approaches and their scalability limits."
          },
          {
            "keyword": "one-class SVM",
            "weight": 8,
            "description": "Describes how one-class SVMs identify anomalies without labeled outlier data."
          },
          {
            "keyword": "anomalies",
            "weight": 6,
            "description": "Differentiates anomalies from noise and outliers with domain examples."
          },
          {
            "keyword": "time series",
            "weight": 6,
            "description": "Explains predictive and unsupervised techniques for detecting temporal anomalies."
          },
          {
            "keyword": "application examples",
            "weight": 8,
            "description": "Illustrates methods through real-world cases like fraud detection or sensor monitoring."
          }
        ],
        "grading_notes": "Essays should integrate theoretical understanding with practical applications and emphasize conceptual distinctions between noise, outliers, and anomalies. Full credit requires linking detection techniques to real-world contexts."
      }
    },
    {
      "id": "pkg7_essay1",
      "prompt": "Write a 700–900 word essay analyzing the role of normalization, discretization, data reduction, and transformations to normality in preparing data for data mining. Discuss (1) when and why each preprocessing step is necessary, (2) compare techniques like Z-score, Min-Max scaling, PCA, and Box-Cox transformation, and (3) evaluate the trade-offs between information preservation, interpretability, and computational efficiency.",
      "expected_keywords": [
        "normalization",
        "Min-Max normalization",
        "Z-score standardization",
        "decimal scaling",
        "discretization",
        "categorical data",
        "dummy coding",
        "data reduction",
        "aggregation",
        "binning",
        "PCA",
        "Discrete Wavelet Transform",
        "Box-Cox transformation",
        "Gaussian distribution",
        "non-parametric methods",
        "information loss",
        "dimensionality reduction"
      ],
      "rubric": {
        "total_points": 100,
        "criteria": [
          {
            "keyword": "normalization",
            "weight": 10,
            "description": "Explains the concept, purpose, and methods of normalization including scaling and standardization."
          },
          {
            "keyword": "discretization",
            "weight": 10,
            "description": "Discusses discretization techniques, their use cases, and drawbacks."
          },
          {
            "keyword": "dummy coding",
            "weight": 8,
            "description": "Describes how categorical variables can be represented numerically."
          },
          {
            "keyword": "data reduction",
            "weight": 10,
            "description": "Explains reduction goals and methods such as aggregation, binning, and sampling."
          },
          {
            "keyword": "PCA",
            "weight": 10,
            "description": "Discusses PCA’s function, advantages, and interpretation limits."
          },
          {
            "keyword": "Box-Cox transformation",
            "weight": 8,
            "description": "Explains transformation for normality and constraints (e.g., positive-only data)."
          },
          {
            "keyword": "information loss",
            "weight": 8,
            "description": "Evaluates trade-offs and risks in discretization and data reduction."
          },
          {
            "keyword": "computational efficiency",
            "weight": 8,
            "description": "Analyzes how preprocessing impacts computational performance and model training time."
          },
          {
            "keyword": "Gaussian distribution",
            "weight": 8,
            "description": "Relates transformation to normality requirements in parametric models."
          },
          {
            "keyword": "interpretability",
            "weight": 10,
            "description": "Assesses how preprocessing steps affect transparency and ease of result interpretation."
          }
        ],
        "grading_notes": "Full credit requires a structured argument linking normalization, reduction, and transformation steps with their mathematical and practical roles in data mining. Examples from engineering or physical sciences should be included where applicable."
      }
    },
    {
      "id": "pkg10_essay1",
      "prompt": "Provide a comprehensive explanation of the pinhole camera model and its extensions in computer vision. Start with the ideal perspective projection geometry and derive the mapping from 3D world points to 2D image points using both Cartesian and homogeneous coordinates. Present the full camera matrix \\(P = K [R \\; t]\\) and explain each component of the intrinsic matrix \\(K\\) (focal length, principal point, skew, pixel aspect ratio). Discuss the role of extrinsic parameters and the decomposition of \\(P\\). Then, cover real-world deviations: radial and tangential lens distortion models, and their parameterization (k1, k2, p1, p2). Describe the standard calibration procedure using multiple views of a checkerboard pattern, including corner detection, homography estimation, and nonlinear optimization. Finally, discuss limitations of the pinhole model (finite aperture blur, rolling shutter, chromatic aberration) and advanced models (rational polynomial, fisheye). Include mathematical formulations and practical implications for downstream tasks like 3D reconstruction and augmented reality.",
      "expected_keywords": [
        "pinhole model",
        "perspective projection",
        "homogeneous coordinates",
        "camera matrix",
        "intrinsic parameters",
        "K matrix",
        "focal length",
        "principal point",
        "skew",
        "extrinsic parameters",
        "rotation R",
        "translation t",
        "projection matrix P",
        "radial distortion",
        "tangential distortion",
        "distortion coefficients",
        "camera calibration",
        "checkerboard",
        "homography",
        "Zhang's method",
        "corner detection",
        "nonlinear optimization",
        "vanishing points",
        "weak perspective",
        "affine camera",
        "rolling shutter",
        "chromatic aberration"
      ],
      "rubric": {
        "total_points": 100,
        "criteria": [
          {
            "keyword": "pinhole model",
            "weight": 6,
            "description": "Defines ideal perspective projection with infinitesimal aperture."
          },
          {
            "keyword": "perspective projection",
            "weight": 7,
            "description": "Derives \\(x = f X/Z, y = f Y/Z\\) geometrically."
          },
          {
            "keyword": "homogeneous coordinates",
            "weight": 7,
            "description": "Shows 3D → 2D via 3×4 matrix and dehomogenization."
          },
          {
            "keyword": "camera matrix",
            "weight": 6,
            "description": "States \\(P = K [R | t]\\) with correct dimensions."
          },
          {
            "keyword": "intrinsic parameters",
            "weight": 6,
            "description": "Lists all five: \\(\\alpha_x, \\alpha_y, s, c_x, c_y\\)."
          },
          {
            "keyword": "K matrix",
            "weight": 6,
            "description": "Writes upper-triangular form explicitly."
          },
          {
            "keyword": "focal length",
            "weight": 5,
            "description": "Relates physical \\(f\\) to pixel units via sensor size."
          },
          {
            "keyword": "principal point",
            "weight": 5,
            "description": "Optical center offset; often near image center."
          },
          {
            "keyword": "skew",
            "weight": 4,
            "description": "Non-zero in scanned film or misaligned sensors."
          },
          {
            "keyword": "extrinsic parameters",
            "weight": 5,
            "description": "6 DOF rigid transformation from world to camera."
          },
          {
            "keyword": "rotation R",
            "weight": 4,
            "description": "3×3 orthogonal matrix with det=1."
          },
          {
            "keyword": "translation t",
            "weight": 4,
            "description": "Camera center in world coordinates."
          },
          {
            "keyword": "projection matrix P",
            "weight": 5,
            "description": "11 DOF (up to scale); QR-like decomposition."
          },
          {
            "keyword": "radial distortion",
            "weight": 6,
            "description": "Polynomial model with \\(k_1, k_2, k_3\\); barrel/pincushion."
          },
          {
            "keyword": "tangential distortion",
            "weight": 5,
            "description": "Parameters \\(p_1, p_2\\) for decentering."
          },
          {
            "keyword": "distortion coefficients",
            "weight": 4,
            "description": "Typically 5 parameters total."
          },
          {
            "keyword": "camera calibration",
            "weight": 6,
            "description": "Multi-view optimization of reprojection error."
          },
          {
            "keyword": "checkerboard",
            "weight": 4,
            "description": "Known 3D corners for correspondences."
          },
          {
            "keyword": "homography",
            "weight": 5,
            "description": "Planar → image mapping per view."
          },
          {
            "keyword": "Zhang's method",
            "weight": 5,
            "description": "Closed-form then nonlinear refinement."
          },
          {
            "keyword": "corner detection",
            "weight": 4,
            "description": "Sub-pixel Harris or saddle-point methods."
          },
          {
            "keyword": "nonlinear optimization",
            "weight": 5,
            "description": "Levenberg-Marquardt on all parameters."
          },
          {
            "keyword": "vanishing points",
            "weight": 3,
            "description": "Bonus: lines at infinity for calibration."
          },
          {
            "keyword": "weak perspective",
            "weight": 3,
            "description": "Bonus: approximation for distant objects."
          },
          {
            "keyword": "rolling shutter",
            "weight": 3,
            "description": "Bonus: CMOS readout distortion."
          },
          {
            "keyword": "chromatic aberration",
            "weight": 3,
            "description": "Bonus: wavelength-dependent focal length."
          }
        ],
        "grading_notes": "Require correct projection equations and full K matrix for full marks. Award partial credit for good geometric intuition without full math. Deduct if intrinsic/extrinsic roles are swapped or distortion model direction is wrong. Bonus for mentioning modern tools (OpenCV calibrateCamera)."
      }
    },
    {
      "id": "pkg7_essay1",
      "prompt": "Explain the role of texture as a monocular cue for 3D shape perception. Define the concepts of slant and tilt and describe how isotropic texture is distorted under perspective projection. Derive the relationship between surface orientation and the aspect ratio/orientation of projected texture elements (e.g., via ellipse fitting in the frequency domain). Discuss the isotropy and homogeneity assumptions, their geometric implications, and common violations in real scenes. Compare shape from texture with shape from shading in terms of input requirements, robustness, and output (qualitative vs. metric). Include a brief mention of texture segmentation using statistical descriptors.",
      "expected_keywords": [
        "texture analysis",
        "shape from texture",
        "monocular cue",
        "slant",
        "tilt",
        "isotropy",
        "homogeneity",
        "foreshortening",
        "perspective projection",
        "ellipse fitting",
        "autocorrelation",
        "power spectrum",
        "frequency domain",
        "aspect ratio",
        "surface normal",
        "shape from shading",
        "statistical descriptors",
        "co-occurrence matrix",
        "segmentation"
      ],
      "rubric": {
        "total_points": 100,
        "criteria": [
          {
            "keyword": "texture analysis",
            "weight": 5,
            "description": "Defines texture as repetitive local pattern; role in segmentation and shape."
          },
          {
            "keyword": "shape from texture",
            "weight": 7,
            "description": "States goal: recover surface orientation from texture distortion."
          },
          {
            "keyword": "monocular cue",
            "weight": 5,
            "description": "Classifies as single-image 3D cue alongside shading, contour."
          },
          {
            "keyword": "slant",
            "weight": 6,
            "description": "Angle between surface normal and viewing direction (0° frontal, 90° edge-on)."
          },
          {
            "keyword": "tilt",
            "weight": 6,
            "description": "Direction of steepest ascent in image plane."
          },
          {
            "keyword": "isotropy",
            "weight": 7,
            "description": "Assumes texture statistically uniform in all 3D directions."
          },
          {
            "keyword": "homogeneity",
            "weight": 5,
            "description": "Texture density constant over surface."
          },
          {
            "keyword": "foreshortening",
            "weight": 7,
            "description": "Compression along tilt direction proportional to \\(\\cos \\phi\\)."
          },
          {
            "keyword": "perspective projection",
            "weight": 6,
            "description": "Links 3D slant to 2D affine distortion of texture elements."
          },
          {
            "keyword": "ellipse fitting",
            "weight": 8,
            "description": "Explains fitting ellipse to spectral peak or autocorrelation to estimate axes."
          },
          {
            "keyword": "autocorrelation",
            "weight": 5,
            "description": "Mentions local ACF reveals texel shape and spacing."
          },
          {
            "keyword": "power spectrum",
            "weight": 5,
            "description": "Frequency domain dual: energy concentrated in elliptical region."
          },
          {
            "keyword": "frequency domain",
            "weight": 5,
            "description": "Advantage: robust to phase, sensitive to scale/orientation."
          },
          {
            "keyword": "aspect ratio",
            "weight": 6,
            "description": "Minor/major axis ratio = \\(\\cos \\phi\\); major axis aligned with tilt."
          },
          {
            "keyword": "surface normal",
            "weight": 6,
            "description": "Recovers normal up to ambiguity (convex/concave)."
          },
          {
            "keyword": "shape from shading",
            "weight": 6,
            "description": "Compares: SfT needs texture, SfS needs smooth surface and known light."
          },
          {
            "keyword": "statistical descriptors",
            "weight": 5,
            "description": "Briefly notes GLCM, LBP for segmentation."
          },
          {
            "keyword": "co-occurrence matrix",
            "weight": 4,
            "description": "Captures spatial gray-level dependencies."
          },
          {
            "keyword": "segmentation",
            "weight": 4,
            "description": "Texture features for region labeling."
          }
        ],
        "grading_notes": "Require geometric derivation (foreshortening → ellipse) for full marks. Allow qualitative explanation of spectrum method. Deduct if slant/tilt confused or assumptions misstated. Partial credit for good intuition without math."
      }
    },
    {
      "id": "pkg8_essay1",
      "prompt": "Explain the complete pipeline for binary image analysis as presented in the lecture. Start with the motivation for binarization and the challenges posed by real-world imaging conditions (e.g., lighting, sensor noise). Describe global and adaptive thresholding methods, their assumptions, and failure cases. Then, detail the role of connectivity (4- vs 8-connected) and the two-pass connected component labeling algorithm using provisional labels and equivalence resolution. Finally, discuss morphological post-processing (opening, closing) with structuring elements to clean up noise and fill holes, and conclude with applications such as document OCR or object counting. Include mathematical definitions where appropriate (e.g., erosion, dilation).",
      "expected_keywords": [
        "binarization",
        "thresholding",
        "global threshold",
        "adaptive threshold",
        "illumination variation",
        "connected component",
        "4-connected",
        "8-connected",
        "labeling algorithm",
        "two-pass",
        "provisional label",
        "equivalence",
        "Union-Find",
        "morphology",
        "structuring element",
        "erosion",
        "dilation",
        "opening",
        "closing",
        "noise removal",
        "hole filling",
        "OCR",
        "document analysis"
      ],
      "rubric": {
        "total_points": 100,
        "criteria": [
          {
            "keyword": "binarization",
            "weight": 6,
            "description": "Defines conversion of grayscale to binary via intensity threshold."
          },
          {
            "keyword": "thresholding",
            "weight": 6,
            "description": "Explains \\(B(x,y) = 1\\) if \\(I(x,y) > T\\), else 0."
          },
          {
            "keyword": "global threshold",
            "weight": 5,
            "description": "Single \\(T\\) for whole image; fails under non-uniform light."
          },
          {
            "keyword": "adaptive threshold",
            "weight": 6,
            "description": "Local \\(T\\) per region (e.g., mean of neighborhood)."
          },
          {
            "keyword": "illumination variation",
            "weight": 5,
            "description": "Identifies shading, shadows as key challenges."
          },
          {
            "keyword": "connected component",
            "weight": 6,
            "description": "Maximal set of foreground pixels connected via adjacency."
          },
          {
            "keyword": "4-connected",
            "weight": 4,
            "description": "Horizontal/vertical neighbors only."
          },
          {
            "keyword": "8-connected",
            "weight": 4,
            "description": "Includes diagonal neighbors."
          },
          {
            "keyword": "labeling algorithm",
            "weight": 6,
            "description": "Assigns unique ID to each component."
          },
          {
            "keyword": "two-pass",
            "weight": 7,
            "description": "First pass: provisional labels + equivalence table; second: resolve."
          },
          {
            "keyword": "provisional label",
            "weight": 5,
            "description": "Temporary IDs during raster scan."
          },
          {
            "keyword": "equivalence",
            "weight": 5,
            "description": "Merged labels when adjacent components connect."
          },
          {
            "keyword": "Union-Find",
            "weight": 5,
            "description": "Efficient data structure for equivalence class management."
          },
          {
            "keyword": "morphology",
            "weight": 5,
            "description": "Set operations using structuring element \\(S\\)."
          },
          {
            "keyword": "structuring element",
            "weight": 5,
            "description": "Small binary mask (e.g., 3×3 cross or disk)."
          },
          {
            "keyword": "erosion",
            "weight": 5,
            "description": "\\(A \\ominus S = \\{z \\mid S_z \\subseteq A\\}\\)."
          },
          {
            "keyword": "dilation",
            "weight": 5,
            "description": "\\(A \\oplus S = \\{z \\mid S_z \\cap A \\neq \\emptyset\\}\\)."
          },
          {
            "keyword": "opening",
            "weight": 5,
            "description": "Erosion then dilation; removes small objects."
          },
          {
            "keyword": "closing",
            "weight": 5,
            "description": "Dilation then erosion; fills small holes."
          },
          {
            "keyword": "noise removal",
            "weight": 4,
            "description": "Opening eliminates salt noise and thin protrusions."
          },
          {
            "keyword": "hole filling",
            "weight": 4,
            "description": "Closing merges gaps in foreground."
          },
          {
            "keyword": "OCR",
            "weight": 4,
            "description": "Clean binary text enables character segmentation."
          },
          {
            "keyword": "document analysis",
            "weight": 4,
            "description": "Layout analysis, table detection, etc."
          }
        ],
        "grading_notes": "Require correct math for erosion/dilation and two-pass labeling logic. Full credit only if adaptive thresholding and morphology sequence are justified. Partial credit for good flow without full formalism. Deduct if connectivity types are confused or applications missing."
      }
    },
    {
      "id": "pkg9_essay1",
      "prompt": "Explain the physics and perception of color in computer vision, focusing on the dichromatic reflection model and its implications for highlight analysis and color constancy. Start with light-surface interaction (body vs. interface reflection) and derive the linear clustering property in RGB space for neutral interface reflection (NIR). Describe how this enables specular highlight detection and removal using pixel clustering or line fitting. Then, discuss computational color constancy: define the problem, explain the Gray World and White Patch assumptions, and their limitations under colored illuminants or non-Lambertian surfaces. Compare RGB-based methods with learning-based approaches (e.g., gamut mapping). Conclude with the role of color in higher-level vision tasks (segmentation, recognition) and challenges posed by sensor metamerism.",
      "expected_keywords": [
        "color perception",
        "additive primaries",
        "RGB",
        "CIE chromaticity",
        "spectral locus",
        "dichromatic model",
        "body reflection",
        "interface reflection",
        "specular highlight",
        "neutral interface",
        "RGB clustering",
        "highlight removal",
        "color constancy",
        "illuminant estimation",
        "Gray World",
        "White Patch",
        "gamut mapping",
        "metamerism",
        "sensor sensitivity",
        "segmentation",
        "recognition"
      ],
      "rubric": {
        "total_points": 100,
        "criteria": [
          {
            "keyword": "color perception",
            "weight": 5,
            "description": "Links human cone responses to trichromatic theory."
          },
          {
            "keyword": "additive primaries",
            "weight": 4,
            "description": "Mentions RGB basis for displays."
          },
          {
            "keyword": "RGB",
            "weight": 4,
            "description": "Standard image representation."
          },
          {
            "keyword": "CIE chromaticity",
            "weight": 5,
            "description": "Normalizes for brightness; shows diagram."
          },
          {
            "keyword": "spectral locus",
            "weight": 5,
            "description": "Boundary of visible monochromatic colors."
          },
          {
            "keyword": "dichromatic model",
            "weight": 7,
            "description": "Two-term: \\(I = I_b + I_s\\)."
          },
          {
            "keyword": "body reflection",
            "weight": 6,
            "description": "Lambertian, colored by surface albedo."
          },
          {
            "keyword": "interface reflection",
            "weight": 6,
            "description": "Mirror-like, colored by light (white for NIR)."
          },
          {
            "keyword": "specular highlight",
            "weight": 6,
            "description": "Bright spots where view aligns with reflection."
          },
          {
            "keyword": "neutral interface",
            "weight": 6,
            "description": "Specular component = illuminant color."
          },
          {
            "keyword": "RGB clustering",
            "weight": 8,
            "description": "Diffuse pixels cluster; specular form line to light color."
          },
          {
            "keyword": "highlight removal",
            "weight": 7,
            "description": "Subtract specular via clustering or inpainting."
          },
          {
            "keyword": "color constancy",
            "weight": 7,
            "description": "Estimate illuminant to recover canonical albedo."
          },
          {
            "keyword": "illuminant estimation",
            "weight": 6,
            "description": "Goal: find light color from image alone."
          },
          {
            "keyword": "Gray World",
            "weight": 6,
            "description": "Average image color = gray → light = average."
          },
          {
            "keyword": "White Patch",
            "weight": 6,
            "description": "Brightest pixel = white under light."
          },
          {
            "keyword": "gamut mapping",
            "weight": 5,
            "description": "Constrain possible illuminants by surface gamut."
          },
          {
            "keyword": "metamerism",
            "weight": 5,
            "description": "Different spectra → same RGB."
          },
          {
            "keyword": "sensor sensitivity",
            "weight": 4,
            "description": "Camera ≠ human cones → color errors."
          },
          {
            "keyword": "segmentation",
            "weight": 4,
            "description": "Color as feature beyond intensity."
          },
          {
            "keyword": "recognition",
            "weight": 4,
            "description": "Material/color diagnostic for object ID."
          }
        ],
        "grading_notes": "Require geometric explanation of dichromatic lines in RGB. Full credit only if both Gray World and White Patch are critiqued (e.g., fails on single-color scenes). Allow modern methods mention for bonus. Deduct if reflection model confused with shading."
      }
    },
    {
      "id": "pkg15_essay1",
      "prompt": "Discuss texture classification using gradient features and LBP. For the given patches, compute Roberts gradients, mean feature vectors, and classify unknown using Euclidean distance. Describe LBP computation and histogram-based classification. Note issues with small patches. Compare to filter banks for texture analysis.",
      "expected_keywords": [
        "texture classification",
        "Roberts masks",
        "feature vectors",
        "Euclidean distance",
        "LBP operator",
        "binary patterns",
        "histogram",
        "patch size issue",
        "filter banks",
        "multi-scale analysis"
      ],
      "rubric": {
        "total_points": 100,
        "criteria": [
          {
            "keyword": "texture classification",
            "weight": 10,
            "description": "Goal: assign class labels."
          },
          {
            "keyword": "Roberts masks",
            "weight": 15,
            "description": "Apply to compute derivatives."
          },
          {
            "keyword": "feature vectors",
            "weight": 15,
            "description": "Means of responses."
          },
          {
            "keyword": "Euclidean distance",
            "weight": 10,
            "description": "Classify to nearest."
          },
          {
            "keyword": "LBP operator",
            "weight": 15,
            "description": "Neighborhood comparisons."
          },
          {
            "keyword": "binary patterns",
            "weight": 10,
            "description": "Code to decimal."
          },
          {
            "keyword": "histogram",
            "weight": 10,
            "description": "Distribution as feature."
          },
          {
            "keyword": "patch size issue",
            "weight": 5,
            "description": "Small samples limit stats."
          },
          {
            "keyword": "filter banks",
            "weight": 5,
            "description": "Multi-filter responses."
          },
          {
            "keyword": "multi-scale analysis",
            "weight": 5,
            "description": "Gabor etc. for texture."
          }
        ],
        "grading_notes": "Require specific computations from patches. Partial if errors but correct steps."
      }
    },
    {
      "id": "pkg12_essay1",
      "prompt": "Discuss 2D transformations in homogeneous coordinates. Write the matrix forms for translation, rotation, similarity (RST), affine, and homography. Specify the minimum point correspondences needed for each. Using the example of a transformed rectangle with given coordinates, explain how to solve for similarity parameters. Extend to noisy affine estimation using least squares. Include advantages of homogeneous representation and applications in vision.",
      "expected_keywords": [
        "homogeneous coordinates",
        "translation",
        "rotation",
        "similarity",
        "affine",
        "homography",
        "point correspondences",
        "least squares",
        "matrix form",
        "degrees of freedom"
      ],
      "rubric": {
        "total_points": 100,
        "criteria": [
          {
            "keyword": "homogeneous coordinates",
            "weight": 10,
            "description": "Explains augmentation to handle projective transforms linearly."
          },
          {
            "keyword": "translation",
            "weight": 8,
            "description": "Matrix and 2 correspondences."
          },
          {
            "keyword": "rotation",
            "weight": 8,
            "description": "Matrix and 2 correspondences."
          },
          {
            "keyword": "similarity",
            "weight": 10,
            "description": "4 DOF, 2 correspondences; solve example."
          },
          {
            "keyword": "affine",
            "weight": 10,
            "description": "6 DOF, 3 correspondences."
          },
          {
            "keyword": "homography",
            "weight": 10,
            "description": "8 DOF, 4 correspondences."
          },
          {
            "keyword": "point correspondences",
            "weight": 10,
            "description": "Minimum for unique solution."
          },
          {
            "keyword": "least squares",
            "weight": 15,
            "description": "Overdetermined system for noisy affine."
          },
          {
            "keyword": "matrix form",
            "weight": 10,
            "description": "Correct 3x3 matrices shown."
          },
          {
            "keyword": "degrees of freedom",
            "weight": 9,
            "description": "Counts for each transform."
          }
        ],
        "grading_notes": "Require math matrices and example calculation. Partial for intuition without full derivation."
      }
    },
    {
      "id": "pkg24_essay1",
      "prompt": "Solve for 3D point from projection matrices using linear system. Compute epipolar line from essential matrix and verify point lies on it. Explain epipolar constraint in stereo for reducing search space.",
      "expected_keywords": [
        "projection matrices",
        "linear system",
        "3D coordinates",
        "essential matrix",
        "epipolar line",
        "verification",
        "constraint usage",
        "stereo search"
      ],
      "rubric": {
        "total_points": 100,
        "criteria": [
          {
            "keyword": "projection matrices",
            "weight": 15,
            "description": "Given C, C'."
          },
          {
            "keyword": "linear system",
            "weight": 20,
            "description": "Homogeneous equations."
          },
          {
            "keyword": "3D coordinates",
            "weight": 15,
            "description": "Solve for X,Y,Z."
          },
          {
            "keyword": "essential matrix",
            "weight": 10,
            "description": "Given E."
          },
          {
            "keyword": "epipolar line",
            "weight": 15,
            "description": "Compute l' = E p."
          },
          {
            "keyword": "verification",
            "weight": 10,
            "description": "p' ^ T l' = 0."
          },
          {
            "keyword": "constraint usage",
            "weight": 10,
            "description": "In stereo."
          },
          {
            "keyword": "stereo search",
            "weight": 5,
            "description": "1D reduction."
          }
        ],
        "grading_notes": "Require math solutions. Partial for calculation errors."
      }
    },
    {
      "id": "pkg23_essay1",
      "prompt": "Explain stereo system with parallel cameras. Derive 3D coordinates from disparity. Compute z for values. Compare disparity search to optical flow matching: similar in finding correspondences, different in stereo geometry vs temporal continuity.",
      "expected_keywords": [
        "stereo system",
        "parallel pinhole",
        "disparity",
        "3D coordinates",
        "depth computation",
        "search similarity",
        "optical flow",
        "differences"
      ],
      "rubric": {
        "total_points": 100,
        "criteria": [
          {
            "keyword": "stereo system",
            "weight": 10,
            "description": "Description."
          },
          {
            "keyword": "parallel pinhole",
            "weight": 10,
            "description": "Setup."
          },
          {
            "keyword": "disparity",
            "weight": 15,
            "description": "xl - xr."
          },
          {
            "keyword": "3D coordinates",
            "weight": 20,
            "description": "Equations."
          },
          {
            "keyword": "depth computation",
            "weight": 10,
            "description": "Example."
          },
          {
            "keyword": "search similarity",
            "weight": 10,
            "description": "To flow."
          },
          {
            "keyword": "optical flow",
            "weight": 10,
            "description": "Comparison."
          },
          {
            "keyword": "differences",
            "weight": 15,
            "description": "Geometry vs time."
          }
        ],
        "grading_notes": "Require equations and calc."
      }
    },
    {
      "id": "pkg22_essay1",
      "prompt": "For texture classification of patches, compute Roberts filter responses, mean features, and classify unknown with Euclidean. Detail LBP procedure, issues with small patches. Discuss filter banks as alternative for texture analysis.",
      "expected_keywords": [
        "texture classification",
        "Roberts filter",
        "mean features",
        "Euclidean distance",
        "LBP procedure",
        "small patch issues",
        "filter banks"
      ],
      "rubric": {
        "total_points": 100,
        "criteria": [
          {
            "keyword": "texture classification",
            "weight": 10,
            "description": "Task overview."
          },
          {
            "keyword": "Roberts filter",
            "weight": 20,
            "description": "Responses calculation."
          },
          {
            "keyword": "mean features",
            "weight": 15,
            "description": "Vectors."
          },
          {
            "keyword": "Euclidean distance",
            "weight": 15,
            "description": "Classification."
          },
          {
            "keyword": "LBP procedure",
            "weight": 20,
            "description": "Steps for texture."
          },
          {
            "keyword": "small patch issues",
            "weight": 10,
            "description": "Reliability."
          },
          {
            "keyword": "filter banks",
            "weight": 10,
            "description": "Alternative."
          }
        ],
        "grading_notes": "Require computations. Partial for errors."
      }
    },
    {
      "id": "pkg25_essay1",
      "prompt": "For deformed rectangle, determine DOF (4) and min correspondences (2) for given nonlinear transform. Derive linear system Ax=b for parameters a,b,c,d from correspondences. Discuss homogeneous coordinates for 2D transforms and applications like image registration.",
      "expected_keywords": [
        "2D deformation",
        "DOF",
        "correspondences",
        "linear system",
        "Ax=b",
        "homogeneous",
        "applications"
      ],
      "rubric": {
        "total_points": 100,
        "criteria": [
          {
            "keyword": "2D deformation",
            "weight": 10,
            "description": "Given function."
          },
          {
            "keyword": "DOF",
            "weight": 15,
            "description": "4 parameters."
          },
          {
            "keyword": "correspondences",
            "weight": 15,
            "description": "Min 2."
          },
          {
            "keyword": "linear system",
            "weight": 20,
            "description": "Setup equations."
          },
          {
            "keyword": "Ax=b",
            "weight": 20,
            "description": "Matrix form."
          },
          {
            "keyword": "homogeneous",
            "weight": 10,
            "description": "For transforms."
          },
          {
            "keyword": "applications",
            "weight": 10,
            "description": "E.g., fitting."
          }
        ],
        "grading_notes": "Require derivation. Partial for missing homogeneous."
      }
    },
    {
      "id": "pkg13_essay1",
      "prompt": "Explain optical flow computation using the brightness constancy assumption. Derive the flow constraint equation involving spatial and temporal derivatives. Using the provided image patches, demonstrate calculation of ∂f/∂x, ∂f/∂y using Prewitt masks and ∂f/∂t at specific points. Discuss limitations like the aperture problem and methods to solve it (e.g., Lucas-Kanade). Include applications in motion segmentation and tracking.",
      "expected_keywords": [
        "optical flow",
        "brightness constancy",
        "constraint equation",
        "spatial derivatives",
        "temporal derivative",
        "Prewitt masks",
        "aperture problem",
        "Lucas-Kanade",
        "neighborhood assumption",
        "motion segmentation"
      ],
      "rubric": {
        "total_points": 100,
        "criteria": [
          {
            "keyword": "optical flow",
            "weight": 10,
            "description": "Defines as apparent motion field."
          },
          {
            "keyword": "brightness constancy",
            "weight": 10,
            "description": "Assumes I(x,y,t) = I(x+u,y+v,t+1)."
          },
          {
            "keyword": "constraint equation",
            "weight": 15,
            "description": "Derives Ix u + Iy v + It = 0."
          },
          {
            "keyword": "spatial derivatives",
            "weight": 15,
            "description": "Computes with Prewitt at given points."
          },
          {
            "keyword": "temporal derivative",
            "weight": 10,
            "description": "Frame difference at points."
          },
          {
            "keyword": "Prewitt masks",
            "weight": 10,
            "description": "Applies correctly to patches."
          },
          {
            "keyword": "aperture problem",
            "weight": 10,
            "description": "Ambiguity in local estimation."
          },
          {
            "keyword": "Lucas-Kanade",
            "weight": 10,
            "description": "Local constancy over window."
          },
          {
            "keyword": "neighborhood assumption",
            "weight": 5,
            "description": "Constant flow in small area."
          },
          {
            "keyword": "motion segmentation",
            "weight": 5,
            "description": "Application example."
          }
        ],
        "grading_notes": "Require accurate derivative calculations from patches. Partial if math errors but correct method."
      }
    },
    {
      "id": "pkg14_essay1",
      "prompt": "Explain stereo vision geometry for parallel cameras. Derive the depth equation z = f b / d from baseline b, focal f, disparity d. Compute z for given values. Compare correspondence search in stereo to optical flow estimation, noting similarities in matching and differences in constraints (epipolar vs. temporal). Discuss challenges like occlusions and methods like semi-global matching.",
      "expected_keywords": [
        "stereo vision",
        "parallel cameras",
        "baseline",
        "focal length",
        "disparity",
        "depth equation",
        "correspondence search",
        "optical flow",
        "epipolar constraint",
        "occlusions"
      ],
      "rubric": {
        "total_points": 100,
        "criteria": [
          {
            "keyword": "stereo vision",
            "weight": 10,
            "description": "Defines binocular depth perception."
          },
          {
            "keyword": "parallel cameras",
            "weight": 10,
            "description": "Assumes aligned optics."
          },
          {
            "keyword": "baseline",
            "weight": 10,
            "description": "Separation b between centers."
          },
          {
            "keyword": "focal length",
            "weight": 10,
            "description": "f in projection."
          },
          {
            "keyword": "disparity",
            "weight": 15,
            "description": "d = xl - xr."
          },
          {
            "keyword": "depth equation",
            "weight": 15,
            "description": "Derives z = f b / d; computes example."
          },
          {
            "keyword": "correspondence search",
            "weight": 10,
            "description": "Finding matches."
          },
          {
            "keyword": "optical flow",
            "weight": 10,
            "description": "Similar matching over time."
          },
          {
            "keyword": "epipolar constraint",
            "weight": 5,
            "description": "Reduces search to lines."
          },
          {
            "keyword": "occlusions",
            "weight": 5,
            "description": "Visibility differences."
          }
        ],
        "grading_notes": "Require derivation and computation. Partial for missing differences."
      }
    },
    {
      "id": "pkg4_essay1",
      "prompt": "Explain the optical flow estimation problem and the two fundamental approaches: Lucas-Kanade (local) and Horn-Schunck (global). Describe the brightness constancy constraint, the aperture problem, and how each method addresses motion ambiguity. Compare their assumptions, advantages, and limitations, particularly in handling occlusions, large motions, and textureless regions. Include a brief discussion on modern extensions (e.g., robust penalties, coarse-to-fine warping).",
      "expected_keywords": [
        "optical flow",
        "brightness constancy",
        "aperture problem",
        "Lucas-Kanade",
        "local method",
        "spatial coherence",
        "weighted least squares",
        "Horn-Schunck",
        "global method",
        "smoothness term",
        "variational formulation",
        "occlusions",
        "large displacement",
        "coarse-to-fine",
        "image pyramid",
        "robust penalty"
      ],
      "rubric": {
        "total_points": 100,
        "criteria": [
          {
            "keyword": "optical flow",
            "weight": 6,
            "description": "Defines optical flow as the apparent 2D motion field induced by 3D scene/camera motion."
          },
          {
            "keyword": "brightness constancy",
            "weight": 8,
            "description": "States \\(I(x,y,t) = I(x+u\\Delta t, y+v\\Delta t, t+\\Delta t)\\) and derives the optical flow constraint equation."
          },
          {
            "keyword": "aperture problem",
            "weight": 8,
            "description": "Explains local ambiguity: only normal flow recoverable from edge-like structures."
          },
          {
            "keyword": "Lucas-Kanade",
            "weight": 8,
            "description": "Describes assumption of constant flow in small window and solution via weighted least squares."
          },
          {
            "keyword": "local method",
            "weight": 5,
            "description": "Notes sparsity (flow computed only at reliable points) and robustness to noise in textured areas."
          },
          {
            "keyword": "spatial coherence",
            "weight": 5,
            "description": "Mentions neighborhood votes to resolve aperture ambiguity."
          },
          {
            "keyword": "weighted least squares",
            "weight": 5,
            "description": "Shows system \\(A^T W A \\mathbf{v} = A^T W \\mathbf{b}\\) for solving local flow."
          },
          {
            "keyword": "Horn-Schunck",
            "weight": 8,
            "description": "Presents energy functional \\(E = \\int (I_x u + I_y v + I_t)^2 + \\alpha (|\\nabla u|^2 + |\\nabla v|^2) \\,dx\\,dy\\)."
          },
          {
            "keyword": "global method",
            "weight": 5,
            "description": "Emphasizes dense flow field and global smoothness propagation."
          },
          {
            "keyword": "smoothness term",
            "weight": 6,
            "description": "Explains role of \\(\\alpha\\) in trading off data fidelity vs. smoothness."
          },
          {
            "keyword": "variational formulation",
            "weight": 5,
            "description": "Mentions Euler-Lagrange equations or iterative solution."
          },
          {
            "keyword": "occlusions",
            "weight": 6,
            "description": "Discusses failure at motion boundaries; LK may skip, HS over-smooths."
          },
          {
            "keyword": "large displacement",
            "weight": 6,
            "description": "Notes linearization fails; both need coarse-to-fine or warping."
          },
          {
            "keyword": "coarse-to-fine",
            "weight": 6,
            "description": "Describes multi-scale pyramid to handle large motions."
          },
          {
            "keyword": "image pyramid",
            "weight": 4,
            "description": "Explains downsampling and incremental flow refinement."
          },
          {
            "keyword": "robust penalty",
            "weight": 4,
            "description": "Mentions modern use of L1 or censored penalties for outlier rejection."
          }
        ],
        "grading_notes": "Require clear distinction between local and global methods. Full credit only if both aperture problem and smoothness regularization are explained. Allow partial credit for intuitive descriptions without full math. Deduct if methods are confused or assumptions misstated."
      }
    },
    {
      "id": "pkg31_essay1",
      "prompt": "(a) Using homogeneous coordinates, write the matrix form of the following 2D transformations: pure translation, pure rotation, similarity (translation + rotation + scale), affine, and homography. How many degrees of freedom does each transformation have? How many 2D point correspondences are needed to estimate each?",
      "expected_keywords": [
        "homogeneous coordinates",
        "translation matrix",
        "rotation matrix",
        "similarity transformation",
        "affine transformation",
        "homography",
        "degrees of freedom",
        "point correspondences",
        "linear equations"
      ],
      "rubric": {
        "total_points": 100,
        "criteria": [
          {
            "keyword": "matrix forms",
            "weight": 40,
            "description": "Correctly writes each transformation in homogeneous matrix form (3x3 for 2D transformations)."
          },
          {
            "keyword": "degrees of freedom",
            "weight": 30,
            "description": "Identifies degrees of freedom for each transformation (translation=2, rotation=1, similarity=4, affine=6, homography=8)."
          },
          {
            "keyword": "point correspondences",
            "weight": 20,
            "description": "States number of point correspondences required to estimate each transformation (translation=1, rotation=2, similarity=2, affine=3, homography=4)."
          },
          {
            "keyword": "clarity",
            "weight": 10,
            "description": "Neat matrix notation and correct use of homogeneous coordinates."
          }
        ],
        "grading_notes": "Full credit for correct matrix representation and corresponding degrees of freedom. Partial credit for correct forms with minor omissions."
      }
    },
    {
      "id": "pkg31_essay2",
      "prompt": "(b) A rectangle with corners A = (−1,1), B = (1,1), C = (1,−1), and D = (−1,−1) undergoes a transformation and the corners are observed at A′ = (1,3), B′ = (3,3), C′ = (−2,1), and D′ = (−6,1). The affine transformation does not perfectly explain the observations, but we assume the transformation is affine and noisy. Estimate the optimal affine transformation using the least squares method.",
      "expected_keywords": [
        "affine transformation",
        "least squares estimation",
        "homogeneous coordinates",
        "overdetermined system",
        "Ax=b",
        "pseudoinverse",
        "normal equations",
        "noise",
        "parameter estimation"
      ],
      "rubric": {
        "total_points": 100,
        "criteria": [
          {
            "keyword": "affine model setup",
            "weight": 40,
            "description": "Forms the affine transformation equations in homogeneous coordinates and constructs the system Ax=b."
          },
          {
            "keyword": "least squares solution",
            "weight": 40,
            "description": "Uses least squares or pseudoinverse to estimate transformation parameters minimizing reprojection error."
          },
          {
            "keyword": "interpretation",
            "weight": 20,
            "description": "Explains the impact of noise and justifies the use of least squares as optimal for Gaussian noise."
          }
        ],
        "grading_notes": "Partial credit for correct system setup without derivation. Full credit requires explicit least-squares formulation or solution steps."
      }
    },
    {
      "id": "pkg31_essay3",
      "prompt": "(c) An affine transformation is the most flexible transformation that is linear in both homogeneous and inhomogeneous coordinates. When represented by a matrix (in homogeneous coordinates), how many elements does the matrix have? How many degrees of freedom does an affine transformation have? How many 2D point matches are necessary to estimate it?",
      "expected_keywords": [
        "affine matrix",
        "3x3 matrix",
        "six degrees of freedom",
        "2D point correspondences",
        "minimum points",
        "linear independence"
      ],
      "rubric": {
        "total_points": 100,
        "criteria": [
          {
            "keyword": "matrix elements",
            "weight": 40,
            "description": "States that the affine matrix has 9 elements in homogeneous form, but one scale factor is redundant."
          },
          {
            "keyword": "degrees of freedom",
            "weight": 30,
            "description": "Correctly identifies 6 degrees of freedom for 2D affine transformations."
          },
          {
            "keyword": "point matches",
            "weight": 30,
            "description": "Explains that 3 non-collinear 2D point correspondences are needed to estimate the affine transformation."
          }
        ],
        "grading_notes": "Partial credit for missing one of the three quantitative answers (elements, DoF, or points). Full credit for correct reasoning with supporting explanation."
      }
    },
    {
      "id": "pkg30_essay1",
      "prompt": "Triangulation using 1D cameras. The projection function for a 1D camera is m ∝ P x, where m = [m, 1]^T (pixel in homogeneous coords), x = [x, y, 1]^T (2D world point in homogeneous coords) and P is a 2×3 projection matrix. (a) Given two cameras P1 and P2 and their measurements m1 and m2 of an unknown point x, derive the constraints on x in the form A x = b (A is 2×2, b is 2×1). Express A and b in terms of P1, P2 and the measurements. (b) Given P1 = [[1,2,0],[2,1,0]] and P2 = [[1,2,3],[4,2,0]] and measurements m1 = 1.25 and m2 = 1, triangulate the point x (solve for x and y). (c) A point is often observed in many images: is there an advantage to estimating the point’s position by considering all images simultaneously? Justify. (d) Many points are observed in an image: is there an advantage to estimating all points jointly instead of independently? Justify.",
      "expected_keywords": [
        "triangulation",
        "homogeneous coordinates",
        "projection matrix",
        "linear constraint",
        "Ax=b",
        "overdetermined system",
        "least squares",
        "bundle adjustment",
        "multi-view",
        "robust estimation",
        "joint estimation",
        "covariance",
        "optimal estimation"
      ],
      "rubric": {
        "total_points": 100,
        "criteria": [
          {
            "keyword": "derivation Ax=b",
            "weight": 40,
            "description": "Correct derivation of A and b from the rows of P1,P2 and measured m1,m2, producing a 2×2 linear system for (x,y)."
          },
          {
            "keyword": "numerical triangulation",
            "weight": 30,
            "description": "Correct solution of the provided numeric example (substituting P1,P2,m1,m2 into derived system and solving for x,y)."
          },
          {
            "keyword": "multi-view advantage",
            "weight": 15,
            "description": "Explains benefits of using many images simultaneously (reduces noise, overdetermined -> least-squares, improved accuracy, outlier handling)."
          },
          {
            "keyword": "joint estimation of many points",
            "weight": 15,
            "description": "Explains advantages of joint estimation (bundle adjustment, enforces consistency, uses image/pose covariances) and notes computational trade-offs."
          }
        ],
        "grading_notes": "Partial credit for correct setup with minor algebraic errors. For (b) accept exact numeric solution or equivalent least-squares result. Award extra credit for mentioning covariance, weighting, or robust methods (e.g., RANSAC) when discussing multi-view or joint estimation."
      }
    },
    {
      "id": "pkg29_essay1",
      "prompt": "Describe an algorithm to segment the seashells from the image. How can morphological operations help to improve segmentation?",
      "expected_keywords": [
        "image segmentation",
        "thresholding",
        "background subtraction",
        "binary mask",
        "morphological operations",
        "erosion",
        "dilation",
        "opening",
        "closing",
        "connected components",
        "noise removal"
      ],
      "rubric": {
        "total_points": 100,
        "criteria": [
          {
            "keyword": "segmentation algorithm",
            "weight": 40,
            "description": "Proposes an algorithm for segmenting non-overlapping shells on a contrasting background using thresholding or color separation."
          },
          {
            "keyword": "morphological operations",
            "weight": 40,
            "description": "Explains how morphological filtering (erosion, dilation, opening, closing) improves segmentation quality and removes noise."
          },
          {
            "keyword": "clarity and application",
            "weight": 20,
            "description": "Provides a clear, step-by-step explanation with reasoning for each operation."
          }
        ],
        "grading_notes": "Partial credit for mentioning morphological operations without explaining their effect. Full credit for integrating them effectively into the segmentation pipeline."
      }
    },
    {
      "id": "pkg29_essay2",
      "prompt": "It has been decided that only texture will be used to classify the seashells. You may use either filter banks, Local Binary Patterns (LBP), or co-occurrence matrices to extract texture properties. Given a segmented image, describe an algorithm to classify a seashell. Where do the concepts of feature vector, distance function, and classifier fit in your algorithm?",
      "expected_keywords": [
        "texture features",
        "filter banks",
        "Gabor filters",
        "LBP",
        "gray-level co-occurrence matrix",
        "feature vector",
        "distance function",
        "Euclidean distance",
        "classifier",
        "training",
        "k-nearest neighbour",
        "support vector machine"
      ],
      "rubric": {
        "total_points": 100,
        "criteria": [
          {
            "keyword": "texture extraction",
            "weight": 30,
            "description": "Describes texture feature extraction using one of the suggested methods and explains its relevance."
          },
          {
            "keyword": "feature vector and distance",
            "weight": 35,
            "description": "Explains how texture features are formed into a feature vector and compared using a distance metric."
          },
          {
            "keyword": "classifier integration",
            "weight": 25,
            "description": "Describes how a classifier (e.g., k-NN or SVM) uses these feature vectors for training and prediction."
          },
          {
            "keyword": "clarity of process",
            "weight": 10,
            "description": "Logical structure and clear algorithmic flow from input to classification output."
          }
        ],
        "grading_notes": "Full credit requires integration of all three concepts—feature extraction, distance measurement, and classification—in a coherent algorithmic pipeline."
      }
    },
    {
      "id": "pkg29_essay3",
      "prompt": "The system should be extended to include shape features. What shape features could be useful for classifying the shells? How should one combine them with the texture features of the previous step? How should the algorithm be adapted to classify using both texture and shape?",
      "expected_keywords": [
        "shape features",
        "contour descriptors",
        "moments",
        "Hu moments",
        "aspect ratio",
        "Fourier descriptors",
        "texture-shape fusion",
        "feature-level fusion",
        "normalization",
        "weighted combination",
        "classifier adaptation"
      ],
      "rubric": {
        "total_points": 100,
        "criteria": [
          {
            "keyword": "shape descriptors",
            "weight": 35,
            "description": "Identifies relevant shape features and explains how they capture geometric differences between shells."
          },
          {
            "keyword": "feature fusion",
            "weight": 35,
            "description": "Describes how to combine texture and shape features via concatenation, normalization, or weighting."
          },
          {
            "keyword": "algorithm adaptation",
            "weight": 20,
            "description": "Proposes classifier or feature scaling modifications to integrate multimodal data."
          },
          {
            "keyword": "explanation clarity",
            "weight": 10,
            "description": "Provides a well-organized, conceptually sound discussion of combining multiple feature types."
          }
        ],
        "grading_notes": "Award partial credit for identifying correct shape features without discussing integration. Full credit for coherent fusion strategy and algorithmic justification."
      }
    },
    {
      "id": "pkg28_essay1",
      "prompt": "Briefly explain the following terms: (a) Optical flow (b) SIFT (c) Histogram (d) K-nearest neighbour classification (e) HSV color space.",
      "expected_keywords": [
        "optical flow",
        "motion estimation",
        "SIFT",
        "keypoints",
        "histogram",
        "feature distribution",
        "k-nearest neighbour",
        "classification",
        "HSV color space",
        "hue",
        "saturation",
        "value"
      ],
      "rubric": {
        "total_points": 100,
        "criteria": [
          {
            "keyword": "optical flow",
            "weight": 20,
            "description": "Defines optical flow as apparent pixel motion between frames and explains its role in motion estimation."
          },
          {
            "keyword": "SIFT",
            "weight": 20,
            "description": "Explains Scale-Invariant Feature Transform for keypoint detection and descriptor matching."
          },
          {
            "keyword": "histogram",
            "weight": 20,
            "description": "Describes histograms as statistical distributions of intensity or color values in an image."
          },
          {
            "keyword": "k-nearest neighbour",
            "weight": 20,
            "description": "Describes KNN as a distance-based classifier using feature similarity."
          },
          {
            "keyword": "HSV color space",
            "weight": 20,
            "description": "Explains the components of hue, saturation, and value and their perceptual significance."
          }
        ],
        "grading_notes": "Award partial credit for accurate but incomplete definitions. Full credit requires both definition and context of use in computer vision."
      }
    },
    {
      "id": "pkg28_essay2",
      "prompt": "Describe the main principles of the following and give one example of their usage: (a) Hough transform (b) RANSAC (c) Mahalanobis distance.",
      "expected_keywords": [
        "Hough transform",
        "parametric space",
        "line detection",
        "RANSAC",
        "robust estimation",
        "outliers",
        "Mahalanobis distance",
        "covariance",
        "multivariate distance",
        "feature matching"
      ],
      "rubric": {
        "total_points": 100,
        "criteria": [
          {
            "keyword": "Hough transform",
            "weight": 30,
            "description": "Describes parameter space voting for detecting geometric shapes such as lines or circles."
          },
          {
            "keyword": "RANSAC",
            "weight": 35,
            "description": "Explains Random Sample Consensus as a method for model estimation in the presence of outliers."
          },
          {
            "keyword": "Mahalanobis distance",
            "weight": 25,
            "description": "Defines Mahalanobis distance as a covariance-scaled metric for multivariate data comparison."
          },
          {
            "keyword": "examples and clarity",
            "weight": 10,
            "description": "Provides clear examples demonstrating real-world usage of each concept."
          }
        ],
        "grading_notes": "Full credit requires a clear example for each method. Partial credit for correct principles without examples."
      }
    },
    {
      "id": "pkg6_essay1",
      "prompt": "Explain the Harris corner detector in detail. Start with the motivation for detecting local features in image matching. Derive the structure tensor \\(M\\) from the image gradients and the autocorrelation approximation. Describe how the corner response \\(R = \\det(M) - k \\trace(M)^2\\) classifies image regions into flat, edge, and corner using eigenvalue analysis. Discuss practical implementation steps (Gaussian smoothing, non-maximum suppression, thresholding) and limitations (scale variance, sensitivity to noise). Conclude with how Harris features enable robust correspondence in stereo, motion, or recognition pipelines.",
      "expected_keywords": [
        "local features",
        "image matching",
        "correspondence",
        "Harris corner",
        "structure tensor",
        "image gradients",
        "I_x",
        "I_y",
        "autocorrelation",
        "corner response",
        "det(M)",
        "trace(M)",
        "eigenvalues",
        "flat region",
        "edge",
        "corner",
        "Gaussian window",
        "non-maximum suppression",
        "thresholding",
        "scale invariance",
        "noise sensitivity",
        "sparse features",
        "robust matching"
      ],
      "rubric": {
        "total_points": 100,
        "criteria": [
          {
            "keyword": "local features",
            "weight": 5,
            "description": "Defines need for repeatable, localizable points for matching."
          },
          {
            "keyword": "image matching",
            "weight": 5,
            "description": "Links to finding correspondences across views."
          },
          {
            "keyword": "correspondence",
            "weight": 5,
            "description": "Mentions applications in stereo, motion, recognition."
          },
          {
            "keyword": "Harris corner",
            "weight": 5,
            "description": "Names the detector and its goal: find L- or X-junctions."
          },
          {
            "keyword": "structure tensor",
            "weight": 8,
            "description": "Shows \\(M = \\begin{bmatrix} \\langle I_x^2 \\rangle & \\langle I_x I_y \\rangle \\\\ \\langle I_x I_y \\rangle & \\langle I_y^2 \\rangle \\end{bmatrix}\\)."
          },
          {
            "keyword": "image gradients",
            "weight": 6,
            "description": "Explains computation of \\(I_x, I_y\\) via finite differences."
          },
          {
            "keyword": "I_x",
            "weight": 3,
            "description": "Used in gradient products."
          },
          {
            "keyword": "I_y",
            "weight": 3,
            "description": "Used in gradient products."
          },
          {
            "keyword": "autocorrelation",
            "weight": 7,
            "description": "Connects to Taylor expansion and shift-induced intensity change."
          },
          {
            "keyword": "corner response",
            "weight": 7,
            "description": "States \\(R = \\det(M) - k \\trace(M)^2\\)."
          },
          {
            "keyword": "det(M)",
            "weight": 5,
            "description": "Interprets as product of eigenvalues."
          },
          {
            "keyword": "trace(M)",
            "weight": 5,
            "description": "Interprets as sum of eigenvalues."
          },
          {
            "keyword": "eigenvalues",
            "weight": 8,
            "description": "Classifies: both small → flat; one large → edge; both large → corner."
          },
          {
            "keyword": "flat region",
            "weight": 4,
            "description": "R ≈ 0 (small negative or positive)."
          },
          {
            "keyword": "edge",
            "weight": 4,
            "description": "R large negative."
          },
          {
            "keyword": "corner",
            "weight": 4,
            "description": "R large positive."
          },
          {
            "keyword": "Gaussian window",
            "weight": 5,
            "description": "Averages gradients to reduce noise."
          },
          {
            "keyword": "non-maximum suppression",
            "weight": 5,
            "description": "Ensures one response per corner."
          },
          {
            "keyword": "thresholding",
            "weight": 4,
            "description": "Filters weak responses."
          },
          {
            "keyword": "scale invariance",
            "weight": 5,
            "description": "Notes Harris is not scale-invariant; needs multi-scale."
          },
          {
            "keyword": "noise sensitivity",
            "weight": 5,
            "description": "Discusses impact of derivative noise."
          },
          {
            "keyword": "sparse features",
            "weight": 4,
            "description": "Contrasts with dense methods."
          },
          {
            "keyword": "robust matching",
            "weight": 5,
            "description": "Enables RANSAC, triangulation, etc."
          }
        ],
        "grading_notes": "Require eigenvalue interpretation and response formula for full credit. Allow partial math if intuition is strong. Deduct if autocorrelation or gradient averaging is missing. Award bonus for mentioning later improvements (e.g., SIFT)."
      }
    },
    {
      "id": "pkg11_essay1",
      "prompt": "Provide an overview of the entire computer vision course as outlined in the introductory lecture. List all 11 modules in order with their exact titles and briefly describe the role of each in building a complete vision system from raw pixels to semantic understanding. Explain how the course progresses from low-level image formation (imaging, light, color) through mid-level feature extraction and geometric reasoning (binary analysis, texture, local features, 2D/3D models, motion, depth cues) to high-level recognition and 3D interpretation. Discuss the pedagogical motivation for this bottom-up structure and how it mirrors both classical and modern vision pipelines. Finally, reflect on the challenges highlighted in the introduction (ambiguity, illumination, occlusion, scale) and how subsequent modules address them.",
      "expected_keywords": [
        "course outline",
        "module sequence",
        "1.1 Introduction",
        "2.1 Imaging – Pinhole model",
        "3.1 Light and color",
        "4.1 Binary image analysis",
        "5.1 Texture analysis",
        "6.1 Local features – Harris",
        "7.1 Recognition – Basics",
        "8.1 Motion – Basics",
        "9.1 2D models – Fitting",
        "10.1 Depth from images – 3D cues",
        "11.1 3D – Affine transformation",
        "low-level processing",
        "mid-level features",
        "high-level recognition",
        "bottom-up pipeline",
        "inverse problem",
        "ambiguity",
        "illumination variation",
        "occlusion",
        "scale changes",
        "photons to semantics"
      ],
      "rubric": {
        "total_points": 100,
        "criteria": [
          {
            "keyword": "course outline",
            "weight": 6,
            "description": "Accurately lists all 11 modules with correct numbering and titles."
          },
          {
            "keyword": "module sequence",
            "weight": 5,
            "description": "Presents modules in exact chronological order."
          },
          {
            "keyword": "1.1 Introduction",
            "weight": 3,
            "description": "Sets context and motivation."
          },
          {
            "keyword": "2.1 Imaging – Pinhole model",
            "weight": 4,
            "description": "Image formation and geometry."
          },
          {
            "keyword": "3.1 Light and color",
            "weight": 4,
            "description": "Physics of light-surface interaction."
          },
          {
            "keyword": "4.1 Binary image analysis",
            "weight": 4,
            "description": "Segmentation and morphology."
          },
          {
            "keyword": "5.1 Texture analysis",
            "weight": 4,
            "description": "Surface properties and shape cues."
          },
          {
            "keyword": "6.1 Local features – Harris",
            "weight": 4,
            "description": "Correspondence and matching."
          },
          {
            "keyword": "7.1 Recognition – Basics",
            "weight": 4,
            "description": "Classification and learning."
          },
          {
            "keyword": "8.1 Motion – Basics",
            "weight": 4,
            "description": "Optical flow and tracking."
          },
          {
            "keyword": "9.1 2D models – Fitting",
            "weight": 4,
            "description": "Lines, conics, RANSAC."
          },
          {
            "keyword": "10.1 Depth from images – 3D cues",
            "weight": 4,
            "description": "Monocular depth estimation."
          },
          {
            "keyword": "11.1 3D – Affine transformation",
            "weight": 4,
            "description": "3D reasoning and reconstruction."
          },
          {
            "keyword": "low-level processing",
            "weight": 5,
            "description": "Modules 2–5: pixels to regions."
          },
          {
            "keyword": "mid-level features",
            "weight": 5,
            "description": "Modules 6–10: structure and relations."
          },
          {
            "keyword": "high-level recognition",
            "weight": 5,
            "description": "Module 7 and beyond: semantics."
          },
          {
            "keyword": "bottom-up pipeline",
            "weight": 6,
            "description": "Mimics classical vision hierarchy."
          },
          {
            "keyword": "inverse problem",
            "weight": 5,
            "description": "Recovering 3D from 2D is ill-posed."
          },
          {
            "keyword": "ambiguity",
            "weight": 4,
            "description": "Multiple 3D scenes → same image."
          },
          {
            "keyword": "illumination variation",
            "weight": 4,
            "description": "Handled in color and recognition."
          },
          {
            "keyword": "occlusion",
            "weight": 4,
            "description": "Addressed in motion and 2D/3D fitting."
          },
          {
            "keyword": "scale changes",
            "weight": 4,
            "description": "Local features and multi-scale analysis."
          },
          {
            "keyword": "photons to semantics",
            "weight": 5,
            "description": "Overall course mantra."
          }
        ],
        "grading_notes": "Require exact module titles and order for full credit. Allow flexible grouping into low/mid/high levels. Deduct if any module is missing or misordered. Award bonus for connecting specific challenges to later modules (e.g., occlusion → motion segmentation)."
      }
    },
    {
      "id": "pkg5_essay1",
      "prompt": "Define pattern recognition and explain its role in computer vision using the handwritten character recognition example from the lecture. Discuss the key challenges: intra-class variability, inter-class similarity, and the need for learning from data. Describe a typical machine learning-based recognition pipeline (preprocessing → feature extraction → classifier training → inference). Contrast this with rule-based or template-matching approaches, highlighting why learning is essential in practice. Finally, connect recognition to prior topics in the course (e.g., 2D model fitting, motion, 3D cues) and explain how robust low-level processing enables high-level recognition.",
      "expected_keywords": [
        "pattern recognition",
        "label assignment",
        "classification",
        "regression",
        "handwritten digits",
        "intra-class variability",
        "inter-class similarity",
        "learning from data",
        "training set",
        "feature extraction",
        "classifier",
        "template matching",
        "rule-based",
        "machine learning",
        "preprocessing",
        "inference",
        "2D model fitting",
        "occlusion",
        "motion segmentation",
        "3D cues"
      ],
      "rubric": {
        "total_points": 100,
        "criteria": [
          {
            "keyword": "pattern recognition",
            "weight": 6,
            "description": "Clearly defines as assigning a label to an input pattern."
          },
          {
            "keyword": "label assignment",
            "weight": 5,
            "description": "Emphasizes decision-making from image to semantic category."
          },
          {
            "keyword": "classification",
            "weight": 5,
            "description": "Identifies discrete output (e.g., digit 0–9)."
          },
          {
            "keyword": "regression",
            "weight": 4,
            "description": "Mentions continuous output as alternative (e.g., bounding box coordinates)."
          },
          {
            "keyword": "handwritten digits",
            "weight": 5,
            "description": "Uses lecture example to illustrate real-world variability."
          },
          {
            "keyword": "intra-class variability",
            "weight": 7,
            "description": "Explains same class (e.g., '1') can look very different."
          },
          {
            "keyword": "inter-class similarity",
            "weight": 6,
            "description": "Notes different classes (e.g., '1' vs '7') can be confusable."
          },
          {
            "keyword": "learning from data",
            "weight": 8,
            "description": "States that rules are hard to write; models must be learned."
          },
          {
            "keyword": "training set",
            "weight": 5,
            "description": "Mentions labeled examples needed for supervised learning."
          },
          {
            "keyword": "feature extraction",
            "weight": 7,
            "description": "Describes transforming raw pixels into discriminative representation."
          },
          {
            "keyword": "classifier",
            "weight": 6,
            "description": "Names examples: k-NN, SVM, neural networks."
          },
          {
            "keyword": "template matching",
            "weight": 6,
            "description": "Critiques as brittle to variation in pose, style, illumination."
          },
          {
            "keyword": "rule-based",
            "weight": 5,
            "description": "Notes difficulty in hand-crafting rules for complex patterns."
          },
          {
            "keyword": "machine learning",
            "weight": 6,
            "description": "Contrasts with traditional AI; data-driven generalization."
          },
          {
            "keyword": "preprocessing",
            "weight": 5,
            "description": "Includes normalization, binarization, noise removal."
          },
          {
            "keyword": "inference",
            "weight": 5,
            "description": "Describes applying trained model to new input."
          },
          {
            "keyword": "2D model fitting",
            "weight": 5,
            "description": "Links to locating object parts despite occlusion/clutter."
          },
          {
            "keyword": "occlusion",
            "weight": 4,
            "description": "Notes recognition must handle partial visibility."
          },
          {
            "keyword": "motion segmentation",
            "weight": 4,
            "description": "Connects to isolating moving objects before recognition."
          },
          {
            "keyword": "3D cues",
            "weight": 4,
            "description": "Mentions depth, perspective aid in disambiguating 2D projections."
          }
        ],
        "grading_notes": "Require pipeline structure and contrast with non-learning methods. Full credit only if challenges (variability) and need for learning are well-explained. Allow flexibility in classifier choice. Deduct if recognition is confused with detection or reconstruction."
      }
    },
    {
      "id": "pkg3_essay1",
      "prompt": "Discuss the key challenges in 2D model fitting as illustrated in the lecture. Categorize the problems into noise, extraneous data (clutter/outliers), and missing data (occlusions). For each category, provide a real-world example from the slides, explain why it violates ideal assumptions, and suggest a computational strategy (e.g., RANSAC, part-based models, robust estimators) to address it. Conclude with a discussion on how increasing model complexity (from lines to articulated cars) impacts both robustness and computational cost.",
      "expected_keywords": [
        "model fitting",
        "parametric model",
        "geometric primitives",
        "noise",
        "extraneous data",
        "clutter",
        "outliers",
        "missing data",
        "occlusion",
        "line detection",
        "circle fitting",
        "car model",
        "RANSAC",
        "robust estimation",
        "part-based model",
        "hypothesis generation",
        "verification",
        "computational cost",
        "articulation"
      ],
      "rubric": {
        "total_points": 100,
        "criteria": [
          {
            "keyword": "model fitting",
            "weight": 5,
            "description": "Clearly defines model fitting as estimating parameters of a geometric model from image features."
          },
          {
            "keyword": "parametric model",
            "weight": 5,
            "description": "Mentions use of mathematical models with parameters (size, position, orientation)."
          },
          {
            "keyword": "geometric primitives",
            "weight": 5,
            "description": "Lists at least two primitives (e.g., line, circle, ellipse)."
          },
          {
            "keyword": "noise",
            "weight": 8,
            "description": "Defines as small random errors in feature locations; links to robust estimators."
          },
          {
            "keyword": "extraneous data",
            "weight": 8,
            "description": "Defines as irrelevant features; example: multiple coins when fitting one circle."
          },
          {
            "keyword": "clutter",
            "weight": 5,
            "description": "Synonym for extraneous data; used in context of background distractors."
          },
          {
            "keyword": "outliers",
            "weight": 5,
            "description": "Refers to data not belonging to the target model instance."
          },
          {
            "keyword": "missing data",
            "weight": 8,
            "description": "Defines as parts of the object not visible; example: occluded car wheel."
          },
          {
            "keyword": "occlusion",
            "weight": 5,
            "description": "Specific cause of missing data in real scenes."
          },
          {
            "keyword": "line detection",
            "weight": 5,
            "description": "Uses house facade example to illustrate multiple lines and occlusion."
          },
          {
            "keyword": "circle fitting",
            "weight": 5,
            "description": "Uses coin example to show clutter/outliers."
          },
          {
            "keyword": "car model",
            "weight": 5,
            "description": "Discusses complexity of articulated, multi-part model."
          },
          {
            "keyword": "RANSAC",
            "weight": 8,
            "description": "Explains hypothesis-and-verify paradigm for handling outliers."
          },
          {
            "keyword": "robust estimation",
            "weight": 6,
            "description": "Mentions M-estimators or least median of squares as alternatives."
          },
          {
            "keyword": "part-based model",
            "weight": 6,
            "description": "Suggests modeling car as assembly of ellipses to handle occlusion."
          },
          {
            "keyword": "hypothesis generation",
            "weight": 4,
            "description": "Briefly notes minimal subsets for parameter estimation."
          },
          {
            "keyword": "verification",
            "weight": 4,
            "description": "Mentions consensus set size or residual error."
          },
          {
            "keyword": "computational cost",
            "weight": 5,
            "description": "Discusses trade-off: more complex models need more hypotheses."
          },
          {
            "keyword": "articulation",
            "weight": 3,
            "description": "Acknowledges deformable or pose-varying components in complex models."
          }
        ],
        "grading_notes": "Require at least one example per challenge category. Full credit for suggesting appropriate algorithm (e.g., RANSAC for outliers). Deduct if strategies are generic or not linked to specific problem. Partial credit for good intuition without naming algorithms."
      }
    },
    {
      "id": "pkg1_essay1",
      "prompt": "Explain the Perspective-3-Point (P3P) problem in the context of absolute pose estimation for a calibrated camera. Describe the geometric formulation, the role of the three point correspondences, and how the problem can be converted into estimating the 3D locations of the points. Discuss the number of possible solutions and practical considerations for disambiguation.",
      "expected_keywords": [
        "P3P",
        "absolute pose",
        "calibrated camera",
        "three point correspondences",
        "unit vectors",
        "distances d_ij",
        "triangulation",
        "up to four solutions",
        "fourth point",
        "RANSAC"
      ],
      "rubric": {
        "total_points": 100,
        "criteria": [
          {
            "keyword": "P3P",
            "weight": 10,
            "description": "Clearly states that P3P solves for camera pose using three 3D-to-2D correspondences."
          },
          {
            "keyword": "absolute pose",
            "weight": 10,
            "description": "Defines absolute pose as estimating rotation R and translation t relative to world frame."
          },
          {
            "keyword": "calibrated camera",
            "weight": 10,
            "description": "Mentions that intrinsic parameters K are known, enabling normalized coordinates."
          },
          {
            "keyword": "three point correspondences",
            "weight": 10,
            "description": "Describes 3D points P_i and their 2D projections Q_i in the image."
          },
          {
            "keyword": "unit vectors",
            "weight": 10,
            "description": "Explains back-projection rays as unit vectors q_i from camera center."
          },
          {
            "keyword": "distances d_ij",
            "weight": 10,
            "description": "Shows computation of distances d_ij between 3D points using cosine law on known 2D angles."
          },
          {
            "keyword": "triangulation",
            "weight": 15,
            "description": "Details conversion to solving for 3D point depths along rays (up to four solutions)."
          },
          {
            "keyword": "up to four solutions",
            "weight": 10,
            "description": "Discusses the algebraic result yielding up to four valid pose configurations."
          },
          {
            "keyword": "fourth point",
            "weight": 10,
            "description": "Explains disambiguation using an additional correspondence or cheirality check."
          },
          {
            "keyword": "RANSAC",
            "weight": 5,
            "description": "Mentions robust estimation in presence of outliers using RANSAC with P3P."
          }
        ],
        "grading_notes": "Award partial credit for correct geometric intuition even if algebraic details are incomplete. Deduct points for confusing absolute vs. relative pose or calibrated vs. uncalibrated settings."
      }
    },
    {
      "id": "pkg16_essay1",
      "prompt": "Describe stereo geometry for parallel pinhole cameras. Derive 3D coordinates from disparity, including depth z = f b / d. Compute for given values. Compare correspondence in stereo to optical flow, similarities in feature matching, differences in constraints and applications.",
      "expected_keywords": [
        "stereo geometry",
        "pinhole cameras",
        "baseline b",
        "focal f",
        "disparity d",
        "3D reconstruction",
        "correspondence",
        "optical flow",
        "similarities",
        "differences"
      ],
      "rubric": {
        "total_points": 100,
        "criteria": [
          {
            "keyword": "stereo geometry",
            "weight": 10,
            "description": "Parallel setup."
          },
          {
            "keyword": "pinhole cameras",
            "weight": 10,
            "description": "Projection model."
          },
          {
            "keyword": "baseline b",
            "weight": 10,
            "description": "Separation."
          },
          {
            "keyword": "focal f",
            "weight": 10,
            "description": "Length."
          },
          {
            "keyword": "disparity d",
            "weight": 15,
            "description": "xl - xr."
          },
          {
            "keyword": "3D reconstruction",
            "weight": 15,
            "description": "Equations for x,y,z."
          },
          {
            "keyword": "correspondence",
            "weight": 10,
            "description": "Matching points."
          },
          {
            "keyword": "optical flow",
            "weight": 10,
            "description": "Temporal matching."
          },
          {
            "keyword": "similarities",
            "weight": 5,
            "description": "Search for matches."
          },
          {
            "keyword": "differences",
            "weight": 5,
            "description": "Geometry vs. dynamics."
          }
        ],
        "grading_notes": "Require derivation and example. Partial for missing comparisons."
      }
    },
    {
      "id": "pkg20_essay1",
      "prompt": "Classify texture patches using Roberts gradients and Euclidean, computing features and distance. Describe LBP procedure for classification, noting small patch issues. Compare to filter banks.",
      "expected_keywords": [
        "texture patches",
        "Roberts gradients",
        "feature means",
        "Euclidean classification",
        "LBP procedure",
        "histogram features",
        "patch size problem",
        "filter banks",
        "comparison"
      ],
      "rubric": {
        "total_points": 100,
        "criteria": [
          {
            "keyword": "texture patches",
            "weight": 10,
            "description": "Given classes."
          },
          {
            "keyword": "Roberts gradients",
            "weight": 15,
            "description": "Compute responses."
          },
          {
            "keyword": "feature means",
            "weight": 15,
            "description": "Vector creation."
          },
          {
            "keyword": "Euclidean classification",
            "weight": 10,
            "description": "Nearest class."
          },
          {
            "keyword": "LBP procedure",
            "weight": 15,
            "description": "Code, histogram."
          },
          {
            "keyword": "histogram features",
            "weight": 10,
            "description": "For comparison."
          },
          {
            "keyword": "patch size problem",
            "weight": 10,
            "description": "Limited stats."
          },
          {
            "keyword": "filter banks",
            "weight": 10,
            "description": "Alternative method."
          },
          {
            "keyword": "comparison",
            "weight": 5,
            "description": "To LBP/gradients."
          }
        ],
        "grading_notes": "Similar to pkg15."
      }
    },
    {
      "id": "pkg18_essay1",
      "prompt": "Explain LBP for texture recognition, including basic operator and uniform/rotation-invariant extensions. Describe texture analysis (filter banks for energy features) and synthesis (e.g., Markov models). For 2D transformations, count DOF and min correspondences. Solve for parameters in given nonlinear example using linear system Ax=b.",
      "expected_keywords": [
        "LBP",
        "binary operator",
        "uniform patterns",
        "texture recognition",
        "filter banks",
        "energy features",
        "texture synthesis",
        "2D transformations",
        "DOF",
        "linear system"
      ],
      "rubric": {
        "total_points": 100,
        "criteria": [
          {
            "keyword": "LBP",
            "weight": 15,
            "description": "Neighborhood thresholding."
          },
          {
            "keyword": "binary operator",
            "weight": 10,
            "description": "Code generation."
          },
          {
            "keyword": "uniform patterns",
            "weight": 10,
            "description": "Reduced labels."
          },
          {
            "keyword": "texture recognition",
            "weight": 10,
            "description": "Histogram comparison."
          },
          {
            "keyword": "filter banks",
            "weight": 10,
            "description": "Gabor etc. for analysis."
          },
          {
            "keyword": "energy features",
            "weight": 10,
            "description": "Response stats."
          },
          {
            "keyword": "texture synthesis",
            "weight": 10,
            "description": "Generating new textures."
          },
          {
            "keyword": "2D transformations",
            "weight": 10,
            "description": "DOF and correspondences."
          },
          {
            "keyword": "linear system",
            "weight": 10,
            "description": "Ax=b setup."
          }
        ],
        "grading_notes": "Require LBP details and transformation equations. Partial for missing synthesis."
      }
    },
    {
      "id": "pkg26_essay1",
      "prompt": "A rectangle is deformed as observed in the figure. It is known that the coordinates of the points deform through a function of the form: x' = ax + by, y' = cx² + dy. (a) How many degrees of freedom does the transformation have? How many point correspondences are needed to estimate the parameters? (b) Derive the equations to estimate the transformation parameters from a set of point correspondences. Hint: the transformation can be estimated linearly through a system of the form Ax = b.",
      "expected_keywords": [
        "degrees of freedom",
        "4",
        "point correspondences",
        "2",
        "linear system",
        "Ax=b",
        "least squares",
        "quadratic term",
        "nonlinear transformation",
        "parameter estimation"
      ],
      "rubric": {
        "total_points": 100,
        "criteria": [
          {
            "keyword": "degrees of freedom",
            "weight": 25,
            "description": "Correctly identifies 4 degrees of freedom (a, b, c, d)."
          },
          {
            "keyword": "point correspondences",
            "weight": 25,
            "description": "States that at least 2 point correspondences are needed for exact solution (4 equations from 2 points)."
          },
          {
            "keyword": "linear system derivation",
            "weight": 40,
            "description": "Correctly derives the linear system Ax = b by substituting correspondences into the transformation equations."
          },
          {
            "keyword": "clarity and structure",
            "weight": 10,
            "description": "Clear mathematical notation and logical flow in derivation."
          }
        ],
        "grading_notes": "Award partial credit for recognizing the quadratic term requires special handling. Deduct for incorrect matrix dimensions or missing terms."
      }
    },
    {
      "id": "pkg26_essay2",
      "prompt": "Using the following measured points before (p) and after (p') deformation, recover the parameters a, b, c, d, while assuming that there is no noise in the measurements. p = [0 2.00 4.00 6.00 8.00 10.00 10.00 8.00 6.00 4.00 2.00 0; 1 1.00 1.00 1.00 1.00 1.00 2.00 2.00 2.00 2.00 2.00 2], p' = [2 4.00 6.00 8.00 10.00 12.00 14.00 12.00 10.00 8.00 6.00 4; 1 1.04 1.16 1.36 1.64 2.00 3.00 2.64 2.36 2.16 2.04 2]. How would your solution be different if there was noise in the measurements?",
      "expected_keywords": [
        "overdetermined system",
        "least squares",
        "pseudoinverse",
        "normal equations",
        "robust estimation",
        "RANSAC",
        "outlier rejection"
      ],
      "rubric": {
        "total_points": 100,
        "criteria": [
          {
            "keyword": "parameter recovery",
            "weight": 40,
            "description": "Correctly computes a, b, c, d using exact data (e.g., via subset selection or full least squares)."
          },
          {
            "keyword": "noise handling",
            "weight": 30,
            "description": "Explains use of least squares on overdetermined system when noise is present."
          },
          {
            "keyword": "robustness",
            "weight": 20,
            "description": "Mentions RANSAC or outlier rejection for noisy real-world data."
          },
          {
            "keyword": "numerical stability",
            "weight": 10,
            "description": "Discusses conditioning or scaling of input coordinates."
          }
        ],
        "grading_notes": "Extra credit for implementing subset selection (e.g., using first two points) and verifying with full set. Deduct for algebraic errors in matrix setup."
      }
    },
    {
      "id": "pkg26_essay3",
      "prompt": "Two pinhole cameras observe a 3D point P = (X, Y, Z)^T from two different viewpoints. The camera projection matrices are C = [[1 0 0 0], [0 1 0 0], [0 0 1 0]] and C' = [[1 0 0 1], [0 0 -1 1], [0 1 0 1]]. The coordinates of the 2D point in the first and second image are p = (-1/2, 1/2)^T and p' = (0, -1/2)^T, respectively. (a) Compute the 3D coordinates X, Y and Z. (Hint: Form a linear system of equations using the projection matrices and homogeneous coordinates.) (b) The essential matrix between the views is E = [[0 1 1], [1 -1 0], [-1 0 -1]]. Find the epipolar line in the second image that corresponds to the point p in the first image. Show that the point p' lies on that epipolar line. (c) How can the epipolar constraint be utilized in stereo imaging?",
      "expected_keywords": [
        "triangulation",
        "homogeneous coordinates",
        "projection equations",
        "epipolar line",
        "epipolar constraint",
        "disparity search",
        "correspondence problem",
        "essential matrix"
      ],
      "rubric": {
        "total_points": 100,
        "criteria": [
          {
            "keyword": "3D reconstruction",
            "weight": 35,
            "description": "Correctly sets up and solves the linear system to find (X, Y, Z)."
          },
          {
            "keyword": "epipolar line",
            "weight": 30,
            "description": "Computes l' = E p and verifies p'^T l' = 0."
          },
          {
            "keyword": "stereo application",
            "weight": 25,
            "description": "Explains how epipolar constraint reduces 2D search to 1D along epipolar line."
          },
          {
            "keyword": "mathematical rigor",
            "weight": 10,
            "description": "Proper use of homogeneous coordinates and matrix notation."
          }
        ],
        "grading_notes": "Partial credit for correct setup even if final algebra has minor errors. Extra credit for discussing scale ambiguity or multiple solutions."
      }
    },
    {
      "id": "pkg26_essay4",
      "prompt": "Let us consider three grayscale image patches. The first two represent texture classes 1 and 2. The third belongs to an unknown class. Classify the unknown sample using: 1. Roberts gradient masks (no padding, valid pixels only), 2. Mean filter responses as feature vectors, 3. Euclidean distance to find closest class. Then, describe in detail a procedure based on Local Binary Patterns (LBP) for solving this texture classification problem. What problem is related to the given image patches?",
      "expected_keywords": [
        "Roberts cross",
        "gradient magnitude",
        "mean response",
        "Euclidean distance",
        "LBP",
        "circular neighborhood",
        "uniform patterns",
        "histogram",
        "rotation invariance",
        "small sample size",
        "border effects"
      ],
      "rubric": {
        "total_points": 100,
        "criteria": [
          {
            "keyword": "Roberts classification",
            "weight": 40,
            "description": "Correctly computes filter responses, means, and classifies via nearest neighbor."
          },
          {
            "keyword": "LBP procedure",
            "weight": 40,
            "description": "Describes LBP encoding, histogram construction, and chi-squared comparison."
          },
          {
            "keyword": "limitation identification",
            "weight": 20,
            "description": "Identifies small patch size, lack of rotation invariance, or border effects as issues."
          }
        ],
        "grading_notes": "Do not require actual computation in LBP part. Award points for clear step-by-step LBP pipeline."
      }
    },
    {
      "id": "pkg26_essay5",
      "prompt": "Given two consecutive frames of imagery with intensity function f(x,y,t), consider pixels with spatial coordinates (3,4) and (4,4) marked at t and t+Δt respectively. Here we assume that the origin is in the upper-left corner so that f(3,4,t) = 7 and f(4,4,t+Δt) = 3. (a) Use the 3x3 Prewitt masks to estimate the spatial derivatives ∂f/∂x and ∂f/∂y at the points (3,4,t) and (4,4,t+Δt). (b) Estimate the temporal derivative ∂f/∂t at the position (3,4,t) and (4,4,t+Δt).",
      "expected_keywords": [
        "Prewitt operator",
        "convolution",
        "spatial gradient",
        "finite difference",
        "temporal derivative",
        "optical flow constraint",
        "aperture problem"
      ],
      "rubric": {
        "total_points": 100,
        "criteria": [
          {
            "keyword": "Prewitt convolution",
            "weight": 50,
            "description": "Correctly applies Prewitt masks to compute fx and fy at both space-time locations."
          },
          {
            "keyword": "temporal derivative",
            "weight": 30,
            "description": "Computes ft using intensity difference and Δt (assumed or stated)."
          },
          {
            "keyword": "clarity of assumptions",
            "weight": 20,
            "description": "States Δt or justifies approximation; discusses limitations."
          }
        ],
        "grading_notes": "Accept either central or forward difference for ft. Deduct for incorrect mask orientation."
      }
    },
    {
      "id": "pkg26_essay6",
      "prompt": "A rectangle with corners A = (-1,1), B = (1,1), C = (1,-1), D = (-1,-1) undergoes an affine transformation and the corners are observed at A' = (1,2), B' = (3,2), C' = (-1,0), D' = (-3,0). Calculate the affine transformation using the least squares method. Then, explain when one should use a homography instead of an affine model. In what scenarios is a homography more expressive and flexible? When should algorithms use one over the other?",
      "expected_keywords": [
        "affine matrix",
        "6 parameters",
        "least squares",
        "overdetermined",
        "homography",
        "8 dof",
        "projective",
        "parallel lines",
        "vanishing point",
        "planar scene"
      ],
      "rubric": {
        "total_points": 100,
        "criteria": [
          {
            "keyword": "affine estimation",
            "weight": 45,
            "description": "Correctly forms and solves the 8x6 system for affine parameters."
          },
          {
            "keyword": "homography comparison",
            "weight": 35,
            "description": "Explains homography models perspective distortion; affine preserves parallelism."
          },
          {
            "keyword": "use cases",
            "weight": 20,
            "description": "Gives examples: affine for parallax-free motion, homography for planar surfaces under perspective."
          }
        ],
        "grading_notes": "Extra credit for verifying transformation on a test point. Accept pseudoinverse or normal equations."
      }
    },
    {
      "id": "pkg19_essay1",
      "prompt": "Develop seashell classifier. Segment using threshold on black background, improve with morphology. Extract texture with filter banks/LBP/co-occurrence for features. Classify with distance/classifier. Add shape like aspect ratio, combine features for joint classification. Use labeled training.",
      "expected_keywords": [
        "seashell classification",
        "segmentation",
        "thresholding",
        "morphology",
        "texture features",
        "filter banks",
        "LBP",
        "co-occurrence",
        "shape features",
        "feature combination",
        "training"
      ],
      "rubric": {
        "total_points": 100,
        "criteria": [
          {
            "keyword": "seashell classification",
            "weight": 5,
            "description": "System overview."
          },
          {
            "keyword": "segmentation",
            "weight": 10,
            "description": "Isolate shells."
          },
          {
            "keyword": "thresholding",
            "weight": 10,
            "description": "Contrast-based."
          },
          {
            "keyword": "morphology",
            "weight": 10,
            "description": "Cleanup."
          },
          {
            "keyword": "texture features",
            "weight": 15,
            "description": "Extraction method."
          },
          {
            "keyword": "filter banks",
            "weight": 5,
            "description": "One option."
          },
          {
            "keyword": "LBP",
            "weight": 5,
            "description": "One option."
          },
          {
            "keyword": "co-occurrence",
            "weight": 5,
            "description": "One option."
          },
          {
            "keyword": "shape features",
            "weight": 15,
            "description": "E.g., moments."
          },
          {
            "keyword": "feature combination",
            "weight": 10,
            "description": "Concatenate or fuse."
          },
          {
            "keyword": "training",
            "weight": 10,
            "description": "Labeled data."
          }
        ],
        "grading_notes": "Similar to pkg17."
      }
    },
    {
      "id": "pkg21_essay1",
      "prompt": "Explain stereo for parallel cameras. Derive 3D from disparity. Compute z for values. Compare stereo matching to flow: similar in correspondence, different in geometry vs time.",
      "expected_keywords": [
        "stereo",
        "parallel",
        "disparity",
        "3D equations",
        "computation",
        "matching",
        "flow comparison",
        "similarities",
        "differences"
      ],
      "rubric": {
        "total_points": 100,
        "criteria": [
          {
            "keyword": "stereo",
            "weight": 10,
            "description": "Setup."
          },
          {
            "keyword": "parallel",
            "weight": 10,
            "description": "Cameras."
          },
          {
            "keyword": "disparity",
            "weight": 15,
            "description": "Definition."
          },
          {
            "keyword": "3D equations",
            "weight": 15,
            "description": "x,y,z."
          },
          {
            "keyword": "computation",
            "weight": 10,
            "description": "Example."
          },
          {
            "keyword": "matching",
            "weight": 10,
            "description": "Correspondence."
          },
          {
            "keyword": "flow comparison",
            "weight": 10,
            "description": "To optical flow."
          },
          {
            "keyword": "similarities",
            "weight": 10,
            "description": "Matching."
          },
          {
            "keyword": "differences",
            "weight": 10,
            "description": "Constraints."
          }
        ],
        "grading_notes": "Similar to pkg14."
      }
    },
    {
      "id": "pkg17_essay1",
      "prompt": "Design a seashell classification system using texture and shape. Describe segmentation from black background using thresholding and morphology for cleanup. Explain texture extraction with filter banks/LBP/co-occurrence and classification with feature vectors, distance, classifier. Suggest shape features like eccentricity, compactness; combine via concatenation or separate classifiers. Discuss training with labeled data.",
      "expected_keywords": [
        "segmentation",
        "thresholding",
        "morphology",
        "texture extraction",
        "filter banks",
        "LBP",
        "co-occurrence",
        "feature vectors",
        "distance function",
        "classifier",
        "shape features",
        "combination"
      ],
      "rubric": {
        "total_points": 100,
        "criteria": [
          {
            "keyword": "segmentation",
            "weight": 10,
            "description": "Threshold on contrast."
          },
          {
            "keyword": "thresholding",
            "weight": 10,
            "description": "Binary separation."
          },
          {
            "keyword": "morphology",
            "weight": 10,
            "description": "Noise removal, hole filling."
          },
          {
            "keyword": "texture extraction",
            "weight": 15,
            "description": "Choose method."
          },
          {
            "keyword": "filter banks",
            "weight": 5,
            "description": "Multi-filter responses."
          },
          {
            "keyword": "LBP",
            "weight": 5,
            "description": "Local patterns."
          },
          {
            "keyword": "co-occurrence",
            "weight": 5,
            "description": "Spatial stats."
          },
          {
            "keyword": "feature vectors",
            "weight": 10,
            "description": "Summarize properties."
          },
          {
            "keyword": "distance function",
            "weight": 10,
            "description": "Similarity measure."
          },
          {
            "keyword": "classifier",
            "weight": 10,
            "description": "kNN or other."
          },
          {
            "keyword": "shape features",
            "weight": 10,
            "description": "Moments, boundaries."
          },
          {
            "keyword": "combination",
            "weight": 10,
            "description": "Fuse features."
          }
        ],
        "grading_notes": "Require full pipeline. Partial if missing shape integration."
      }
    },
    {
      "id": "pkg2_essay1",
      "prompt": "Discuss the challenges and assumptions in recovering 3D shape from shading using a single image under the Lambertian reflectance model. Explain the role of the brightness equation \\(I = \\mathbf{N} \\cdot \\mathbf{L}\\), why it is underconstrained, and how additional priors such as smoothness, integrability, or boundary conditions are used to regularize the solution. Include practical limitations and real-world violations of the model.",
      "expected_keywords": [
        "shape from shading",
        "Lambertian",
        "brightness equation",
        "I = N · L",
        "underconstrained",
        "integrability",
        "smoothness",
        "boundary conditions",
        "albedo variation",
        "cast shadows",
        "specular reflections",
        "unknown lighting"
      ],
      "rubric": {
        "total_points": 100,
        "criteria": [
          {
            "keyword": "shape from shading",
            "weight": 8,
            "description": "Clearly defines the goal: recover surface normals or depth from intensity variations."
          },
          {
            "keyword": "Lambertian",
            "weight": 8,
            "description": "States assumption of diffuse reflection with constant albedo."
          },
          {
            "keyword": "brightness equation",
            "weight": 8,
            "description": "Presents \\(I = \\mathbf{N} \\cdot \\mathbf{L}\\) or \\(I = \\cos \\theta_i\\) correctly."
          },
          {
            "keyword": "I = N · L",
            "weight": 8,
            "description": "Explains that intensity depends on dot product of normal and light direction."
          },
          {
            "keyword": "underconstrained",
            "weight": 10,
            "description": "Shows that one equation per pixel is insufficient to solve for two normal components."
          },
          {
            "keyword": "integrability",
            "weight": 12,
            "description": "Discusses need for normal field to be integrable into a consistent surface."
          },
          {
            "keyword": "smoothness",
            "weight": 10,
            "description": "Mentions regularization via minimizing changes in depth or normals."
          },
          {
            "keyword": "boundary conditions",
            "weight": 8,
            "description": "Notes importance of known depths or normals at occluding boundaries."
          },
          {
            "keyword": "albedo variation",
            "weight": 8,
            "description": "Identifies violation when surface reflectance is not uniform."
          },
          {
            "keyword": "cast shadows",
            "weight": 8,
            "description": "Recognizes that self-shadowing breaks the simple model."
          },
          {
            "keyword": "specular reflections",
            "weight": 6,
            "description": "Acknowledges specular highlights invalidate diffuse assumption."
          },
          {
            "keyword": "unknown lighting",
            "weight": 6,
            "description": "Points out difficulty when light direction is not known."
          }
        ],
        "grading_notes": "Award partial credit for intuitive explanations even if mathematical details are missing. Deduct for confusing shape from shading with stereo or structured light. Require at least two regularization methods for full marks on constraints."
      }
    },
    {
      "id": "pkg31_essay1",
      "prompt": "(a) Using homogeneous coordinates, write the matrix form of the following 2D transformations: pure translation, pure rotation, similarity (translation + rotation + scale), affine, and homography. How many degrees of freedom does each transformation have? How many 2D point correspondences are needed to estimate each?",
      "expected_keywords": [
        "homogeneous coordinates",
        "translation matrix",
        "rotation matrix",
        "similarity transformation",
        "affine transformation",
        "homography",
        "degrees of freedom",
        "point correspondences",
        "linear equations"
      ],
      "rubric": {
        "total_points": 100,
        "criteria": [
          {
            "keyword": "matrix forms",
            "weight": 40,
            "description": "Correctly writes each transformation in homogeneous matrix form (3x3 for 2D transformations)."
          },
          {
            "keyword": "degrees of freedom",
            "weight": 30,
            "description": "Identifies degrees of freedom for each transformation (translation=2, rotation=1, similarity=4, affine=6, homography=8)."
          },
          {
            "keyword": "point correspondences",
            "weight": 20,
            "description": "States number of point correspondences required to estimate each transformation (translation=1, rotation=2, similarity=2, affine=3, homography=4)."
          },
          {
            "keyword": "clarity",
            "weight": 10,
            "description": "Neat matrix notation and correct use of homogeneous coordinates."
          }
        ],
        "grading_notes": "Full credit for correct matrix representation and corresponding degrees of freedom. Partial credit for correct forms with minor omissions."
      }
    },
    {
      "id": "pkg31_essay2",
      "prompt": "(b) A rectangle with corners A = (−1,1), B = (1,1), C = (1,−1), and D = (−1,−1) undergoes a transformation and the corners are observed at A′ = (1,3), B′ = (3,3), C′ = (−2,1), and D′ = (−6,1). The affine transformation does not perfectly explain the observations, but we assume the transformation is affine and noisy. Estimate the optimal affine transformation using the least squares method.",
      "expected_keywords": [
        "affine transformation",
        "least squares estimation",
        "homogeneous coordinates",
        "overdetermined system",
        "Ax=b",
        "pseudoinverse",
        "normal equations",
        "noise",
        "parameter estimation"
      ],
      "rubric": {
        "total_points": 100,
        "criteria": [
          {
            "keyword": "affine model setup",
            "weight": 40,
            "description": "Forms the affine transformation equations in homogeneous coordinates and constructs the system Ax=b."
          },
          {
            "keyword": "least squares solution",
            "weight": 40,
            "description": "Uses least squares or pseudoinverse to estimate transformation parameters minimizing reprojection error."
          },
          {
            "keyword": "interpretation",
            "weight": 20,
            "description": "Explains the impact of noise and justifies the use of least squares as optimal for Gaussian noise."
          }
        ],
        "grading_notes": "Partial credit for correct system setup without derivation. Full credit requires explicit least-squares formulation or solution steps."
      }
    },
    {
      "id": "pkg31_essay3",
      "prompt": "(c) An affine transformation is the most flexible transformation that is linear in both homogeneous and inhomogeneous coordinates. When represented by a matrix (in homogeneous coordinates), how many elements does the matrix have? How many degrees of freedom does an affine transformation have? How many 2D point matches are necessary to estimate it?",
      "expected_keywords": [
        "affine matrix",
        "3x3 matrix",
        "six degrees of freedom",
        "2D point correspondences",
        "minimum points",
        "linear independence"
      ],
      "rubric": {
        "total_points": 100,
        "criteria": [
          {
            "keyword": "matrix elements",
            "weight": 40,
            "description": "States that the affine matrix has 9 elements in homogeneous form, but one scale factor is redundant."
          },
          {
            "keyword": "degrees of freedom",
            "weight": 30,
            "description": "Correctly identifies 6 degrees of freedom for 2D affine transformations."
          },
          {
            "keyword": "point matches",
            "weight": 30,
            "description": "Explains that 3 non-collinear 2D point correspondences are needed to estimate the affine transformation."
          }
        ],
        "grading_notes": "Partial credit for missing one of the three quantitative answers (elements, DoF, or points). Full credit for correct reasoning with supporting explanation."
      }
    },
    {
      "id": "pkg30_essay1",
      "prompt": "Triangulation using 1D cameras. The projection function for a 1D camera is m ∝ P x, where m = [m, 1]^T (pixel in homogeneous coords), x = [x, y, 1]^T (2D world point in homogeneous coords) and P is a 2×3 projection matrix. (a) Given two cameras P1 and P2 and their measurements m1 and m2 of an unknown point x, derive the constraints on x in the form A x = b (A is 2×2, b is 2×1). Express A and b in terms of P1, P2 and the measurements. (b) Given P1 = [[1,2,0],[2,1,0]] and P2 = [[1,2,3],[4,2,0]] and measurements m1 = 1.25 and m2 = 1, triangulate the point x (solve for x and y). (c) A point is often observed in many images: is there an advantage to estimating the point’s position by considering all images simultaneously? Justify. (d) Many points are observed in an image: is there an advantage to estimating all points jointly instead of independently? Justify.",
      "expected_keywords": [
        "triangulation",
        "homogeneous coordinates",
        "projection matrix",
        "linear constraint",
        "Ax=b",
        "overdetermined system",
        "least squares",
        "bundle adjustment",
        "multi-view",
        "robust estimation",
        "joint estimation",
        "covariance",
        "optimal estimation"
      ],
      "rubric": {
        "total_points": 100,
        "criteria": [
          {
            "keyword": "derivation Ax=b",
            "weight": 40,
            "description": "Correct derivation of A and b from the rows of P1,P2 and measured m1,m2, producing a 2×2 linear system for (x,y)."
          },
          {
            "keyword": "numerical triangulation",
            "weight": 30,
            "description": "Correct solution of the provided numeric example (substituting P1,P2,m1,m2 into derived system and solving for x,y)."
          },
          {
            "keyword": "multi-view advantage",
            "weight": 15,
            "description": "Explains benefits of using many images simultaneously (reduces noise, overdetermined -> least-squares, improved accuracy, outlier handling)."
          },
          {
            "keyword": "joint estimation of many points",
            "weight": 15,
            "description": "Explains advantages of joint estimation (bundle adjustment, enforces consistency, uses image/pose covariances) and notes computational trade-offs."
          }
        ],
        "grading_notes": "Partial credit for correct setup with minor algebraic errors. For (b) accept exact numeric solution or equivalent least-squares result. Award extra credit for mentioning covariance, weighting, or robust methods (e.g., RANSAC) when discussing multi-view or joint estimation."
      }
    },
    {
      "id": "pkg29_essay1",
      "prompt": "Describe an algorithm to segment the seashells from the image. How can morphological operations help to improve segmentation?",
      "expected_keywords": [
        "image segmentation",
        "thresholding",
        "background subtraction",
        "binary mask",
        "morphological operations",
        "erosion",
        "dilation",
        "opening",
        "closing",
        "connected components",
        "noise removal"
      ],
      "rubric": {
        "total_points": 100,
        "criteria": [
          {
            "keyword": "segmentation algorithm",
            "weight": 40,
            "description": "Proposes an algorithm for segmenting non-overlapping shells on a contrasting background using thresholding or color separation."
          },
          {
            "keyword": "morphological operations",
            "weight": 40,
            "description": "Explains how morphological filtering (erosion, dilation, opening, closing) improves segmentation quality and removes noise."
          },
          {
            "keyword": "clarity and application",
            "weight": 20,
            "description": "Provides a clear, step-by-step explanation with reasoning for each operation."
          }
        ],
        "grading_notes": "Partial credit for mentioning morphological operations without explaining their effect. Full credit for integrating them effectively into the segmentation pipeline."
      }
    },
    {
      "id": "pkg29_essay2",
      "prompt": "It has been decided that only texture will be used to classify the seashells. You may use either filter banks, Local Binary Patterns (LBP), or co-occurrence matrices to extract texture properties. Given a segmented image, describe an algorithm to classify a seashell. Where do the concepts of feature vector, distance function, and classifier fit in your algorithm?",
      "expected_keywords": [
        "texture features",
        "filter banks",
        "Gabor filters",
        "LBP",
        "gray-level co-occurrence matrix",
        "feature vector",
        "distance function",
        "Euclidean distance",
        "classifier",
        "training",
        "k-nearest neighbour",
        "support vector machine"
      ],
      "rubric": {
        "total_points": 100,
        "criteria": [
          {
            "keyword": "texture extraction",
            "weight": 30,
            "description": "Describes texture feature extraction using one of the suggested methods and explains its relevance."
          },
          {
            "keyword": "feature vector and distance",
            "weight": 35,
            "description": "Explains how texture features are formed into a feature vector and compared using a distance metric."
          },
          {
            "keyword": "classifier integration",
            "weight": 25,
            "description": "Describes how a classifier (e.g., k-NN or SVM) uses these feature vectors for training and prediction."
          },
          {
            "keyword": "clarity of process",
            "weight": 10,
            "description": "Logical structure and clear algorithmic flow from input to classification output."
          }
        ],
        "grading_notes": "Full credit requires integration of all three concepts—feature extraction, distance measurement, and classification—in a coherent algorithmic pipeline."
      }
    },
    {
      "id": "pkg29_essay3",
      "prompt": "The system should be extended to include shape features. What shape features could be useful for classifying the shells? How should one combine them with the texture features of the previous step? How should the algorithm be adapted to classify using both texture and shape?",
      "expected_keywords": [
        "shape features",
        "contour descriptors",
        "moments",
        "Hu moments",
        "aspect ratio",
        "Fourier descriptors",
        "texture-shape fusion",
        "feature-level fusion",
        "normalization",
        "weighted combination",
        "classifier adaptation"
      ],
      "rubric": {
        "total_points": 100,
        "criteria": [
          {
            "keyword": "shape descriptors",
            "weight": 35,
            "description": "Identifies relevant shape features and explains how they capture geometric differences between shells."
          },
          {
            "keyword": "feature fusion",
            "weight": 35,
            "description": "Describes how to combine texture and shape features via concatenation, normalization, or weighting."
          },
          {
            "keyword": "algorithm adaptation",
            "weight": 20,
            "description": "Proposes classifier or feature scaling modifications to integrate multimodal data."
          },
          {
            "keyword": "explanation clarity",
            "weight": 10,
            "description": "Provides a well-organized, conceptually sound discussion of combining multiple feature types."
          }
        ],
        "grading_notes": "Award partial credit for identifying correct shape features without discussing integration. Full credit for coherent fusion strategy and algorithmic justification."
      }
    },
    {
      "id": "pkg28_essay1",
      "prompt": "Briefly explain the following terms: (a) Optical flow (b) SIFT (c) Histogram (d) K-nearest neighbour classification (e) HSV color space.",
      "expected_keywords": [
        "optical flow",
        "motion estimation",
        "SIFT",
        "keypoints",
        "histogram",
        "feature distribution",
        "k-nearest neighbour",
        "classification",
        "HSV color space",
        "hue",
        "saturation",
        "value"
      ],
      "rubric": {
        "total_points": 100,
        "criteria": [
          {
            "keyword": "optical flow",
            "weight": 20,
            "description": "Defines optical flow as apparent pixel motion between frames and explains its role in motion estimation."
          },
          {
            "keyword": "SIFT",
            "weight": 20,
            "description": "Explains Scale-Invariant Feature Transform for keypoint detection and descriptor matching."
          },
          {
            "keyword": "histogram",
            "weight": 20,
            "description": "Describes histograms as statistical distributions of intensity or color values in an image."
          },
          {
            "keyword": "k-nearest neighbour",
            "weight": 20,
            "description": "Describes KNN as a distance-based classifier using feature similarity."
          },
          {
            "keyword": "HSV color space",
            "weight": 20,
            "description": "Explains the components of hue, saturation, and value and their perceptual significance."
          }
        ],
        "grading_notes": "Award partial credit for accurate but incomplete definitions. Full credit requires both definition and context of use in computer vision."
      }
    },
    {
      "id": "pkg28_essay2",
      "prompt": "Describe the main principles of the following and give one example of their usage: (a) Hough transform (b) RANSAC (c) Mahalanobis distance.",
      "expected_keywords": [
        "Hough transform",
        "parametric space",
        "line detection",
        "RANSAC",
        "robust estimation",
        "outliers",
        "Mahalanobis distance",
        "covariance",
        "multivariate distance",
        "feature matching"
      ],
      "rubric": {
        "total_points": 100,
        "criteria": [
          {
            "keyword": "Hough transform",
            "weight": 30,
            "description": "Describes parameter space voting for detecting geometric shapes such as lines or circles."
          },
          {
            "keyword": "RANSAC",
            "weight": 35,
            "description": "Explains Random Sample Consensus as a method for model estimation in the presence of outliers."
          },
          {
            "keyword": "Mahalanobis distance",
            "weight": 25,
            "description": "Defines Mahalanobis distance as a covariance-scaled metric for multivariate data comparison."
          },
          {
            "keyword": "examples and clarity",
            "weight": 10,
            "description": "Provides clear examples demonstrating real-world usage of each concept."
          }
        ],
        "grading_notes": "Full credit requires a clear example for each method. Partial credit for correct principles without examples."
      }
    }
  ],
  "notes": "Merged automatically using Question Bank Generator"
}