{
  "timestamp": "2025-11-13T09:53:23.664528",
  "subject": "towards_data_mining",
  "package_id": "package_11",
  "mcq_score": 0,
  "mcq_total": 0,
  "essay_score": 46,
  "essay_total": 100,
  "final_score": 46.0,
  "matched_keywords": [
    "artificial data",
    "RCT",
    "data management",
    "validation set",
    "data normalization",
    "predictive model",
    "data modeling"
  ],
  "user_answers": {},
  "user_essay_answers": {
    "pkg27_essay1": "https://chatgpt.com/c/691580f6-3ad0-832b-b28f-fdf2db39bf94\n\nZ-score: A normalization technique that rescales data to have a mean of 0 and a standard deviation of 1.\n\n a. Artificial data:  Synthetic data generated by sampling from a statistical distribution that approximates the original dataset. Artificial data is created by modeling the original dataset using parameters such as mean (\u03bc) and standard deviation (\u03c3). Each class is modeled separately, and the distribution must be known or estimated. New samples are then drawn from these distributions to form a larger dataset. This method is especially useful when the original dataset is too small for effective model training.\nb. Randomized controlled trial (RCT):  A type of scientific experiment designed to minimize bias by randomly assigning participants to experimental and control groups. RCTs are widely used in medical and behavioral studies to establish causal relationships. Variants include parallel, crossover, cluster, and factorial designs. Randomization ensures comparability between groups.\nc. Data management: The set of practices that ensure data is properly stored, maintained, and accessible for processing and analysis. Data management encompasses persistent storage, security, metadata curation, and accessibility. Proper management ensures reliability and longevity of data for research and applications.\nd. Validation set:  A subset of data used to tune model hyperparameters or select among competing models before final evaluation on the test set. The validation set is used during model selection to ensure that the chosen model is not overfitting the training data. After models are compared based on validation performance, the final selected model is evaluated on a separate test set for unbiased performance estimation. This partitioning improves generalizability and prevents overly optimistic results.\ne. Data normalization\nf. Predictive model\n\nData Modeling: process of organizing data into meaningful structures (entities, attributes, and relationships) so that it accurately represents real-world concepts. It defines how data is stored, connected, and constrained.\n\nWhy relational databases are used:\n\nData integrity: Enforce rules such as primary keys, foreign keys, and constraints to prevent errors.\n\nAvoid redundancy: Normalize tables so data is not duplicated, making updates consistent.\n\nEfficient querying: SQL allows fast searching, joining, filtering, and aggregation.\n\nScalability and consistency: Relational databases follow ACID properties, ensuring reliable behavior in multi-user environments.\n\ni. SELECT firstName, lastName FROM Caretaker ORDER BY age DESC; Sort data from oldest to youngest\nii. SELECT * FROM Animal WHERE pricePerHead BETWEEN 300 AND 600; return data for animal prices between 300 to 600\niii. SELECT animalKind FROM Animal A INNER JOIN Caretaker C ON A.caretakerId = C.caretakerId WHERE firstName = 'Betty': find animal that are taken care by Better \niv. SELECT MIN(pricePerHead) FROM Animal WHERE headCount < 50; Given a headcount less than 50 (small group animal), find the mininum price per head\n\n3. \ni. mean imputation: filling empty value by the mean value of it\nAdvantage: Simple and fast; preserves variable mean.\nDisadvantage: Underestimates variance; weakens relationships between variables.\n\nii. regression imputation: filling empty value by the regression model output of it\nAdvantage: Preserves relationships between variables.\nDisadvantage: Reduces variance because all imputed values lie exactly on the regression line.\n\niii. stochastic regression imputation: similar to regression imputation with the addition of noise sampling\nAdvantage: Preserves both relationships and natural variability by adding noise.\nDisadvantage: Assumes correct model specification; can add unrealistic randomness if assumptions fail.\n\niv. multiple imputation: combination of all of these 3 imputation methods\n\na. Three ways data collection can go wrong (3p)\n\nSelection bias: The sample is not representative of the population.\nExample: surveying only university students for national health statistics.\n\nNon-response bias: People who choose not to respond differ from those who do.\nExample: only healthy people respond to a health survey.\n\nMeasurement bias: Faulty instruments or inconsistent procedures create incorrect values.\nExample: sensor errors, incorrect labeling, survey misunderstandings.\n\n(Other acceptable answers from the lecture: incomplete data, timing errors, transmission errors.)\n\nb. Dataset: 25 cancer, 275 no cancer \u2192 imbalanced. Suggest a balancing method (3p)\n\nRecommended method: SMOTE (Synthetic Minority Oversampling Technique).\n\nJustification:\n\nSMOTE creates synthetic samples for the minority class (cancer patients) by interpolating between existing minority examples.\n\nIt avoids simple duplication, which can cause overfitting.\n\nProduces a more balanced dataset, improving sensitivity and overall predictive power.\n\nWorks well for continuous feature spaces.\n\n(Alternatives: SRSWR oversampling of minority class, artificial data generation, noise injection.)\n\n5. Data Normalization (6p)\na. Why normalization is needed before predictive modeling? (2p)\n\nPrevents attributes with large scales from dominating:\nFeatures like income (0\u2013100,000) overshadow features like age (0\u2013100).\n\nImproves model performance:\nMany algorithms (k-NN, neural networks, gradient descent models) work faster and more accurately when features are on similar scales.\n\nEnsures stable training:\nPrevents numerical instability and speeds up convergence in optimization.\n\nb. Describe normalization formulas (1p each)\nMin\u2013max scaling\n\nv' = (v \u2013 v_min) / (v_max \u2013 v_min)\n\nTransforms values to range 0\u20131 (or custom range if extended).\n\nZ-score standardization\n\nv' = (v \u2013 \u03bc) / \u03c3\n\nCenters data at mean 0 with standard deviation 1.\n\nc. Effect on mean, variance, extreme values, distribution shape (2p)\nMin\u2013max normalization\n\nMean: Changes; depends on scaling.\n\nVariance: Shrinks; values compressed into smaller range.\n\nExtreme values: Strongly affected because min and max set the scale.\n\nDistribution shape: Preserves shape but compresses/stretch axes.\n\nZ-score normalization\n\nMean: Becomes 0.\n\nVariance: Becomes 1 (if computed on sample).\n\nExtreme values: Less compressive; outliers become large positive/negative values.\n\nDistribution shape: Shape preserved; only re-centered and re-scaled."
  }
}