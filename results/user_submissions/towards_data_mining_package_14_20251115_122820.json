{
  "timestamp": "2025-11-15T12:28:20.804152",
  "subject": "towards_data_mining",
  "package_id": "package_14",
  "mcq_score": 0,
  "mcq_total": 0,
  "essay_score": 0,
  "essay_total": 500,
  "final_score": 0.0,
  "matched_keywords": [],
  "user_answers": {},
  "user_essay_answers": {
    "pkg26_essay1": " a. Open data (1p): Data that anyone anyone can freely access, use, modify, and share for any purpose, subject at most to minimal conditions such as attribution. These introduce transparency, innovation, and collaboration. It must be findable, accessible, interoperable, and reusable according to the FAIR principles.\nb. Contextual outlier (1p): Values that are only outliers within a specific contextual setting or subset of conditions. \nc. Data model (1p) \nd. Accuracy paradox (1p): model with high accuracy performs poorly on the actual predictive task, especially when the dataset is imbalanced, which is misleading impression of model performance because it does not account for class imbalance. In problems where one class dominates, models can achieve high accuracy by simply predicting the majority class, despite having little real predictive power\ne. Variable construction (1p): Constructing new variables from existing ones to improve predictive performance or simplify data representation, which Includes building composite indicators or derived attributes.\nf. Cross-validation (1p): resampling technique used to assess the performance and generalization of a model by partitioning the dataset into subsets, which divides the data into multiple folds, training the model on some while testing on others",
    "pkg26_essay2": "Deletion is the most common method used to handle missing values, essentially pretending the problem does not exist\n\nListwise Deletion (also called Complete-Case Analysis): discarding any rows (observations/cases) that contain any missing values (MVs). If the data are Missing Completely At Random (MCAR), this method does not bias the results, but it does decrease the statistical power if many observations are discarded\n\nIn the case when the data are not MCAR, \n\n2. Pairwise Deletion (also called Available-Case Analysis):\n    \u25e6 This method uses all the instances of a variable (or a group of variables) that are present for a specific analysis. When calculating the mean or covariance of variables, you would use only the available pairs of data points\n\nFor Listwise Deletion, use the data that have all columns, hence only 2 rows that we use (row 2 and 6) for further analysis. Whereas the pairwise deletion, only the empty value for respective analysis, in our cases like weight, hence for female, we are going to use (row 2, 4) and for male we are going to use (row 6 and 7)\n\nMultiple Imputation aims for taking into account of the inherent uncertainty regarding missing data imputations. The process starts by Imputation steps, like several copies of the complete dataset are created and use different imputed value, that reflec the uncertainty of the data, and the observed data  entries remain identical across all copies. Next, the analysis step will be conducted as the independent analysis is conducted, and further pooling steps, which is from the sets of parameters estimates and standard error obtains will be combined into a single set of results. As the methods are combining all independent imputation methods, the pooled estimates will be unbiased and have correct statistical properties. For example, the mean and regression imputation that are underestimated variability and standard errors, combined with other methods, makes the final variance estimate is statistically robust\n\nMCAR: missingness mechanism where the probability that data are missing is independent of both observed and unobserved data.\nnor MCAR means Missing At Random (MAR), Missing Not At Random (MNAR) or Not Missing At Random (NMAR)",
    "pkg26_essay3": "Data ethics study about  dealing responsibly with the data you've been entrusted with. Anonymity is the main defense against an unintentional breach of privacy, as the data might purged of any information that may result in the identification of individual subjects, which leads to data privacy protection. Wheras the Informed consent is a crucial principle in any research involving human subject, which talks about permission from each individual whose data you wish to us\n\nOpen means anyone can freely access, use, modify, and share for any purpose. Several constraints that we need to take into account are \nPreserving Provenance and Openness: Constraints generally mandate that if you share the data with others, you may be obliged to keep it open and tell everyone where it originally came from\nLicensing Schemes: The license of an open dataset determines the terms and conditions17. For example, the Attribution (CC BY) license permits distribution and derivative works, as long as the original creator is credited\n\nThe benefit of open data are \n1. Individual Data Miner (User): which saves resources that might be s[end for collecting data or obtaining access to non-open datasets. Open data also allows for more overall value to be extracted from a dataset\n2. Society as a Whole (Producer/Funder): For the producer, especially if the data collection is publicly funded, openness ensures that the funder's interest is served, as data managed and published according to principles like FAIR is increasingly viewed as a research merit. Publication of open data by public bodies also promotes transparency",
    "pkg26_essay4": "Knowledge Discovery Process is the overall process of discovering useful knowledge from data, whereas Data Mining (DM) is the application of algorithms for extracting patterns from data\n1. Data Cleaning\n2. Data Integration (using multiple databases or files)2.\n3. Data Transformation\n4. Data Reduction\n5. Data Discretization (which is part of data reduction, involving replacing numerical variables with nominal ones)\n\nData Reduction:  is a crucial part of pre-processing aimed at obtaining a reduced representation of the dataset that is much smaller in volume but still produces the same (or almost the same) analytical results, which helps to bring efficiency with the focused on the needed features for analysis. Several types of data reduction are  Reducing the Number of Variables, which is focused on reducing the dimensionality of the data by removing or combining attributes. Which includes Removing Irrelevant Attributes, Aggregation in the summary form based on certain group, using methods like Principal Component Analysis (PCA) that works only for numeric data and when the number of dimensions is large, numerosity reduction that finds a smaller set of orthogonal vectors (principal components) that can best represent the data. Discrete Wavelet Transform (DWT) can be used for very high dimensionality data, useful when the number of variables is large and the number of observations is small. Reducing the Number of Variable Values is another type, like binning, clustering, or categorizing/discretization and generalization. And another type are Reducing the Number of Observations, like sampling (selecting subset observation) like Simple Random Sample Without Replacement (SRSWOR) or Simple Random Sample With Replacement (SRSWR). \n\nSimple Random Sample Without Replacement (SRSWOR): each element of the dataset has an equal chance of being selected, and once selected, it is not replaced. \n\nSimple Random Sample With Replacement (SRSWR):  each element can be selected more than once, as it is replaced back into the dataset after being drawn. It is often used in bootstrapping and data augmentation where variability is desired despite small datasets.\n",
    "pkg26_essay5": ""
  }
}