{
  "timestamp": "2025-11-15T10:58:21.867454",
  "subject": "towards_data_mining",
  "package_id": "package_11",
  "mcq_score": 0,
  "mcq_total": 0,
  "essay_score": 36,
  "essay_total": 100,
  "final_score": 36.0,
  "matched_keywords": [
    "artificial data",
    "RCT",
    "data management",
    "validation set",
    "data normalization",
    "predictive model"
  ],
  "user_answers": {},
  "user_essay_answers": {
    "pkg27_essay1": "https://chatgpt.com/c/69183a5f-f8c0-8330-ae1b-2465797e2e4d\n\n1.\na. Artificial data\nb. Randomized controlled trial (RCT): scientific experiment that aims to minimize bias by randomly assigning participants to experimental and control groups.\nc. Data management: availability of suitable and sufficient data for training and evaluating models.\nd. Validation set: Subset data with the objective to tune model hyperparameters or finding competitive model before final evaluation on the test set, and ensure not overfitting the training data and improves generalizability and prevents overly optimistic results.\ne. Data normalization: process of adjusting values measured on different scales to a notionally common scale without distorting differences in the ranges of values, that aims for the algorithms converge faster and ensures that attributes with large numerical ranges do not dominate those with smaller range\nf. Predictive model: modeling approach that uses historical data to predict future or unseen observations.\n\n4. \nSynthetic Minority Oversampling Technique (SMOTE) can be used, as \n\n5. \nIt ensure the features operate on a comparable scale and make the algorithm converge. For example in predictive models, when the features are sensitive to differences in feature magnitude, like large numerical range, that makes it dominate distance calculations, gradient updates, or loss contributions. Normalization improves model stability, ensures faster and more reliable convergence, and often increases predictive performance by preventing scale-related bias.\n\nMin-max is rescaling the minimum value into 0 and largest into one, with the rest of value are scaled proportional to the range of the data\nZ-score, instead of just min-max, it will be based on the mean data and data's standard deviation. The value is represented by how much the value deviated from the mean by the multiple of standard deviation\n\nMin\u2013max normalization\nMean: It will be based on the min and max value as it's shifted by the minimum value and rescaled by the range of the value \u03bc' = ( \u03bc \u2212 v_min ) / (v_max \u2212 v_min)\nVariance: Shrinks because the entire feature range is compressed to [0, 1]. Var(v') = Var(v) / (v_max \u2212 v_min)\u00b2\n\nExtreme values: Very sensitive to outliers; as the extreme values affected by the denuminator of range data, which can makes it larger distance (if the range distance are smaller than 1)\nDistribution shape: The shape is preserved (monotonic transformation) but horizontally compressed (\n\n\nMean: Always becomes 0 after standardization.\n\nVariance: Becomes 1 because values are scaled by the standard deviation.\n\nExtreme values: Outliers remain outliers but become expressed in standard deviation units (for example, +5 or \u22124).\n\nDistribution shape: Shape is preserved; standardization does not change skewness or kurtosis.\n\n3. \n1. Mean Imputation\nMean imputation involves replacing missing values with the average of the observed values for that variable34.\na. Mathematical Form of Imputation Generation\nThe imputed value (v \n\u2032\n ) is generated by calculating the mean ( \nv\n\u02c9\n ) of the observed values of the variable v. This can be done globally (using all available instances) or stratified (using instances belonging to the same class)4.\nv \n\u2032\n = \nv\n\u02c9\n \nb. Advantage and Disadvantage\nComponent\nExplanation (Source Citation)\nAdvantage\nIt is widely used and makes it possible to analyze the data5. Also, mean estimates are unbiased, provided the data is Missing Completely At Random (MCAR)56.\nDisadvantage\nIt reduces the data's variability because all imputed values cluster or \"pile up at the mean\"5. Furthermore, imputed values are constant and therefore cannot correlate with other variables5.\n\n--------------------------------------------------------------------------------\n2. Regression Imputation\nRegression imputation uses a regression equation based on other available variables to predict the missing value5.\na. Mathematical Form of Imputation Generation\nThe imputed value ( \nv\n^\n ) is the predicted value derived from a regression model, where X \ni\n\u200b\n  represents the complete (explanatory) variables, and \u03b2 \ni\n\u200b\n  represents the regression weights (coefficients) determined by fitting the model to the non-missing data56:\nv\n^\n =\u03b2 \n0\n\u200b\n +\u03b2 \n1\n\u200b\n X \n1\n\u200b\n +\u03b2 \n2\n\u200b\n X \n2\n\u200b\n +\u2026\nSince the prediction is deterministic, the imputed values fall directly onto the regression surface5.\nb. Advantage and Disadvantage\nComponent\nExplanation (Source Citation)\nAdvantage\nMean estimates are unbiased under MCAR6. Regression weights of the imputation model are unbiased if the explanatory variables used in the regression are complete6. Additionally, if the imputation model is correctly specified, it can be unbiased under Missing At Random (MAR) conditions6.\nDisadvantage\nThere is a lack of variability because the imputed values fall directly on the regression surface5. This causes estimates of variation and association strength to be biased, and the variability of the imputed data is underestimated6.\n\n--------------------------------------------------------------------------------\n3. Stochastic Regression Imputation\nStochastic regression imputation builds upon standard regression imputation by adding random noise (a residual error term) to the predicted value, thereby restoring some of the lost variability7.\na. Mathematical Form of Imputation Generation\nThe imputed value ( \nv\n^\n  \nstoch\n\u200b\n ) is generated by adding a noise term (\u03f5) to the deterministic regression estimate. The noise term is drawn from the estimated residual variance (\u03c3 \n\u03f5\n\u200b\n ) calculated when the regression model is fit to the complete data7:\nv\n^\n  \nstoch\n\u200b\n =(\u03b2 \n0\n\u200b\n +\u03b2 \n1\n\u200b\n X \n1\n\u200b\n +\u2026)+\u03f5\nwhere \u03f5 is a residual drawn according to the estimated residual variance7.\nb. Advantage and Disadvantage\nComponent\nExplanation (Source Citation)\nAdvantage\nIt restores the lost variability in the imputed data points and preserves the relationship and correlation of the variables7. This helps prevent the biasing effects seen when deterministic values are used7.\nDisadvantage\nThis method underestimates standard errors7. Furthermore, it can sometimes lead to impossible values (e.g., imputing a negative ozone concentration or a gender/pregnancy combination that is infeasible)8."
  }
}